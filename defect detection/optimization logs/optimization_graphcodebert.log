/opt/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/opt/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
INFO:name:device: cuda:0, n_gpu: 1
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/main/config.json HTTP/1.1" 200 0
INFO:name:Initialization of the population, may take a while ...
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/graphcodebert-base/resolve/main/vocab.json HTTP/1.1" 200 0
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
[INFO|(OpenDelta)basemodel:700]2024-04-20 14:57:09,422 >> Trainable Ratio: 842881/124897921=0.674856%
[INFO|(OpenDelta)basemodel:702]2024-04-20 14:57:09,422 >> Delta Parameter Ratio: 251520/124897921=0.201380%
[INFO|(OpenDelta)basemodel:704]2024-04-20 14:57:09,422 >> Static Memory 0.00 GB, Max Memory 0.00 GB

 ==> Delta parameters :   [0, 0, 0, 0, {'insert_modules': ['attention'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['attention', 'output', 'attention.self'], 'bottleneck_dim': [64, 32, 24], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.544: 100%|█████████▉| 408/410 [01:35<00:00,  4.20it/s]
epoch 0 loss 0.689  acc 0.543: 100%|█████████▉| 408/410 [01:35<00:00,  4.20it/s]
epoch 0 loss 0.689  acc 0.543: 100%|█████████▉| 409/410 [01:35<00:00,  4.20it/s]
epoch 0 loss 0.689  acc 0.544: 100%|█████████▉| 409/410 [01:36<00:00,  4.20it/s]
epoch 0 loss 0.689  acc 0.544: 100%|██████████| 410/410 [01:36<00:00,  4.55it/s]
epoch 0 loss 0.689  acc 0.544: 100%|██████████| 410/410 [01:36<00:00,  4.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.676  acc 0.57: 100%|█████████▉| 408/410 [01:37<00:00,  4.16it/s]
epoch 1 loss 0.676  acc 0.57: 100%|█████████▉| 408/410 [01:37<00:00,  4.16it/s]
epoch 1 loss 0.676  acc 0.57: 100%|█████████▉| 409/410 [01:37<00:00,  4.16it/s]
epoch 1 loss 0.676  acc 0.569: 100%|█████████▉| 409/410 [01:37<00:00,  4.16it/s]
epoch 1 loss 0.676  acc 0.569: 100%|██████████| 410/410 [01:37<00:00,  4.49it/s]
epoch 1 loss 0.676  acc 0.569: 100%|██████████| 410/410 [01:37<00:00,  4.20it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.668  acc 0.573: 100%|█████████▉| 408/410 [01:37<00:00,  4.21it/s]
epoch 2 loss 0.668  acc 0.573: 100%|█████████▉| 408/410 [01:37<00:00,  4.21it/s]
epoch 2 loss 0.668  acc 0.573: 100%|█████████▉| 409/410 [01:37<00:00,  4.21it/s]
epoch 2 loss 0.668  acc 0.573: 100%|█████████▉| 409/410 [01:37<00:00,  4.21it/s]
epoch 2 loss 0.668  acc 0.573: 100%|██████████| 410/410 [01:37<00:00,  4.54it/s]
epoch 2 loss 0.668  acc 0.573: 100%|██████████| 410/410 [01:37<00:00,  4.20it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.664  acc 0.583: 100%|█████████▉| 408/410 [01:37<00:00,  4.20it/s]
epoch 3 loss 0.664  acc 0.583: 100%|█████████▉| 408/410 [01:37<00:00,  4.20it/s]
epoch 3 loss 0.664  acc 0.583: 100%|█████████▉| 409/410 [01:37<00:00,  4.20it/s]
epoch 3 loss 0.664  acc 0.583: 100%|█████████▉| 409/410 [01:37<00:00,  4.20it/s]
epoch 3 loss 0.664  acc 0.583: 100%|██████████| 410/410 [01:37<00:00,  4.55it/s]
epoch 3 loss 0.664  acc 0.583: 100%|██████████| 410/410 [01:37<00:00,  4.20it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.66  acc 0.592: 100%|█████████▉| 408/410 [02:21<00:01,  1.96it/s]
epoch 4 loss 0.66  acc 0.592: 100%|█████████▉| 408/410 [02:22<00:01,  1.96it/s]
epoch 4 loss 0.66  acc 0.592: 100%|█████████▉| 409/410 [02:22<00:00,  1.96it/s]
epoch 4 loss 0.66  acc 0.592: 100%|█████████▉| 409/410 [02:22<00:00,  1.96it/s]
epoch 4 loss 0.66  acc 0.592: 100%|██████████| 410/410 [02:22<00:00,  2.12it/s]
epoch 4 loss 0.66  acc 0.592: 100%|██████████| 410/410 [02:22<00:00,  2.87it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 15:07:02,041 >> Trainable Ratio: 1046121/125101161=0.836220%
[INFO|(OpenDelta)basemodel:702]2024-04-20 15:07:02,041 >> Delta Parameter Ratio: 454760/125101161=0.363514%
[INFO|(OpenDelta)basemodel:704]2024-04-20 15:07:02,041 >> Static Memory 0.48 GB, Max Memory 5.49 GB

 ==> Fitness ( Validation acc ) :   0.58 


 ==> Delta parameters :   [{'insert_modules': ['output'], 'bottleneck_dim': [64, 128], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [16, 24], 'non_linearity': 'relu'}, 0, 0, 0, {'insert_modules': ['output', 'attention.self', 'attention'], 'bottleneck_dim': [16, 64, 64], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.541: 100%|█████████▉| 408/410 [04:10<00:01,  1.63it/s]
epoch 0 loss 0.689  acc 0.541: 100%|█████████▉| 408/410 [04:11<00:01,  1.63it/s]
epoch 0 loss 0.689  acc 0.541: 100%|█████████▉| 409/410 [04:11<00:00,  1.62it/s]
epoch 0 loss 0.689  acc 0.541: 100%|█████████▉| 409/410 [04:11<00:00,  1.62it/s]
epoch 0 loss 0.689  acc 0.541: 100%|██████████| 410/410 [04:11<00:00,  1.76it/s]
epoch 0 loss 0.689  acc 0.541: 100%|██████████| 410/410 [04:11<00:00,  1.63it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.678  acc 0.565: 100%|█████████▉| 408/410 [02:09<00:00,  3.50it/s]
epoch 1 loss 0.678  acc 0.564: 100%|█████████▉| 408/410 [02:09<00:00,  3.50it/s]
epoch 1 loss 0.678  acc 0.564: 100%|█████████▉| 409/410 [02:09<00:00,  3.50it/s]
epoch 1 loss 0.678  acc 0.564: 100%|█████████▉| 409/410 [02:09<00:00,  3.50it/s]
epoch 1 loss 0.678  acc 0.564: 100%|██████████| 410/410 [02:09<00:00,  3.78it/s]
epoch 1 loss 0.678  acc 0.564: 100%|██████████| 410/410 [02:09<00:00,  3.16it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.67  acc 0.573: 100%|█████████▉| 408/410 [01:57<00:00,  3.47it/s]
epoch 2 loss 0.67  acc 0.574: 100%|█████████▉| 408/410 [01:57<00:00,  3.47it/s]
epoch 2 loss 0.67  acc 0.574: 100%|█████████▉| 409/410 [01:57<00:00,  3.47it/s]
epoch 2 loss 0.67  acc 0.573: 100%|█████████▉| 409/410 [01:57<00:00,  3.47it/s]
epoch 2 loss 0.67  acc 0.573: 100%|██████████| 410/410 [01:57<00:00,  3.75it/s]
epoch 2 loss 0.67  acc 0.573: 100%|██████████| 410/410 [01:57<00:00,  3.48it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.663  acc 0.594: 100%|█████████▉| 408/410 [01:57<00:00,  3.48it/s]
epoch 3 loss 0.663  acc 0.593: 100%|█████████▉| 408/410 [01:57<00:00,  3.48it/s]
epoch 3 loss 0.663  acc 0.593: 100%|█████████▉| 409/410 [01:58<00:00,  3.48it/s]
epoch 3 loss 0.663  acc 0.593: 100%|█████████▉| 409/410 [01:58<00:00,  3.48it/s]
epoch 3 loss 0.663  acc 0.593: 100%|██████████| 410/410 [01:58<00:00,  3.76it/s]
epoch 3 loss 0.663  acc 0.593: 100%|██████████| 410/410 [01:58<00:00,  3.47it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.659  acc 0.597: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 4 loss 0.66  acc 0.596: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s] 
epoch 4 loss 0.66  acc 0.596: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 4 loss 0.66  acc 0.596: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 4 loss 0.66  acc 0.596: 100%|██████████| 410/410 [01:57<00:00,  3.78it/s]
epoch 4 loss 0.66  acc 0.596: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 15:20:17,227 >> Trainable Ratio: 3680921/127735961=2.881664%
[INFO|(OpenDelta)basemodel:702]2024-04-20 15:20:17,227 >> Delta Parameter Ratio: 3089560/127735961=2.418708%
[INFO|(OpenDelta)basemodel:704]2024-04-20 15:20:17,227 >> Static Memory 0.48 GB, Max Memory 8.45 GB

 ==> Fitness ( Validation acc ) :   0.578 


 ==> Delta parameters :   [{'insert_modules': ['attention', 'output', 'attention.self'], 'bottleneck_dim': [64, 16, 24], 'non_linearity': 'relu'}, 0, {'insert_modules': ['intermediate', 'output', 'attention.self'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [24, 24], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention', 'intermediate'], 'bottleneck_dim': [128, 64], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention.self', 'intermediate'], 'bottleneck_dim': [24, 24, 128], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'attention', 'output'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [16, 24, 128], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention.self', 'attention'], 'bottleneck_dim': [64, 64, 32], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [32, 32], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention.self', 'attention'], 'bottleneck_dim': [128, 128, 24], 'non_linearity': 'relu'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.543: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 0 loss 0.689  acc 0.543: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 0 loss 0.689  acc 0.543: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 0 loss 0.689  acc 0.543: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 0 loss 0.689  acc 0.543: 100%|██████████| 410/410 [02:10<00:00,  3.38it/s]
epoch 0 loss 0.689  acc 0.543: 100%|██████████| 410/410 [02:10<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.676  acc 0.57: 100%|█████████▉| 408/410 [02:10<00:00,  3.16it/s]
epoch 1 loss 0.676  acc 0.571: 100%|█████████▉| 408/410 [02:10<00:00,  3.16it/s]
epoch 1 loss 0.676  acc 0.571: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 1 loss 0.676  acc 0.571: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 1 loss 0.676  acc 0.571: 100%|██████████| 410/410 [02:10<00:00,  3.41it/s]
epoch 1 loss 0.676  acc 0.571: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.663  acc 0.587: 100%|█████████▉| 408/410 [02:08<00:00,  3.17it/s]
epoch 2 loss 0.664  acc 0.587: 100%|█████████▉| 408/410 [02:09<00:00,  3.17it/s]
epoch 2 loss 0.664  acc 0.587: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 2 loss 0.664  acc 0.587: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 2 loss 0.664  acc 0.587: 100%|██████████| 410/410 [02:09<00:00,  3.43it/s]
epoch 2 loss 0.664  acc 0.587: 100%|██████████| 410/410 [02:09<00:00,  3.17it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.654  acc 0.606: 100%|█████████▉| 408/410 [02:08<00:00,  3.16it/s]
epoch 3 loss 0.654  acc 0.606: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 3 loss 0.654  acc 0.606: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 3 loss 0.654  acc 0.606: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 3 loss 0.654  acc 0.606: 100%|██████████| 410/410 [02:09<00:00,  3.41it/s]
epoch 3 loss 0.654  acc 0.606: 100%|██████████| 410/410 [02:09<00:00,  3.16it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.643  acc 0.613: 100%|█████████▉| 408/410 [02:08<00:00,  3.17it/s]
epoch 4 loss 0.643  acc 0.613: 100%|█████████▉| 408/410 [02:09<00:00,  3.17it/s]
epoch 4 loss 0.643  acc 0.613: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 4 loss 0.643  acc 0.612: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 4 loss 0.643  acc 0.612: 100%|██████████| 410/410 [02:09<00:00,  3.43it/s]
epoch 4 loss 0.643  acc 0.612: 100%|██████████| 410/410 [02:09<00:00,  3.17it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 15:32:01,190 >> Trainable Ratio: 1792513/125847553=1.424353%
[INFO|(OpenDelta)basemodel:702]2024-04-20 15:32:01,190 >> Delta Parameter Ratio: 1201152/125847553=0.954450%
[INFO|(OpenDelta)basemodel:704]2024-04-20 15:32:01,190 >> Static Memory 0.96 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.607 


 ==> Delta parameters :   [{'insert_modules': ['attention'], 'bottleneck_dim': [128, 128], 'non_linearity': 'relu'}, 0, {'insert_modules': ['intermediate', 'attention'], 'bottleneck_dim': [128], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.544: 100%|█████████▉| 408/410 [01:59<00:00,  3.40it/s]
epoch 0 loss 0.689  acc 0.544: 100%|█████████▉| 408/410 [01:59<00:00,  3.40it/s]
epoch 0 loss 0.689  acc 0.544: 100%|█████████▉| 409/410 [01:59<00:00,  3.40it/s]
epoch 0 loss 0.689  acc 0.545: 100%|█████████▉| 409/410 [02:00<00:00,  3.40it/s]
epoch 0 loss 0.689  acc 0.545: 100%|██████████| 410/410 [02:00<00:00,  3.68it/s]
epoch 0 loss 0.689  acc 0.545: 100%|██████████| 410/410 [02:00<00:00,  3.41it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.672  acc 0.573: 100%|█████████▉| 408/410 [01:58<00:00,  3.43it/s]
epoch 1 loss 0.673  acc 0.573: 100%|█████████▉| 408/410 [01:58<00:00,  3.43it/s]
epoch 1 loss 0.673  acc 0.573: 100%|█████████▉| 409/410 [01:58<00:00,  3.44it/s]
epoch 1 loss 0.673  acc 0.573: 100%|█████████▉| 409/410 [01:59<00:00,  3.44it/s]
epoch 1 loss 0.673  acc 0.573: 100%|██████████| 410/410 [01:59<00:00,  3.72it/s]
epoch 1 loss 0.673  acc 0.573: 100%|██████████| 410/410 [01:59<00:00,  3.44it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.666  acc 0.584: 100%|█████████▉| 408/410 [01:58<00:00,  3.45it/s]
epoch 2 loss 0.666  acc 0.585: 100%|█████████▉| 408/410 [01:58<00:00,  3.45it/s]
epoch 2 loss 0.666  acc 0.585: 100%|█████████▉| 409/410 [01:58<00:00,  3.45it/s]
epoch 2 loss 0.666  acc 0.585: 100%|█████████▉| 409/410 [01:59<00:00,  3.45it/s]
epoch 2 loss 0.666  acc 0.585: 100%|██████████| 410/410 [01:59<00:00,  3.73it/s]
epoch 2 loss 0.666  acc 0.585: 100%|██████████| 410/410 [01:59<00:00,  3.44it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.653  acc 0.606: 100%|█████████▉| 408/410 [01:58<00:00,  3.45it/s]
epoch 3 loss 0.653  acc 0.607: 100%|█████████▉| 408/410 [01:58<00:00,  3.45it/s]
epoch 3 loss 0.653  acc 0.607: 100%|█████████▉| 409/410 [01:58<00:00,  3.45it/s]
epoch 3 loss 0.653  acc 0.607: 100%|█████████▉| 409/410 [01:59<00:00,  3.45it/s]
epoch 3 loss 0.653  acc 0.607: 100%|██████████| 410/410 [01:59<00:00,  3.73it/s]
epoch 3 loss 0.653  acc 0.607: 100%|██████████| 410/410 [01:59<00:00,  3.44it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.642  acc 0.624: 100%|█████████▉| 408/410 [01:58<00:00,  3.45it/s]
epoch 4 loss 0.642  acc 0.624: 100%|█████████▉| 408/410 [01:58<00:00,  3.45it/s]
epoch 4 loss 0.642  acc 0.624: 100%|█████████▉| 409/410 [01:58<00:00,  3.45it/s]
epoch 4 loss 0.642  acc 0.624: 100%|█████████▉| 409/410 [01:59<00:00,  3.45it/s]
epoch 4 loss 0.642  acc 0.624: 100%|██████████| 410/410 [01:59<00:00,  3.73it/s]
epoch 4 loss 0.642  acc 0.624: 100%|██████████| 410/410 [01:59<00:00,  3.44it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 15:42:50,395 >> Trainable Ratio: 4064153/128119193=3.172166%
[INFO|(OpenDelta)basemodel:702]2024-04-20 15:42:50,395 >> Delta Parameter Ratio: 3472792/128119193=2.710595%
[INFO|(OpenDelta)basemodel:704]2024-04-20 15:42:50,395 >> Static Memory 0.49 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.592 


 ==> Delta parameters :   [0, {'insert_modules': ['intermediate', 'output', 'attention.self'], 'bottleneck_dim': [128, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'output'], 'bottleneck_dim': [24, 24, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'attention.self', 'output'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate'], 'bottleneck_dim': [64, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [128, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'attention.self', 'intermediate'], 'bottleneck_dim': [24, 128, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.691  acc 0.529: 100%|█████████▉| 408/410 [02:05<00:00,  3.25it/s]
epoch 0 loss 0.691  acc 0.528: 100%|█████████▉| 408/410 [02:06<00:00,  3.25it/s]
epoch 0 loss 0.691  acc 0.528: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 0 loss 0.691  acc 0.529: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 0 loss 0.691  acc 0.529: 100%|██████████| 410/410 [02:06<00:00,  3.52it/s]
epoch 0 loss 0.691  acc 0.529: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.674  acc 0.576: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 1 loss 0.674  acc 0.576: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 1 loss 0.674  acc 0.576: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 1 loss 0.674  acc 0.576: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 1 loss 0.674  acc 0.576: 100%|██████████| 410/410 [02:05<00:00,  3.53it/s]
epoch 1 loss 0.674  acc 0.576: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.66  acc 0.585: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 2 loss 0.661  acc 0.585: 100%|█████████▉| 408/410 [02:05<00:00,  3.28it/s]
epoch 2 loss 0.661  acc 0.585: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 2 loss 0.66  acc 0.585: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s] 
epoch 2 loss 0.66  acc 0.585: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 2 loss 0.66  acc 0.585: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.639  acc 0.615: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 3 loss 0.639  acc 0.615: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 3 loss 0.639  acc 0.615: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 3 loss 0.639  acc 0.614: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 3 loss 0.639  acc 0.614: 100%|██████████| 410/410 [02:05<00:00,  3.53it/s]
epoch 3 loss 0.639  acc 0.614: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.624  acc 0.635: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.624  acc 0.636: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.624  acc 0.636: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.624  acc 0.635: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.624  acc 0.635: 100%|██████████| 410/410 [02:05<00:00,  3.53it/s]
epoch 4 loss 0.624  acc 0.635: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 15:54:13,023 >> Trainable Ratio: 1562809/125617849=1.244098%
[INFO|(OpenDelta)basemodel:702]2024-04-20 15:54:13,023 >> Delta Parameter Ratio: 971448/125617849=0.773336%
[INFO|(OpenDelta)basemodel:704]2024-04-20 15:54:13,023 >> Static Memory 0.97 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.617 


 ==> Delta parameters :   [{'insert_modules': ['attention', 'attention.self'], 'bottleneck_dim': [16, 24], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, 0, 0, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [128, 128, 32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'attention', 'output'], 'bottleneck_dim': [24, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'output', 'intermediate'], 'bottleneck_dim': [16], 'non_linearity': 'gelu_new'}, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.534: 100%|█████████▉| 408/410 [02:03<00:00,  3.31it/s]
epoch 0 loss 0.69  acc 0.535: 100%|█████████▉| 408/410 [02:03<00:00,  3.31it/s]
epoch 0 loss 0.69  acc 0.535: 100%|█████████▉| 409/410 [02:03<00:00,  3.31it/s]
epoch 0 loss 0.689  acc 0.535: 100%|█████████▉| 409/410 [02:03<00:00,  3.31it/s]
epoch 0 loss 0.689  acc 0.535: 100%|██████████| 410/410 [02:03<00:00,  3.58it/s]
epoch 0 loss 0.689  acc 0.535: 100%|██████████| 410/410 [02:03<00:00,  3.31it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.678  acc 0.564: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 1 loss 0.678  acc 0.564: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 1 loss 0.678  acc 0.564: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 1 loss 0.678  acc 0.564: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 1 loss 0.678  acc 0.564: 100%|██████████| 410/410 [02:02<00:00,  3.62it/s]
epoch 1 loss 0.678  acc 0.564: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.671  acc 0.58: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 2 loss 0.671  acc 0.58: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 2 loss 0.671  acc 0.58: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 2 loss 0.671  acc 0.58: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 2 loss 0.671  acc 0.58: 100%|██████████| 410/410 [02:02<00:00,  3.61it/s]
epoch 2 loss 0.671  acc 0.58: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.659  acc 0.598: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.659  acc 0.598: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.659  acc 0.598: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 3 loss 0.659  acc 0.598: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 3 loss 0.659  acc 0.598: 100%|██████████| 410/410 [02:02<00:00,  3.62it/s]
epoch 3 loss 0.659  acc 0.598: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.65  acc 0.602: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 4 loss 0.65  acc 0.602: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 4 loss 0.65  acc 0.602: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.65  acc 0.602: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.65  acc 0.602: 100%|██████████| 410/410 [02:02<00:00,  3.62it/s]
epoch 4 loss 0.65  acc 0.602: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 16:05:20,093 >> Trainable Ratio: 1889433/125944473=1.500211%
[INFO|(OpenDelta)basemodel:702]2024-04-20 16:05:20,094 >> Delta Parameter Ratio: 1298072/125944473=1.030670%
[INFO|(OpenDelta)basemodel:704]2024-04-20 16:05:20,094 >> Static Memory 0.48 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.593 


 ==> Delta parameters :   [0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [32, 64, 32], 'non_linearity': 'relu'}, 0, {'insert_modules': ['output', 'intermediate', 'attention'], 'bottleneck_dim': [64, 24, 24], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'intermediate', 'attention.self'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, 0, 0, 0, 0, {'insert_modules': ['attention', 'attention.self', 'output'], 'bottleneck_dim': [24, 128, 128], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [16, 128], 'non_linearity': 'relu'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.539: 100%|█████████▉| 408/410 [01:51<00:00,  3.62it/s]
epoch 0 loss 0.689  acc 0.539: 100%|█████████▉| 408/410 [01:52<00:00,  3.62it/s]
epoch 0 loss 0.689  acc 0.539: 100%|█████████▉| 409/410 [01:52<00:00,  3.62it/s]
epoch 0 loss 0.689  acc 0.539: 100%|█████████▉| 409/410 [01:52<00:00,  3.62it/s]
epoch 0 loss 0.689  acc 0.539: 100%|██████████| 410/410 [01:52<00:00,  3.92it/s]
epoch 0 loss 0.689  acc 0.539: 100%|██████████| 410/410 [01:52<00:00,  3.65it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.676  acc 0.564: 100%|█████████▉| 408/410 [01:51<00:00,  3.67it/s]
epoch 1 loss 0.676  acc 0.564: 100%|█████████▉| 408/410 [01:51<00:00,  3.67it/s]
epoch 1 loss 0.676  acc 0.564: 100%|█████████▉| 409/410 [01:51<00:00,  3.67it/s]
epoch 1 loss 0.676  acc 0.565: 100%|█████████▉| 409/410 [01:51<00:00,  3.67it/s]
epoch 1 loss 0.676  acc 0.565: 100%|██████████| 410/410 [01:51<00:00,  3.97it/s]
epoch 1 loss 0.676  acc 0.565: 100%|██████████| 410/410 [01:51<00:00,  3.67it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.667  acc 0.581: 100%|█████████▉| 408/410 [01:50<00:00,  3.69it/s]
epoch 2 loss 0.667  acc 0.582: 100%|█████████▉| 408/410 [01:51<00:00,  3.69it/s]
epoch 2 loss 0.667  acc 0.582: 100%|█████████▉| 409/410 [01:51<00:00,  3.68it/s]
epoch 2 loss 0.667  acc 0.581: 100%|█████████▉| 409/410 [01:51<00:00,  3.68it/s]
epoch 2 loss 0.667  acc 0.581: 100%|██████████| 410/410 [01:51<00:00,  3.98it/s]
epoch 2 loss 0.667  acc 0.581: 100%|██████████| 410/410 [01:51<00:00,  3.68it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.659  acc 0.597: 100%|█████████▉| 408/410 [01:50<00:00,  3.68it/s]
epoch 3 loss 0.659  acc 0.597: 100%|█████████▉| 408/410 [01:51<00:00,  3.68it/s]
epoch 3 loss 0.659  acc 0.597: 100%|█████████▉| 409/410 [01:51<00:00,  3.68it/s]
epoch 3 loss 0.659  acc 0.598: 100%|█████████▉| 409/410 [01:51<00:00,  3.68it/s]
epoch 3 loss 0.659  acc 0.598: 100%|██████████| 410/410 [01:51<00:00,  3.98it/s]
epoch 3 loss 0.659  acc 0.598: 100%|██████████| 410/410 [01:51<00:00,  3.68it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.653  acc 0.598: 100%|█████████▉| 408/410 [01:50<00:00,  3.69it/s]
epoch 4 loss 0.653  acc 0.599: 100%|█████████▉| 408/410 [01:51<00:00,  3.69it/s]
epoch 4 loss 0.653  acc 0.599: 100%|█████████▉| 409/410 [01:51<00:00,  3.69it/s]
epoch 4 loss 0.653  acc 0.599: 100%|█████████▉| 409/410 [01:51<00:00,  3.69it/s]
epoch 4 loss 0.653  acc 0.599: 100%|██████████| 410/410 [01:51<00:00,  3.99it/s]
epoch 4 loss 0.653  acc 0.599: 100%|██████████| 410/410 [01:51<00:00,  3.68it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 16:15:31,055 >> Trainable Ratio: 1588513/125643553=1.264301%
[INFO|(OpenDelta)basemodel:702]2024-04-20 16:15:31,055 >> Delta Parameter Ratio: 997152/125643553=0.793636%
[INFO|(OpenDelta)basemodel:704]2024-04-20 16:15:31,055 >> Static Memory 0.49 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.584 


 ==> Delta parameters :   [{'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['output'], 'bottleneck_dim': [32, 128, 128], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['attention.self'], 'bottleneck_dim': [16, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [16], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['intermediate', 'output', 'attention'], 'bottleneck_dim': [32, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention'], 'bottleneck_dim': [24, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.535: 100%|█████████▉| 408/410 [02:03<00:00,  3.30it/s]
epoch 0 loss 0.689  acc 0.535: 100%|█████████▉| 408/410 [02:04<00:00,  3.30it/s]
epoch 0 loss 0.689  acc 0.535: 100%|█████████▉| 409/410 [02:04<00:00,  3.30it/s]
epoch 0 loss 0.689  acc 0.536: 100%|█████████▉| 409/410 [02:04<00:00,  3.30it/s]
epoch 0 loss 0.689  acc 0.536: 100%|██████████| 410/410 [02:04<00:00,  3.57it/s]
epoch 0 loss 0.689  acc 0.536: 100%|██████████| 410/410 [02:04<00:00,  3.30it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.675  acc 0.571: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.675  acc 0.571: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.675  acc 0.571: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 1 loss 0.675  acc 0.571: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 1 loss 0.675  acc 0.571: 100%|██████████| 410/410 [02:02<00:00,  3.62it/s]
epoch 1 loss 0.675  acc 0.571: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.665  acc 0.586: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 2 loss 0.665  acc 0.586: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 2 loss 0.665  acc 0.586: 100%|█████████▉| 409/410 [02:02<00:00,  3.33it/s]
epoch 2 loss 0.665  acc 0.586: 100%|█████████▉| 409/410 [02:02<00:00,  3.33it/s]
epoch 2 loss 0.665  acc 0.586: 100%|██████████| 410/410 [02:02<00:00,  3.60it/s]
epoch 2 loss 0.665  acc 0.586: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.654  acc 0.6: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 3 loss 0.654  acc 0.6: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 3 loss 0.654  acc 0.6: 100%|█████████▉| 409/410 [02:02<00:00,  3.33it/s]
epoch 3 loss 0.654  acc 0.6: 100%|█████████▉| 409/410 [02:03<00:00,  3.33it/s]
epoch 3 loss 0.654  acc 0.6: 100%|██████████| 410/410 [02:03<00:00,  3.59it/s]
epoch 3 loss 0.654  acc 0.6: 100%|██████████| 410/410 [02:03<00:00,  3.33it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.645  acc 0.616: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 4 loss 0.645  acc 0.616: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 4 loss 0.645  acc 0.616: 100%|█████████▉| 409/410 [02:02<00:00,  3.33it/s]
epoch 4 loss 0.645  acc 0.615: 100%|█████████▉| 409/410 [02:03<00:00,  3.33it/s]
epoch 4 loss 0.645  acc 0.615: 100%|██████████| 410/410 [02:03<00:00,  3.60it/s]
epoch 4 loss 0.645  acc 0.615: 100%|██████████| 410/410 [02:03<00:00,  3.33it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 16:26:39,617 >> Trainable Ratio: 1284353/125339393=1.024700%
[INFO|(OpenDelta)basemodel:702]2024-04-20 16:26:39,617 >> Delta Parameter Ratio: 692992/125339393=0.552892%
[INFO|(OpenDelta)basemodel:704]2024-04-20 16:26:39,617 >> Static Memory 0.96 GB, Max Memory 9.36 GB
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 16:26:41,156 >> Trainable Ratio: 4151961/128207001=3.238482%
[INFO|(OpenDelta)basemodel:702]2024-04-20 16:26:41,156 >> Delta Parameter Ratio: 3560600/128207001=2.777227%
[INFO|(OpenDelta)basemodel:704]2024-04-20 16:26:41,156 >> Static Memory 1.42 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.585 


 ==> Delta parameters :   [0, 0, 0, 0, {'insert_modules': ['intermediate', 'attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention.self'], 'bottleneck_dim': [128], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0] 


 ==> Delta parameters :   [{'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [24, 128, 32], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['attention'], 'bottleneck_dim': [16, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'attention.self'], 'bottleneck_dim': [32, 64, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'intermediate', 'attention.self'], 'bottleneck_dim': [16, 32], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['intermediate', 'attention.self', 'output'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [24, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention', 'attention.self'], 'bottleneck_dim': [128, 64, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'intermediate', 'attention'], 'bottleneck_dim': [24, 32], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.54: 100%|█████████▉| 408/410 [02:11<00:00,  3.10it/s]
epoch 0 loss 0.69  acc 0.54: 100%|█████████▉| 408/410 [02:12<00:00,  3.10it/s]
epoch 0 loss 0.69  acc 0.54: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 0 loss 0.69  acc 0.539: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 0 loss 0.69  acc 0.539: 100%|██████████| 410/410 [02:12<00:00,  3.36it/s]
epoch 0 loss 0.69  acc 0.539: 100%|██████████| 410/410 [02:12<00:00,  3.10it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.673  acc 0.561: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 1 loss 0.673  acc 0.561: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 1 loss 0.673  acc 0.561: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 1 loss 0.674  acc 0.56: 100%|█████████▉| 409/410 [02:11<00:00,  3.14it/s] 
epoch 1 loss 0.674  acc 0.56: 100%|██████████| 410/410 [02:11<00:00,  3.39it/s]
epoch 1 loss 0.674  acc 0.56: 100%|██████████| 410/410 [02:11<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.661  acc 0.588: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 2 loss 0.661  acc 0.588: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 2 loss 0.661  acc 0.588: 100%|█████████▉| 409/410 [02:10<00:00,  3.12it/s]
epoch 2 loss 0.661  acc 0.588: 100%|█████████▉| 409/410 [02:10<00:00,  3.12it/s]
epoch 2 loss 0.661  acc 0.588: 100%|██████████| 410/410 [02:10<00:00,  3.37it/s]
epoch 2 loss 0.661  acc 0.588: 100%|██████████| 410/410 [02:10<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.645  acc 0.618: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.645  acc 0.618: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.645  acc 0.618: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.645  acc 0.618: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 3 loss 0.645  acc 0.618: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 3 loss 0.645  acc 0.618: 100%|██████████| 410/410 [02:11<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.626  acc 0.633: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.626  acc 0.633: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.626  acc 0.633: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.626  acc 0.634: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 4 loss 0.626  acc 0.634: 100%|██████████| 410/410 [02:11<00:00,  3.37it/s]
epoch 4 loss 0.626  acc 0.634: 100%|██████████| 410/410 [02:11<00:00,  3.13it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 16:38:32,337 >> Trainable Ratio: 1017409/125072449=0.813456%
[INFO|(OpenDelta)basemodel:702]2024-04-20 16:38:32,337 >> Delta Parameter Ratio: 426048/125072449=0.340641%
[INFO|(OpenDelta)basemodel:704]2024-04-20 16:38:32,337 >> Static Memory 0.49 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.62 


 ==> Delta parameters :   [0, 0, 0, 0, 0, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [128, 16, 32], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.531: 100%|█████████▉| 408/410 [01:33<00:00,  4.35it/s]
epoch 0 loss 0.69  acc 0.531: 100%|█████████▉| 408/410 [01:33<00:00,  4.35it/s]
epoch 0 loss 0.69  acc 0.531: 100%|█████████▉| 409/410 [01:33<00:00,  4.35it/s]
epoch 0 loss 0.69  acc 0.532: 100%|█████████▉| 409/410 [01:33<00:00,  4.35it/s]
epoch 0 loss 0.69  acc 0.532: 100%|██████████| 410/410 [01:33<00:00,  4.70it/s]
epoch 0 loss 0.69  acc 0.532: 100%|██████████| 410/410 [01:33<00:00,  4.38it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.677  acc 0.563: 100%|█████████▉| 408/410 [01:33<00:00,  4.41it/s]
epoch 1 loss 0.677  acc 0.563: 100%|█████████▉| 408/410 [01:33<00:00,  4.41it/s]
epoch 1 loss 0.677  acc 0.563: 100%|█████████▉| 409/410 [01:33<00:00,  4.41it/s]
epoch 1 loss 0.677  acc 0.563: 100%|█████████▉| 409/410 [01:33<00:00,  4.41it/s]
epoch 1 loss 0.677  acc 0.563: 100%|██████████| 410/410 [01:33<00:00,  4.76it/s]
epoch 1 loss 0.677  acc 0.563: 100%|██████████| 410/410 [01:33<00:00,  4.38it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.665  acc 0.581: 100%|█████████▉| 408/410 [01:32<00:00,  4.41it/s]
epoch 2 loss 0.665  acc 0.581: 100%|█████████▉| 408/410 [01:32<00:00,  4.41it/s]
epoch 2 loss 0.665  acc 0.581: 100%|█████████▉| 409/410 [01:32<00:00,  4.41it/s]
epoch 2 loss 0.665  acc 0.581: 100%|█████████▉| 409/410 [01:32<00:00,  4.41it/s]
epoch 2 loss 0.665  acc 0.581: 100%|██████████| 410/410 [01:32<00:00,  4.77it/s]
epoch 2 loss 0.665  acc 0.581: 100%|██████████| 410/410 [01:32<00:00,  4.41it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.659  acc 0.591: 100%|█████████▉| 408/410 [01:32<00:00,  4.41it/s]
epoch 3 loss 0.659  acc 0.591: 100%|█████████▉| 408/410 [01:32<00:00,  4.41it/s]
epoch 3 loss 0.659  acc 0.591: 100%|█████████▉| 409/410 [01:32<00:00,  4.41it/s]
epoch 3 loss 0.658  acc 0.592: 100%|█████████▉| 409/410 [01:32<00:00,  4.41it/s]
epoch 3 loss 0.658  acc 0.592: 100%|██████████| 410/410 [01:32<00:00,  4.77it/s]
epoch 3 loss 0.658  acc 0.592: 100%|██████████| 410/410 [01:32<00:00,  4.41it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.655  acc 0.6: 100%|█████████▉| 408/410 [01:32<00:00,  4.42it/s]
epoch 4 loss 0.655  acc 0.6: 100%|█████████▉| 408/410 [01:32<00:00,  4.42it/s]
epoch 4 loss 0.655  acc 0.6: 100%|█████████▉| 409/410 [01:32<00:00,  4.42it/s]
epoch 4 loss 0.655  acc 0.6: 100%|█████████▉| 409/410 [01:32<00:00,  4.42it/s]
epoch 4 loss 0.655  acc 0.6: 100%|██████████| 410/410 [01:32<00:00,  4.77it/s]
epoch 4 loss 0.655  acc 0.6: 100%|██████████| 410/410 [01:32<00:00,  4.41it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 16:47:10,071 >> Trainable Ratio: 1400993/125456033=1.116720%
[INFO|(OpenDelta)basemodel:702]2024-04-20 16:47:10,071 >> Delta Parameter Ratio: 809632/125456033=0.645351%
[INFO|(OpenDelta)basemodel:704]2024-04-20 16:47:10,071 >> Static Memory 0.48 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.591 


 ==> Delta parameters :   [0, {'insert_modules': ['attention.self', 'attention', 'output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, 0, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [128, 128, 32], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.687  acc 0.55: 100%|█████████▉| 408/410 [01:55<00:00,  3.55it/s]
epoch 0 loss 0.687  acc 0.55: 100%|█████████▉| 408/410 [01:55<00:00,  3.55it/s]
epoch 0 loss 0.687  acc 0.55: 100%|█████████▉| 409/410 [01:55<00:00,  3.56it/s]
epoch 0 loss 0.687  acc 0.549: 100%|█████████▉| 409/410 [01:55<00:00,  3.56it/s]
epoch 0 loss 0.687  acc 0.549: 100%|██████████| 410/410 [01:55<00:00,  3.84it/s]
epoch 0 loss 0.687  acc 0.549: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.675  acc 0.576: 100%|█████████▉| 408/410 [01:54<00:00,  3.57it/s]
epoch 1 loss 0.675  acc 0.575: 100%|█████████▉| 408/410 [01:54<00:00,  3.57it/s]
epoch 1 loss 0.675  acc 0.575: 100%|█████████▉| 409/410 [01:54<00:00,  3.57it/s]
epoch 1 loss 0.675  acc 0.575: 100%|█████████▉| 409/410 [01:54<00:00,  3.57it/s]
epoch 1 loss 0.675  acc 0.575: 100%|██████████| 410/410 [01:54<00:00,  3.86it/s]
epoch 1 loss 0.675  acc 0.575: 100%|██████████| 410/410 [01:54<00:00,  3.57it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.664  acc 0.585: 100%|█████████▉| 408/410 [01:54<00:00,  3.58it/s]
epoch 2 loss 0.664  acc 0.585: 100%|█████████▉| 408/410 [01:54<00:00,  3.58it/s]
epoch 2 loss 0.664  acc 0.585: 100%|█████████▉| 409/410 [01:54<00:00,  3.57it/s]
epoch 2 loss 0.664  acc 0.585: 100%|█████████▉| 409/410 [01:54<00:00,  3.57it/s]
epoch 2 loss 0.664  acc 0.585: 100%|██████████| 410/410 [01:54<00:00,  3.86it/s]
epoch 2 loss 0.664  acc 0.585: 100%|██████████| 410/410 [01:54<00:00,  3.57it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.66  acc 0.586: 100%|█████████▉| 408/410 [01:54<00:00,  3.57it/s]
epoch 3 loss 0.66  acc 0.587: 100%|█████████▉| 408/410 [01:54<00:00,  3.57it/s]
epoch 3 loss 0.66  acc 0.587: 100%|█████████▉| 409/410 [01:54<00:00,  3.57it/s]
epoch 3 loss 0.66  acc 0.586: 100%|█████████▉| 409/410 [01:54<00:00,  3.57it/s]
epoch 3 loss 0.66  acc 0.586: 100%|██████████| 410/410 [01:54<00:00,  3.85it/s]
epoch 3 loss 0.66  acc 0.586: 100%|██████████| 410/410 [01:54<00:00,  3.57it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.656  acc 0.594: 100%|█████████▉| 408/410 [01:54<00:00,  3.56it/s]
epoch 4 loss 0.656  acc 0.594: 100%|█████████▉| 408/410 [01:54<00:00,  3.56it/s]
epoch 4 loss 0.656  acc 0.594: 100%|█████████▉| 409/410 [01:54<00:00,  3.56it/s]
epoch 4 loss 0.656  acc 0.594: 100%|█████████▉| 409/410 [01:54<00:00,  3.56it/s]
epoch 4 loss 0.656  acc 0.594: 100%|██████████| 410/410 [01:54<00:00,  3.85it/s]
epoch 4 loss 0.656  acc 0.594: 100%|██████████| 410/410 [01:54<00:00,  3.57it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 16:57:37,817 >> Trainable Ratio: 1168761/125223801=0.933338%
[INFO|(OpenDelta)basemodel:702]2024-04-20 16:57:37,817 >> Delta Parameter Ratio: 577400/125223801=0.461094%
[INFO|(OpenDelta)basemodel:704]2024-04-20 16:57:37,817 >> Static Memory 0.95 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.591 


 ==> Delta parameters :   [0, 0, {'insert_modules': ['attention', 'output'], 'bottleneck_dim': [24, 24], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention', 'attention.self', 'output'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, 0, 0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [32], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.686  acc 0.544: 100%|█████████▉| 408/410 [01:50<00:00,  3.66it/s]
epoch 0 loss 0.686  acc 0.545: 100%|█████████▉| 408/410 [01:50<00:00,  3.66it/s]
epoch 0 loss 0.686  acc 0.545: 100%|█████████▉| 409/410 [01:50<00:00,  3.66it/s]
epoch 0 loss 0.686  acc 0.545: 100%|█████████▉| 409/410 [01:51<00:00,  3.66it/s]
epoch 0 loss 0.686  acc 0.545: 100%|██████████| 410/410 [01:51<00:00,  3.97it/s]
epoch 0 loss 0.686  acc 0.545: 100%|██████████| 410/410 [01:51<00:00,  3.69it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.674  acc 0.568: 100%|█████████▉| 408/410 [01:49<00:00,  3.74it/s]
epoch 1 loss 0.674  acc 0.568: 100%|█████████▉| 408/410 [01:49<00:00,  3.74it/s]
epoch 1 loss 0.674  acc 0.568: 100%|█████████▉| 409/410 [01:49<00:00,  3.74it/s]
epoch 1 loss 0.674  acc 0.567: 100%|█████████▉| 409/410 [01:50<00:00,  3.74it/s]
epoch 1 loss 0.674  acc 0.567: 100%|██████████| 410/410 [01:50<00:00,  4.04it/s]
epoch 1 loss 0.674  acc 0.567: 100%|██████████| 410/410 [01:50<00:00,  3.72it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.663  acc 0.583: 100%|█████████▉| 408/410 [01:49<00:00,  3.73it/s]
epoch 2 loss 0.663  acc 0.583: 100%|█████████▉| 408/410 [01:49<00:00,  3.73it/s]
epoch 2 loss 0.663  acc 0.583: 100%|█████████▉| 409/410 [01:49<00:00,  3.73it/s]
epoch 2 loss 0.663  acc 0.583: 100%|█████████▉| 409/410 [01:49<00:00,  3.73it/s]
epoch 2 loss 0.663  acc 0.583: 100%|██████████| 410/410 [01:49<00:00,  4.04it/s]
epoch 2 loss 0.663  acc 0.583: 100%|██████████| 410/410 [01:49<00:00,  3.73it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.65  acc 0.608: 100%|█████████▉| 408/410 [01:49<00:00,  3.74it/s]
epoch 3 loss 0.65  acc 0.608: 100%|█████████▉| 408/410 [01:49<00:00,  3.74it/s]
epoch 3 loss 0.65  acc 0.608: 100%|█████████▉| 409/410 [01:49<00:00,  3.74it/s]
epoch 3 loss 0.65  acc 0.607: 100%|█████████▉| 409/410 [01:49<00:00,  3.74it/s]
epoch 3 loss 0.65  acc 0.607: 100%|██████████| 410/410 [01:49<00:00,  4.03it/s]
epoch 3 loss 0.65  acc 0.607: 100%|██████████| 410/410 [01:49<00:00,  3.73it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.644  acc 0.614: 100%|█████████▉| 408/410 [01:49<00:00,  3.73it/s]
epoch 4 loss 0.645  acc 0.614: 100%|█████████▉| 408/410 [01:49<00:00,  3.73it/s]
epoch 4 loss 0.645  acc 0.614: 100%|█████████▉| 409/410 [01:49<00:00,  3.73it/s]
epoch 4 loss 0.644  acc 0.615: 100%|█████████▉| 409/410 [01:49<00:00,  3.73it/s]
epoch 4 loss 0.644  acc 0.615: 100%|██████████| 410/410 [01:49<00:00,  4.03it/s]
epoch 4 loss 0.644  acc 0.615: 100%|██████████| 410/410 [01:49<00:00,  3.73it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 17:07:41,101 >> Trainable Ratio: 2664697/126719737=2.102827%
[INFO|(OpenDelta)basemodel:702]2024-04-20 17:07:41,101 >> Delta Parameter Ratio: 2073336/126719737=1.636159%
[INFO|(OpenDelta)basemodel:704]2024-04-20 17:07:41,101 >> Static Memory 0.48 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.602 


 ==> Delta parameters :   [0, {'insert_modules': ['output'], 'bottleneck_dim': [32, 24], 'non_linearity': 'relu'}, 0, 0, 0, 0, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [128, 128], 'non_linearity': 'relu'}, {'insert_modules': ['attention', 'intermediate', 'attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, 0, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [24, 64], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [128, 32], 'non_linearity': 'relu'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.543: 100%|█████████▉| 408/410 [01:56<00:00,  3.48it/s]
epoch 0 loss 0.69  acc 0.544: 100%|█████████▉| 408/410 [01:56<00:00,  3.48it/s]
epoch 0 loss 0.69  acc 0.544: 100%|█████████▉| 409/410 [01:56<00:00,  3.48it/s]
epoch 0 loss 0.69  acc 0.543: 100%|█████████▉| 409/410 [01:57<00:00,  3.48it/s]
epoch 0 loss 0.69  acc 0.543: 100%|██████████| 410/410 [01:57<00:00,  3.77it/s]
epoch 0 loss 0.69  acc 0.543: 100%|██████████| 410/410 [01:57<00:00,  3.50it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.675  acc 0.574: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 1 loss 0.675  acc 0.574: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 1 loss 0.675  acc 0.574: 100%|█████████▉| 409/410 [01:55<00:00,  3.52it/s]
epoch 1 loss 0.675  acc 0.574: 100%|█████████▉| 409/410 [01:56<00:00,  3.52it/s]
epoch 1 loss 0.675  acc 0.574: 100%|██████████| 410/410 [01:56<00:00,  3.81it/s]
epoch 1 loss 0.675  acc 0.574: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.666  acc 0.576: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 2 loss 0.667  acc 0.575: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 2 loss 0.667  acc 0.575: 100%|█████████▉| 409/410 [01:55<00:00,  3.53it/s]
epoch 2 loss 0.667  acc 0.575: 100%|█████████▉| 409/410 [01:56<00:00,  3.53it/s]
epoch 2 loss 0.667  acc 0.575: 100%|██████████| 410/410 [01:56<00:00,  3.81it/s]
epoch 2 loss 0.667  acc 0.575: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.655  acc 0.605: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 3 loss 0.655  acc 0.605: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 3 loss 0.655  acc 0.605: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 3 loss 0.655  acc 0.605: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 3 loss 0.655  acc 0.605: 100%|██████████| 410/410 [01:55<00:00,  3.82it/s]
epoch 3 loss 0.655  acc 0.605: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.641  acc 0.617: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 4 loss 0.641  acc 0.617: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 4 loss 0.641  acc 0.617: 100%|█████████▉| 409/410 [01:55<00:00,  3.53it/s]
epoch 4 loss 0.641  acc 0.617: 100%|█████████▉| 409/410 [01:56<00:00,  3.53it/s]
epoch 4 loss 0.641  acc 0.617: 100%|██████████| 410/410 [01:56<00:00,  3.81it/s]
epoch 4 loss 0.641  acc 0.617: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 17:18:15,437 >> Trainable Ratio: 1386577/125441617=1.105356%
[INFO|(OpenDelta)basemodel:702]2024-04-20 17:18:15,437 >> Delta Parameter Ratio: 795216/125441617=0.633933%
[INFO|(OpenDelta)basemodel:704]2024-04-20 17:18:15,437 >> Static Memory 0.96 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.604 


 ==> Delta parameters :   [{'insert_modules': ['attention.self'], 'bottleneck_dim': [16, 128, 32], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [64, 64, 16], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [16, 32, 128], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.536: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s]
epoch 0 loss 0.69  acc 0.537: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s]
epoch 0 loss 0.69  acc 0.537: 100%|█████████▉| 409/410 [02:01<00:00,  3.36it/s]
epoch 0 loss 0.69  acc 0.537: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 0 loss 0.69  acc 0.537: 100%|██████████| 410/410 [02:02<00:00,  3.63it/s]
epoch 0 loss 0.69  acc 0.537: 100%|██████████| 410/410 [02:02<00:00,  3.35it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.677  acc 0.568: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 1 loss 0.677  acc 0.568: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 1 loss 0.677  acc 0.568: 100%|█████████▉| 409/410 [02:00<00:00,  3.39it/s]
epoch 1 loss 0.677  acc 0.569: 100%|█████████▉| 409/410 [02:00<00:00,  3.39it/s]
epoch 1 loss 0.677  acc 0.569: 100%|██████████| 410/410 [02:00<00:00,  3.67it/s]
epoch 1 loss 0.677  acc 0.569: 100%|██████████| 410/410 [02:00<00:00,  3.39it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.668  acc 0.581: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 2 loss 0.668  acc 0.581: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 2 loss 0.668  acc 0.581: 100%|█████████▉| 409/410 [02:00<00:00,  3.41it/s]
epoch 2 loss 0.668  acc 0.581: 100%|█████████▉| 409/410 [02:00<00:00,  3.41it/s]
epoch 2 loss 0.668  acc 0.581: 100%|██████████| 410/410 [02:00<00:00,  3.68it/s]
epoch 2 loss 0.668  acc 0.581: 100%|██████████| 410/410 [02:00<00:00,  3.39it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.663  acc 0.583: 100%|█████████▉| 408/410 [02:00<00:00,  3.39it/s]
epoch 3 loss 0.663  acc 0.583: 100%|█████████▉| 408/410 [02:00<00:00,  3.39it/s]
epoch 3 loss 0.663  acc 0.583: 100%|█████████▉| 409/410 [02:00<00:00,  3.39it/s]
epoch 3 loss 0.662  acc 0.584: 100%|█████████▉| 409/410 [02:00<00:00,  3.39it/s]
epoch 3 loss 0.662  acc 0.584: 100%|██████████| 410/410 [02:00<00:00,  3.67it/s]
epoch 3 loss 0.662  acc 0.584: 100%|██████████| 410/410 [02:00<00:00,  3.39it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.659  acc 0.592: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 4 loss 0.659  acc 0.592: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 4 loss 0.659  acc 0.592: 100%|█████████▉| 409/410 [02:00<00:00,  3.39it/s]
epoch 4 loss 0.659  acc 0.592: 100%|█████████▉| 409/410 [02:00<00:00,  3.39it/s]
epoch 4 loss 0.659  acc 0.592: 100%|██████████| 410/410 [02:00<00:00,  3.67it/s]
epoch 4 loss 0.659  acc 0.592: 100%|██████████| 410/410 [02:00<00:00,  3.39it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 17:29:13,595 >> Trainable Ratio: 3223073/127278113=2.532307%
[INFO|(OpenDelta)basemodel:702]2024-04-20 17:29:13,595 >> Delta Parameter Ratio: 2631712/127278113=2.067686%
[INFO|(OpenDelta)basemodel:704]2024-04-20 17:29:13,595 >> Static Memory 0.48 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.586 


 ==> Delta parameters :   [{'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [128, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self', 'intermediate'], 'bottleneck_dim': [32, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention.self'], 'bottleneck_dim': [64, 24, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention.self'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [64, 24, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [64, 32], 'non_linearity': 'gelu_new'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.538: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 0 loss 0.688  acc 0.538: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 0 loss 0.688  acc 0.538: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 0 loss 0.688  acc 0.538: 100%|█████████▉| 409/410 [02:10<00:00,  3.17it/s]
epoch 0 loss 0.688  acc 0.538: 100%|██████████| 410/410 [02:10<00:00,  3.42it/s]
epoch 0 loss 0.688  acc 0.538: 100%|██████████| 410/410 [02:10<00:00,  3.15it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.673  acc 0.57: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.57: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.57: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.57: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.57: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 1 loss 0.673  acc 0.57: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.661  acc 0.589: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.661  acc 0.588: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.661  acc 0.588: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.661  acc 0.589: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.661  acc 0.589: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 2 loss 0.661  acc 0.589: 100%|██████████| 410/410 [02:08<00:00,  3.19it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.65  acc 0.602: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.65  acc 0.602: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.65  acc 0.602: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.65  acc 0.602: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.65  acc 0.602: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 3 loss 0.65  acc 0.602: 100%|██████████| 410/410 [02:08<00:00,  3.19it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.64  acc 0.619: 100%|█████████▉| 408/410 [02:08<00:00,  3.20it/s]
epoch 4 loss 0.64  acc 0.619: 100%|█████████▉| 408/410 [02:08<00:00,  3.20it/s]
epoch 4 loss 0.64  acc 0.619: 100%|█████████▉| 409/410 [02:08<00:00,  3.20it/s]
epoch 4 loss 0.64  acc 0.62: 100%|█████████▉| 409/410 [02:08<00:00,  3.20it/s] 
epoch 4 loss 0.64  acc 0.62: 100%|██████████| 410/410 [02:08<00:00,  3.46it/s]
epoch 4 loss 0.64  acc 0.62: 100%|██████████| 410/410 [02:08<00:00,  3.19it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 17:40:53,101 >> Trainable Ratio: 3555721/127610761=2.786380%
[INFO|(OpenDelta)basemodel:702]2024-04-20 17:40:53,101 >> Delta Parameter Ratio: 2964360/127610761=2.322970%
[INFO|(OpenDelta)basemodel:704]2024-04-20 17:40:53,101 >> Static Memory 0.49 GB, Max Memory 9.52 GB

 ==> Fitness ( Validation acc ) :   0.598 


 ==> Delta parameters :   [{'insert_modules': ['output', 'intermediate', 'attention.self'], 'bottleneck_dim': [24, 24], 'non_linearity': 'relu'}, {'insert_modules': ['intermediate', 'attention.self', 'attention'], 'bottleneck_dim': [32, 64, 16], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [64, 128], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention', 'attention.self'], 'bottleneck_dim': [24, 16], 'non_linearity': 'relu'}, {'insert_modules': ['intermediate'], 'bottleneck_dim': [16, 16], 'non_linearity': 'relu'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [16, 16], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [24, 128], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [32], 'non_linearity': 'relu'}, {'insert_modules': ['attention'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention', 'attention.self'], 'bottleneck_dim': [32], 'non_linearity': 'relu'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [32, 32], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'intermediate', 'attention'], 'bottleneck_dim': [128, 16, 64], 'non_linearity': 'relu'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.53: 100%|█████████▉| 408/410 [02:12<00:00,  3.11it/s]
epoch 0 loss 0.69  acc 0.531: 100%|█████████▉| 408/410 [02:12<00:00,  3.11it/s]
epoch 0 loss 0.69  acc 0.531: 100%|█████████▉| 409/410 [02:12<00:00,  3.11it/s]
epoch 0 loss 0.69  acc 0.53: 100%|█████████▉| 409/410 [02:12<00:00,  3.11it/s] 
epoch 0 loss 0.69  acc 0.53: 100%|██████████| 410/410 [02:12<00:00,  3.37it/s]
epoch 0 loss 0.69  acc 0.53: 100%|██████████| 410/410 [02:12<00:00,  3.09it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.677  acc 0.569: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 1 loss 0.677  acc 0.569: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 1 loss 0.677  acc 0.569: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 1 loss 0.677  acc 0.568: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 1 loss 0.677  acc 0.568: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 1 loss 0.677  acc 0.568: 100%|██████████| 410/410 [02:11<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.663  acc 0.588: 100%|█████████▉| 408/410 [02:10<00:00,  3.11it/s]
epoch 2 loss 0.663  acc 0.588: 100%|█████████▉| 408/410 [02:11<00:00,  3.11it/s]
epoch 2 loss 0.663  acc 0.588: 100%|█████████▉| 409/410 [02:11<00:00,  3.11it/s]
epoch 2 loss 0.663  acc 0.588: 100%|█████████▉| 409/410 [02:11<00:00,  3.11it/s]
epoch 2 loss 0.663  acc 0.588: 100%|██████████| 410/410 [02:11<00:00,  3.36it/s]
epoch 2 loss 0.663  acc 0.588: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.654  acc 0.593: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 3 loss 0.654  acc 0.592: 100%|█████████▉| 408/410 [02:11<00:00,  3.12it/s]
epoch 3 loss 0.654  acc 0.592: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 3 loss 0.655  acc 0.592: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 3 loss 0.655  acc 0.592: 100%|██████████| 410/410 [02:11<00:00,  3.37it/s]
epoch 3 loss 0.655  acc 0.592: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.642  acc 0.618: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.642  acc 0.617: 100%|█████████▉| 408/410 [02:11<00:00,  3.13it/s]
epoch 4 loss 0.642  acc 0.617: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 4 loss 0.642  acc 0.617: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 4 loss 0.642  acc 0.617: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 4 loss 0.642  acc 0.617: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 17:52:45,769 >> Trainable Ratio: 958377/125013417=0.766619%
[INFO|(OpenDelta)basemodel:702]2024-04-20 17:52:45,769 >> Delta Parameter Ratio: 367016/125013417=0.293581%
[INFO|(OpenDelta)basemodel:704]2024-04-20 17:52:45,769 >> Static Memory 0.97 GB, Max Memory 9.63 GB

 ==> Fitness ( Validation acc ) :   0.592 


 ==> Delta parameters :   [0, 0, 0, 0, 0, {'insert_modules': ['attention.self', 'intermediate', 'attention'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [24, 24, 64], 'non_linearity': 'relu'}, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.536: 100%|█████████▉| 408/410 [01:33<00:00,  4.30it/s]
epoch 0 loss 0.69  acc 0.536: 100%|█████████▉| 408/410 [01:33<00:00,  4.30it/s]
epoch 0 loss 0.69  acc 0.536: 100%|█████████▉| 409/410 [01:33<00:00,  4.32it/s]
epoch 0 loss 0.69  acc 0.536: 100%|█████████▉| 409/410 [01:34<00:00,  4.32it/s]
epoch 0 loss 0.69  acc 0.536: 100%|██████████| 410/410 [01:34<00:00,  4.67it/s]
epoch 0 loss 0.69  acc 0.536: 100%|██████████| 410/410 [01:34<00:00,  4.36it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.675  acc 0.569: 100%|█████████▉| 408/410 [01:33<00:00,  4.40it/s]
epoch 1 loss 0.675  acc 0.569: 100%|█████████▉| 408/410 [01:34<00:00,  4.40it/s]
epoch 1 loss 0.675  acc 0.569: 100%|█████████▉| 409/410 [01:34<00:00,  4.40it/s]
epoch 1 loss 0.675  acc 0.569: 100%|█████████▉| 409/410 [01:34<00:00,  4.40it/s]
epoch 1 loss 0.675  acc 0.569: 100%|██████████| 410/410 [01:34<00:00,  4.75it/s]
epoch 1 loss 0.675  acc 0.569: 100%|██████████| 410/410 [01:34<00:00,  4.35it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.667  acc 0.578: 100%|█████████▉| 408/410 [01:33<00:00,  4.39it/s]
epoch 2 loss 0.667  acc 0.577: 100%|█████████▉| 408/410 [01:33<00:00,  4.39it/s]
epoch 2 loss 0.667  acc 0.577: 100%|█████████▉| 409/410 [01:33<00:00,  4.39it/s]
epoch 2 loss 0.667  acc 0.577: 100%|█████████▉| 409/410 [01:33<00:00,  4.39it/s]
epoch 2 loss 0.667  acc 0.577: 100%|██████████| 410/410 [01:33<00:00,  4.74it/s]
epoch 2 loss 0.667  acc 0.577: 100%|██████████| 410/410 [01:33<00:00,  4.38it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.658  acc 0.593: 100%|█████████▉| 408/410 [01:32<00:00,  4.40it/s]
epoch 3 loss 0.658  acc 0.592: 100%|█████████▉| 408/410 [01:33<00:00,  4.40it/s]
epoch 3 loss 0.658  acc 0.592: 100%|█████████▉| 409/410 [01:33<00:00,  4.39it/s]
epoch 3 loss 0.658  acc 0.593: 100%|█████████▉| 409/410 [01:33<00:00,  4.39it/s]
epoch 3 loss 0.658  acc 0.593: 100%|██████████| 410/410 [01:33<00:00,  4.74it/s]
epoch 3 loss 0.658  acc 0.593: 100%|██████████| 410/410 [01:33<00:00,  4.39it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.654  acc 0.598: 100%|█████████▉| 408/410 [01:33<00:00,  4.39it/s]
epoch 4 loss 0.654  acc 0.598: 100%|█████████▉| 408/410 [01:33<00:00,  4.39it/s]
epoch 4 loss 0.654  acc 0.598: 100%|█████████▉| 409/410 [01:33<00:00,  4.39it/s]
epoch 4 loss 0.654  acc 0.598: 100%|█████████▉| 409/410 [01:33<00:00,  4.39it/s]
epoch 4 loss 0.654  acc 0.598: 100%|██████████| 410/410 [01:33<00:00,  4.74it/s]
epoch 4 loss 0.654  acc 0.598: 100%|██████████| 410/410 [01:33<00:00,  4.38it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 18:01:26,835 >> Trainable Ratio: 1158737/125213777=0.925407%
[INFO|(OpenDelta)basemodel:702]2024-04-20 18:01:26,836 >> Delta Parameter Ratio: 567376/125213777=0.453126%
[INFO|(OpenDelta)basemodel:704]2024-04-20 18:01:26,836 >> Static Memory 0.48 GB, Max Memory 9.63 GB

 ==> Fitness ( Validation acc ) :   0.591 


 ==> Delta parameters :   [0, 0, 0, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, 0, 0, 0, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [32, 32, 32], 'non_linearity': 'gelu_new'}, 0, 0, 0, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [16, 24], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.535: 100%|█████████▉| 408/410 [01:44<00:00,  3.88it/s]
epoch 0 loss 0.689  acc 0.535: 100%|█████████▉| 408/410 [01:44<00:00,  3.88it/s]
epoch 0 loss 0.689  acc 0.535: 100%|█████████▉| 409/410 [01:44<00:00,  3.89it/s]
epoch 0 loss 0.69  acc 0.534: 100%|█████████▉| 409/410 [01:44<00:00,  3.89it/s] 
epoch 0 loss 0.69  acc 0.534: 100%|██████████| 410/410 [01:44<00:00,  4.20it/s]
epoch 0 loss 0.69  acc 0.534: 100%|██████████| 410/410 [01:44<00:00,  3.91it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.674  acc 0.572: 100%|█████████▉| 408/410 [01:43<00:00,  3.95it/s]
epoch 1 loss 0.674  acc 0.572: 100%|█████████▉| 408/410 [01:44<00:00,  3.95it/s]
epoch 1 loss 0.674  acc 0.572: 100%|█████████▉| 409/410 [01:44<00:00,  3.95it/s]
epoch 1 loss 0.674  acc 0.572: 100%|█████████▉| 409/410 [01:44<00:00,  3.95it/s]
epoch 1 loss 0.674  acc 0.572: 100%|██████████| 410/410 [01:44<00:00,  4.27it/s]
epoch 1 loss 0.674  acc 0.572: 100%|██████████| 410/410 [01:44<00:00,  3.93it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.666  acc 0.576: 100%|█████████▉| 408/410 [01:43<00:00,  3.94it/s]
epoch 2 loss 0.666  acc 0.576: 100%|█████████▉| 408/410 [01:43<00:00,  3.94it/s]
epoch 2 loss 0.666  acc 0.576: 100%|█████████▉| 409/410 [01:43<00:00,  3.92it/s]
epoch 2 loss 0.666  acc 0.576: 100%|█████████▉| 409/410 [01:43<00:00,  3.92it/s]
epoch 2 loss 0.666  acc 0.576: 100%|██████████| 410/410 [01:43<00:00,  4.24it/s]
epoch 2 loss 0.666  acc 0.576: 100%|██████████| 410/410 [01:43<00:00,  3.95it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.661  acc 0.594: 100%|█████████▉| 408/410 [01:43<00:00,  3.94it/s]
epoch 3 loss 0.661  acc 0.594: 100%|█████████▉| 408/410 [01:43<00:00,  3.94it/s]
epoch 3 loss 0.661  acc 0.594: 100%|█████████▉| 409/410 [01:43<00:00,  3.94it/s]
epoch 3 loss 0.661  acc 0.593: 100%|█████████▉| 409/410 [01:43<00:00,  3.94it/s]
epoch 3 loss 0.661  acc 0.593: 100%|██████████| 410/410 [01:43<00:00,  4.25it/s]
epoch 3 loss 0.661  acc 0.593: 100%|██████████| 410/410 [01:43<00:00,  3.95it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.655  acc 0.602: 100%|█████████▉| 408/410 [01:43<00:00,  3.93it/s]
epoch 4 loss 0.655  acc 0.602: 100%|█████████▉| 408/410 [01:44<00:00,  3.93it/s]
epoch 4 loss 0.655  acc 0.602: 100%|█████████▉| 409/410 [01:44<00:00,  3.94it/s]
epoch 4 loss 0.655  acc 0.602: 100%|█████████▉| 409/410 [01:44<00:00,  3.94it/s]
epoch 4 loss 0.655  acc 0.602: 100%|██████████| 410/410 [01:44<00:00,  4.25it/s]
epoch 4 loss 0.655  acc 0.602: 100%|██████████| 410/410 [01:44<00:00,  3.93it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 18:11:00,407 >> Trainable Ratio: 1057257/125112297=0.845046%
[INFO|(OpenDelta)basemodel:702]2024-04-20 18:11:00,407 >> Delta Parameter Ratio: 465896/125112297=0.372382%
[INFO|(OpenDelta)basemodel:704]2024-04-20 18:11:00,407 >> Static Memory 0.95 GB, Max Memory 9.63 GB

 ==> Fitness ( Validation acc ) :   0.581 


 ==> Delta parameters :   [0, 0, 0, 0, 0, 0, 0, {'insert_modules': ['attention.self', 'attention', 'intermediate'], 'bottleneck_dim': [32, 24], 'non_linearity': 'relu'}, 0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [32, 32], 'non_linearity': 'relu'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.691  acc 0.538: 100%|█████████▉| 408/410 [01:23<00:00,  4.81it/s]
epoch 0 loss 0.691  acc 0.538: 100%|█████████▉| 408/410 [01:23<00:00,  4.81it/s]
epoch 0 loss 0.691  acc 0.538: 100%|█████████▉| 409/410 [01:23<00:00,  4.80it/s]
epoch 0 loss 0.691  acc 0.539: 100%|█████████▉| 409/410 [01:24<00:00,  4.80it/s]
epoch 0 loss 0.691  acc 0.539: 100%|██████████| 410/410 [01:24<00:00,  5.19it/s]
epoch 0 loss 0.691  acc 0.539: 100%|██████████| 410/410 [01:24<00:00,  4.87it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.677  acc 0.568: 100%|█████████▉| 408/410 [01:24<00:00,  4.88it/s]
epoch 1 loss 0.677  acc 0.568: 100%|█████████▉| 408/410 [01:24<00:00,  4.88it/s]
epoch 1 loss 0.677  acc 0.568: 100%|█████████▉| 409/410 [01:24<00:00,  4.88it/s]
epoch 1 loss 0.677  acc 0.567: 100%|█████████▉| 409/410 [01:24<00:00,  4.88it/s]
epoch 1 loss 0.677  acc 0.567: 100%|██████████| 410/410 [01:24<00:00,  5.26it/s]
epoch 1 loss 0.677  acc 0.567: 100%|██████████| 410/410 [01:24<00:00,  4.86it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.672  acc 0.579: 100%|█████████▉| 408/410 [01:23<00:00,  4.93it/s]
epoch 2 loss 0.672  acc 0.579: 100%|█████████▉| 408/410 [01:23<00:00,  4.93it/s]
epoch 2 loss 0.672  acc 0.579: 100%|█████████▉| 409/410 [01:23<00:00,  4.92it/s]
epoch 2 loss 0.672  acc 0.579: 100%|█████████▉| 409/410 [01:23<00:00,  4.92it/s]
epoch 2 loss 0.672  acc 0.579: 100%|██████████| 410/410 [01:23<00:00,  5.32it/s]
epoch 2 loss 0.672  acc 0.579: 100%|██████████| 410/410 [01:23<00:00,  4.91it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.665  acc 0.586: 100%|█████████▉| 408/410 [01:22<00:00,  4.90it/s]
epoch 3 loss 0.665  acc 0.586: 100%|█████████▉| 408/410 [01:23<00:00,  4.90it/s]
epoch 3 loss 0.665  acc 0.586: 100%|█████████▉| 409/410 [01:23<00:00,  4.90it/s]
epoch 3 loss 0.665  acc 0.586: 100%|█████████▉| 409/410 [01:23<00:00,  4.90it/s]
epoch 3 loss 0.665  acc 0.586: 100%|██████████| 410/410 [01:23<00:00,  5.30it/s]
epoch 3 loss 0.665  acc 0.586: 100%|██████████| 410/410 [01:23<00:00,  4.92it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.662  acc 0.595: 100%|█████████▉| 408/410 [01:22<00:00,  4.94it/s]
epoch 4 loss 0.662  acc 0.595: 100%|█████████▉| 408/410 [01:23<00:00,  4.94it/s]
epoch 4 loss 0.662  acc 0.595: 100%|█████████▉| 409/410 [01:23<00:00,  4.95it/s]
epoch 4 loss 0.662  acc 0.594: 100%|█████████▉| 409/410 [01:23<00:00,  4.95it/s]
epoch 4 loss 0.662  acc 0.594: 100%|██████████| 410/410 [01:23<00:00,  5.34it/s]
epoch 4 loss 0.662  acc 0.594: 100%|██████████| 410/410 [01:23<00:00,  4.92it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 18:18:51,674 >> Trainable Ratio: 1238481/125293521=0.988464%
[INFO|(OpenDelta)basemodel:702]2024-04-20 18:18:51,675 >> Delta Parameter Ratio: 647120/125293521=0.516483%
[INFO|(OpenDelta)basemodel:704]2024-04-20 18:18:51,675 >> Static Memory 0.48 GB, Max Memory 9.63 GB

 ==> Fitness ( Validation acc ) :   0.588 


 ==> Delta parameters :   [0, 0, {'insert_modules': ['attention', 'attention.self'], 'bottleneck_dim': [24, 64, 64], 'non_linearity': 'relu'}, 0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [32, 128], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.691  acc 0.535: 100%|█████████▉| 408/410 [01:49<00:00,  3.70it/s]
epoch 0 loss 0.691  acc 0.535: 100%|█████████▉| 408/410 [01:49<00:00,  3.70it/s]
epoch 0 loss 0.691  acc 0.535: 100%|█████████▉| 409/410 [01:49<00:00,  3.70it/s]
epoch 0 loss 0.691  acc 0.534: 100%|█████████▉| 409/410 [01:49<00:00,  3.70it/s]
epoch 0 loss 0.691  acc 0.534: 100%|██████████| 410/410 [01:49<00:00,  3.99it/s]
epoch 0 loss 0.691  acc 0.534: 100%|██████████| 410/410 [01:50<00:00,  3.73it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.676  acc 0.565: 100%|█████████▉| 408/410 [01:48<00:00,  3.75it/s]
epoch 1 loss 0.676  acc 0.565: 100%|█████████▉| 408/410 [01:49<00:00,  3.75it/s]
epoch 1 loss 0.676  acc 0.565: 100%|█████████▉| 409/410 [01:49<00:00,  3.75it/s]
epoch 1 loss 0.676  acc 0.565: 100%|█████████▉| 409/410 [01:49<00:00,  3.75it/s]
epoch 1 loss 0.676  acc 0.565: 100%|██████████| 410/410 [01:49<00:00,  4.05it/s]
epoch 1 loss 0.676  acc 0.565: 100%|██████████| 410/410 [01:49<00:00,  3.75it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.67  acc 0.572: 100%|█████████▉| 408/410 [01:48<00:00,  3.76it/s]
epoch 2 loss 0.67  acc 0.572: 100%|█████████▉| 408/410 [01:48<00:00,  3.76it/s]
epoch 2 loss 0.67  acc 0.572: 100%|█████████▉| 409/410 [01:48<00:00,  3.77it/s]
epoch 2 loss 0.67  acc 0.573: 100%|█████████▉| 409/410 [01:48<00:00,  3.77it/s]
epoch 2 loss 0.67  acc 0.573: 100%|██████████| 410/410 [01:48<00:00,  4.07it/s]
epoch 2 loss 0.67  acc 0.573: 100%|██████████| 410/410 [01:48<00:00,  3.76it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.665  acc 0.586: 100%|█████████▉| 408/410 [01:48<00:00,  3.77it/s]
epoch 3 loss 0.665  acc 0.586: 100%|█████████▉| 408/410 [01:48<00:00,  3.77it/s]
epoch 3 loss 0.665  acc 0.586: 100%|█████████▉| 409/410 [01:48<00:00,  3.77it/s]
epoch 3 loss 0.665  acc 0.586: 100%|█████████▉| 409/410 [01:48<00:00,  3.77it/s]
epoch 3 loss 0.665  acc 0.586: 100%|██████████| 410/410 [01:48<00:00,  4.08it/s]
epoch 3 loss 0.665  acc 0.586: 100%|██████████| 410/410 [01:48<00:00,  3.76it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.662  acc 0.593: 100%|█████████▉| 408/410 [01:48<00:00,  3.77it/s]
epoch 4 loss 0.662  acc 0.593: 100%|█████████▉| 408/410 [01:48<00:00,  3.77it/s]
epoch 4 loss 0.662  acc 0.593: 100%|█████████▉| 409/410 [01:48<00:00,  3.77it/s]
epoch 4 loss 0.661  acc 0.594: 100%|█████████▉| 409/410 [01:48<00:00,  3.77it/s]
epoch 4 loss 0.661  acc 0.594: 100%|██████████| 410/410 [01:48<00:00,  4.08it/s]
epoch 4 loss 0.661  acc 0.594: 100%|██████████| 410/410 [01:48<00:00,  3.76it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 18:28:49,946 >> Trainable Ratio: 1897249/125952289=1.506324%
[INFO|(OpenDelta)basemodel:702]2024-04-20 18:28:49,946 >> Delta Parameter Ratio: 1305888/125952289=1.036812%
[INFO|(OpenDelta)basemodel:704]2024-04-20 18:28:49,946 >> Static Memory 0.95 GB, Max Memory 9.63 GB

 ==> Fitness ( Validation acc ) :   0.58 


 ==> Delta parameters :   [{'insert_modules': ['attention'], 'bottleneck_dim': [128, 64], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['attention.self'], 'bottleneck_dim': [24, 64, 32], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [32, 24], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, {'insert_modules': ['intermediate', 'attention', 'attention.self'], 'bottleneck_dim': [24, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'intermediate'], 'bottleneck_dim': [24, 32, 128], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.54: 100%|█████████▉| 408/410 [02:03<00:00,  3.31it/s]
epoch 0 loss 0.689  acc 0.54: 100%|█████████▉| 408/410 [02:03<00:00,  3.31it/s]
epoch 0 loss 0.689  acc 0.54: 100%|█████████▉| 409/410 [02:03<00:00,  3.31it/s]
epoch 0 loss 0.689  acc 0.54: 100%|█████████▉| 409/410 [02:04<00:00,  3.31it/s]
epoch 0 loss 0.689  acc 0.54: 100%|██████████| 410/410 [02:04<00:00,  3.58it/s]
epoch 0 loss 0.689  acc 0.54: 100%|██████████| 410/410 [02:04<00:00,  3.30it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.677  acc 0.571: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.677  acc 0.571: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.677  acc 0.571: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.677  acc 0.571: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.677  acc 0.571: 100%|██████████| 410/410 [02:02<00:00,  3.60it/s]
epoch 1 loss 0.677  acc 0.571: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.666  acc 0.581: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 2 loss 0.666  acc 0.581: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 2 loss 0.666  acc 0.581: 100%|█████████▉| 409/410 [02:02<00:00,  3.33it/s]
epoch 2 loss 0.666  acc 0.581: 100%|█████████▉| 409/410 [02:02<00:00,  3.33it/s]
epoch 2 loss 0.666  acc 0.581: 100%|██████████| 410/410 [02:02<00:00,  3.60it/s]
epoch 2 loss 0.666  acc 0.581: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.659  acc 0.592: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.659  acc 0.592: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.659  acc 0.592: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.659  acc 0.591: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.659  acc 0.591: 100%|██████████| 410/410 [02:02<00:00,  3.62it/s]
epoch 3 loss 0.659  acc 0.591: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.652  acc 0.6: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.652  acc 0.601: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.652  acc 0.601: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.652  acc 0.601: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.652  acc 0.601: 100%|██████████| 410/410 [02:02<00:00,  3.62it/s]
epoch 4 loss 0.652  acc 0.601: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 18:39:58,751 >> Trainable Ratio: 2708721/126763761=2.136826%
[INFO|(OpenDelta)basemodel:702]2024-04-20 18:39:58,751 >> Delta Parameter Ratio: 2117360/126763761=1.670320%
[INFO|(OpenDelta)basemodel:704]2024-04-20 18:39:58,751 >> Static Memory 0.49 GB, Max Memory 9.63 GB

 ==> Fitness ( Validation acc ) :   0.58 


 ==> Delta parameters :   [0, 0, 0, 0, {'insert_modules': ['intermediate', 'output', 'attention.self'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['attention', 'intermediate'], 'bottleneck_dim': [32, 128, 24], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['intermediate', 'attention.self', 'attention'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention', 'intermediate'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.691  acc 0.539: 100%|█████████▉| 408/410 [01:44<00:00,  3.87it/s]
epoch 0 loss 0.691  acc 0.539: 100%|█████████▉| 408/410 [01:45<00:00,  3.87it/s]
epoch 0 loss 0.691  acc 0.539: 100%|█████████▉| 409/410 [01:45<00:00,  3.87it/s]
epoch 0 loss 0.691  acc 0.539: 100%|█████████▉| 409/410 [01:45<00:00,  3.87it/s]
epoch 0 loss 0.691  acc 0.539: 100%|██████████| 410/410 [01:45<00:00,  4.19it/s]
epoch 0 loss 0.691  acc 0.539: 100%|██████████| 410/410 [01:45<00:00,  3.89it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.676  acc 0.564: 100%|█████████▉| 408/410 [01:44<00:00,  3.92it/s]
epoch 1 loss 0.676  acc 0.564: 100%|█████████▉| 408/410 [01:44<00:00,  3.92it/s]
epoch 1 loss 0.676  acc 0.564: 100%|█████████▉| 409/410 [01:44<00:00,  3.92it/s]
epoch 1 loss 0.676  acc 0.563: 100%|█████████▉| 409/410 [01:44<00:00,  3.92it/s]
epoch 1 loss 0.676  acc 0.563: 100%|██████████| 410/410 [01:44<00:00,  4.23it/s]
epoch 1 loss 0.676  acc 0.563: 100%|██████████| 410/410 [01:44<00:00,  3.92it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.656  acc 0.6: 100%|█████████▉| 408/410 [01:43<00:00,  3.92it/s]
epoch 2 loss 0.656  acc 0.6: 100%|█████████▉| 408/410 [01:44<00:00,  3.92it/s]
epoch 2 loss 0.656  acc 0.6: 100%|█████████▉| 409/410 [01:44<00:00,  3.93it/s]
epoch 2 loss 0.656  acc 0.6: 100%|█████████▉| 409/410 [01:44<00:00,  3.93it/s]
epoch 2 loss 0.656  acc 0.6: 100%|██████████| 410/410 [01:44<00:00,  4.24it/s]
epoch 2 loss 0.656  acc 0.6: 100%|██████████| 410/410 [01:44<00:00,  3.92it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.64  acc 0.62: 100%|█████████▉| 408/410 [01:44<00:00,  3.93it/s]
epoch 3 loss 0.64  acc 0.619: 100%|█████████▉| 408/410 [01:44<00:00,  3.93it/s]
epoch 3 loss 0.64  acc 0.619: 100%|█████████▉| 409/410 [01:44<00:00,  3.93it/s]
epoch 3 loss 0.64  acc 0.619: 100%|█████████▉| 409/410 [01:44<00:00,  3.93it/s]
epoch 3 loss 0.64  acc 0.619: 100%|██████████| 410/410 [01:44<00:00,  4.24it/s]
epoch 3 loss 0.64  acc 0.619: 100%|██████████| 410/410 [01:44<00:00,  3.92it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.629  acc 0.63: 100%|█████████▉| 408/410 [01:44<00:00,  3.92it/s]
epoch 4 loss 0.629  acc 0.63: 100%|█████████▉| 408/410 [01:44<00:00,  3.92it/s]
epoch 4 loss 0.629  acc 0.63: 100%|█████████▉| 409/410 [01:44<00:00,  3.93it/s]
epoch 4 loss 0.629  acc 0.63: 100%|█████████▉| 409/410 [01:44<00:00,  3.93it/s]
epoch 4 loss 0.629  acc 0.63: 100%|██████████| 410/410 [01:44<00:00,  4.24it/s]
epoch 4 loss 0.629  acc 0.63: 100%|██████████| 410/410 [01:44<00:00,  3.92it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 18:49:35,870 >> Trainable Ratio: 1110321/125165361=0.887083%
[INFO|(OpenDelta)basemodel:702]2024-04-20 18:49:35,870 >> Delta Parameter Ratio: 518960/125165361=0.414620%
[INFO|(OpenDelta)basemodel:704]2024-04-20 18:49:35,870 >> Static Memory 0.96 GB, Max Memory 9.63 GB

 ==> Fitness ( Validation acc ) :   0.601 


 ==> Delta parameters :   [0, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [16, 64, 128], 'non_linearity': 'relu'}, {'insert_modules': ['attention'], 'bottleneck_dim': [128, 64], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.528: 100%|█████████▉| 408/410 [01:54<00:00,  3.54it/s]
epoch 0 loss 0.69  acc 0.528: 100%|█████████▉| 408/410 [01:54<00:00,  3.54it/s]
epoch 0 loss 0.69  acc 0.528: 100%|█████████▉| 409/410 [01:54<00:00,  3.55it/s]
epoch 0 loss 0.69  acc 0.527: 100%|█████████▉| 409/410 [01:55<00:00,  3.55it/s]
epoch 0 loss 0.69  acc 0.527: 100%|██████████| 410/410 [01:55<00:00,  3.84it/s]
epoch 0 loss 0.69  acc 0.527: 100%|██████████| 410/410 [01:55<00:00,  3.56it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.678  acc 0.561: 100%|█████████▉| 408/410 [01:53<00:00,  3.59it/s]
epoch 1 loss 0.678  acc 0.561: 100%|█████████▉| 408/410 [01:53<00:00,  3.59it/s]
epoch 1 loss 0.678  acc 0.561: 100%|█████████▉| 409/410 [01:53<00:00,  3.59it/s]
epoch 1 loss 0.678  acc 0.561: 100%|█████████▉| 409/410 [01:54<00:00,  3.59it/s]
epoch 1 loss 0.678  acc 0.561: 100%|██████████| 410/410 [01:54<00:00,  3.88it/s]
epoch 1 loss 0.678  acc 0.561: 100%|██████████| 410/410 [01:54<00:00,  3.59it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.67  acc 0.582: 100%|█████████▉| 408/410 [01:53<00:00,  3.60it/s]
epoch 2 loss 0.67  acc 0.582: 100%|█████████▉| 408/410 [01:53<00:00,  3.60it/s]
epoch 2 loss 0.67  acc 0.582: 100%|█████████▉| 409/410 [01:53<00:00,  3.60it/s]
epoch 2 loss 0.67  acc 0.582: 100%|█████████▉| 409/410 [01:53<00:00,  3.60it/s]
epoch 2 loss 0.67  acc 0.582: 100%|██████████| 410/410 [01:53<00:00,  3.89it/s]
epoch 2 loss 0.67  acc 0.582: 100%|██████████| 410/410 [01:54<00:00,  3.60it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.667  acc 0.58: 100%|█████████▉| 408/410 [01:53<00:00,  3.61it/s]
epoch 3 loss 0.666  acc 0.58: 100%|█████████▉| 408/410 [01:53<00:00,  3.61it/s]
epoch 3 loss 0.666  acc 0.58: 100%|█████████▉| 409/410 [01:53<00:00,  3.61it/s]
epoch 3 loss 0.667  acc 0.58: 100%|█████████▉| 409/410 [01:53<00:00,  3.61it/s]
epoch 3 loss 0.667  acc 0.58: 100%|██████████| 410/410 [01:53<00:00,  3.90it/s]
epoch 3 loss 0.667  acc 0.58: 100%|██████████| 410/410 [01:54<00:00,  3.60it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.662  acc 0.59: 100%|█████████▉| 408/410 [01:53<00:00,  3.59it/s]
epoch 4 loss 0.662  acc 0.59: 100%|█████████▉| 408/410 [01:53<00:00,  3.59it/s]
epoch 4 loss 0.662  acc 0.59: 100%|█████████▉| 409/410 [01:53<00:00,  3.59it/s]
epoch 4 loss 0.663  acc 0.59: 100%|█████████▉| 409/410 [01:53<00:00,  3.59it/s]
epoch 4 loss 0.663  acc 0.59: 100%|██████████| 410/410 [01:53<00:00,  3.88it/s]
epoch 4 loss 0.663  acc 0.59: 100%|██████████| 410/410 [01:53<00:00,  3.60it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 18:59:59,615 >> Trainable Ratio: 4720321/128775361=3.665547%
[INFO|(OpenDelta)basemodel:702]2024-04-20 18:59:59,615 >> Delta Parameter Ratio: 4128960/128775361=3.206328%
[INFO|(OpenDelta)basemodel:704]2024-04-20 18:59:59,615 >> Static Memory 0.48 GB, Max Memory 9.63 GB

 ==> Fitness ( Validation acc ) :   0.579 


 ==> Delta parameters :   [{'insert_modules': ['attention.self'], 'bottleneck_dim': [64, 16, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [128, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [64, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [16, 32, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self', 'intermediate'], 'bottleneck_dim': [24, 128, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [16, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [128, 64, 128], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['attention.self', 'intermediate', 'attention'], 'bottleneck_dim': [128, 24, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'intermediate', 'output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.692  acc 0.539: 100%|█████████▉| 408/410 [02:15<00:00,  3.05it/s]
epoch 0 loss 0.691  acc 0.539: 100%|█████████▉| 408/410 [02:15<00:00,  3.05it/s]
epoch 0 loss 0.691  acc 0.539: 100%|█████████▉| 409/410 [02:15<00:00,  3.05it/s]
epoch 0 loss 0.691  acc 0.539: 100%|█████████▉| 409/410 [02:15<00:00,  3.05it/s]
epoch 0 loss 0.691  acc 0.539: 100%|██████████| 410/410 [02:15<00:00,  3.29it/s]
epoch 0 loss 0.691  acc 0.539: 100%|██████████| 410/410 [02:15<00:00,  3.02it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.677  acc 0.566: 100%|█████████▉| 408/410 [02:13<00:00,  3.06it/s]
epoch 1 loss 0.677  acc 0.567: 100%|█████████▉| 408/410 [02:13<00:00,  3.06it/s]
epoch 1 loss 0.677  acc 0.567: 100%|█████████▉| 409/410 [02:13<00:00,  3.06it/s]
epoch 1 loss 0.677  acc 0.566: 100%|█████████▉| 409/410 [02:14<00:00,  3.06it/s]
epoch 1 loss 0.677  acc 0.566: 100%|██████████| 410/410 [02:14<00:00,  3.30it/s]
epoch 1 loss 0.677  acc 0.566: 100%|██████████| 410/410 [02:14<00:00,  3.06it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.665  acc 0.583: 100%|█████████▉| 408/410 [02:13<00:00,  3.06it/s]
epoch 2 loss 0.665  acc 0.584: 100%|█████████▉| 408/410 [02:13<00:00,  3.06it/s]
epoch 2 loss 0.665  acc 0.584: 100%|█████████▉| 409/410 [02:13<00:00,  3.06it/s]
epoch 2 loss 0.665  acc 0.583: 100%|█████████▉| 409/410 [02:14<00:00,  3.06it/s]
epoch 2 loss 0.665  acc 0.583: 100%|██████████| 410/410 [02:14<00:00,  3.30it/s]
epoch 2 loss 0.665  acc 0.583: 100%|██████████| 410/410 [02:14<00:00,  3.06it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.652  acc 0.604: 100%|█████████▉| 408/410 [02:13<00:00,  3.05it/s]
epoch 3 loss 0.653  acc 0.604: 100%|█████████▉| 408/410 [02:13<00:00,  3.05it/s]
epoch 3 loss 0.653  acc 0.604: 100%|█████████▉| 409/410 [02:13<00:00,  3.05it/s]
epoch 3 loss 0.652  acc 0.604: 100%|█████████▉| 409/410 [02:14<00:00,  3.05it/s]
epoch 3 loss 0.652  acc 0.604: 100%|██████████| 410/410 [02:14<00:00,  3.29it/s]
epoch 3 loss 0.652  acc 0.604: 100%|██████████| 410/410 [02:14<00:00,  3.05it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.637  acc 0.622: 100%|█████████▉| 408/410 [02:13<00:00,  3.06it/s]
epoch 4 loss 0.637  acc 0.621: 100%|█████████▉| 408/410 [02:13<00:00,  3.06it/s]
epoch 4 loss 0.637  acc 0.621: 100%|█████████▉| 409/410 [02:13<00:00,  3.06it/s]
epoch 4 loss 0.637  acc 0.622: 100%|█████████▉| 409/410 [02:14<00:00,  3.06it/s]
epoch 4 loss 0.637  acc 0.622: 100%|██████████| 410/410 [02:14<00:00,  3.30it/s]
epoch 4 loss 0.637  acc 0.622: 100%|██████████| 410/410 [02:14<00:00,  3.06it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 19:12:07,822 >> Trainable Ratio: 4144505/128199545=3.232855%
[INFO|(OpenDelta)basemodel:702]2024-04-20 19:12:07,822 >> Delta Parameter Ratio: 3553144/128199545=2.771573%
[INFO|(OpenDelta)basemodel:704]2024-04-20 19:12:07,822 >> Static Memory 0.50 GB, Max Memory 9.88 GB

 ==> Fitness ( Validation acc ) :   0.597 


 ==> Delta parameters :   [0, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [128, 128, 32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output', 'intermediate'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output'], 'bottleneck_dim': [16, 24, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output'], 'bottleneck_dim': [16, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output'], 'bottleneck_dim': [24, 16, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self', 'intermediate'], 'bottleneck_dim': [16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output'], 'bottleneck_dim': [16, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention.self', 'output'], 'bottleneck_dim': [64, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [64, 128], 'non_linearity': 'gelu_new'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.693  acc 0.535: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 0 loss 0.693  acc 0.535: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 0 loss 0.693  acc 0.535: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 0 loss 0.693  acc 0.535: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 0 loss 0.693  acc 0.535: 100%|██████████| 410/410 [02:06<00:00,  3.51it/s]
epoch 0 loss 0.693  acc 0.535: 100%|██████████| 410/410 [02:06<00:00,  3.23it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.676  acc 0.567: 100%|█████████▉| 408/410 [02:04<00:00,  3.26it/s]
epoch 1 loss 0.676  acc 0.567: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.676  acc 0.567: 100%|█████████▉| 409/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.676  acc 0.567: 100%|█████████▉| 409/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.676  acc 0.567: 100%|██████████| 410/410 [02:05<00:00,  3.53it/s]
epoch 1 loss 0.676  acc 0.567: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.66  acc 0.595: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 2 loss 0.66  acc 0.595: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 2 loss 0.66  acc 0.595: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 2 loss 0.66  acc 0.594: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 2 loss 0.66  acc 0.594: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 2 loss 0.66  acc 0.594: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.648  acc 0.606: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 3 loss 0.648  acc 0.605: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 3 loss 0.648  acc 0.605: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 3 loss 0.648  acc 0.605: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 3 loss 0.648  acc 0.605: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 3 loss 0.648  acc 0.605: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.631  acc 0.629: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 4 loss 0.631  acc 0.629: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.631  acc 0.629: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.631  acc 0.628: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.631  acc 0.628: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 4 loss 0.631  acc 0.628: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 19:23:31,188 >> Trainable Ratio: 1999905/126054945=1.586534%
[INFO|(OpenDelta)basemodel:702]2024-04-20 19:23:31,188 >> Delta Parameter Ratio: 1408544/126054945=1.117405%
[INFO|(OpenDelta)basemodel:704]2024-04-20 19:23:31,188 >> Static Memory 0.98 GB, Max Memory 9.88 GB

 ==> Fitness ( Validation acc ) :   0.608 


 ==> Delta parameters :   [0, {'insert_modules': ['attention.self'], 'bottleneck_dim': [16, 16], 'non_linearity': 'relu'}, {'insert_modules': ['attention', 'output'], 'bottleneck_dim': [64], 'non_linearity': 'relu'}, 0, 0, {'insert_modules': ['attention', 'attention.self', 'intermediate'], 'bottleneck_dim': [128], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.686  acc 0.55: 100%|█████████▉| 408/410 [01:56<00:00,  3.52it/s]
epoch 0 loss 0.686  acc 0.55: 100%|█████████▉| 408/410 [01:56<00:00,  3.52it/s]
epoch 0 loss 0.686  acc 0.55: 100%|█████████▉| 409/410 [01:56<00:00,  3.52it/s]
epoch 0 loss 0.686  acc 0.55: 100%|█████████▉| 409/410 [01:56<00:00,  3.52it/s]
epoch 0 loss 0.686  acc 0.55: 100%|██████████| 410/410 [01:56<00:00,  3.81it/s]
epoch 0 loss 0.686  acc 0.55: 100%|██████████| 410/410 [01:56<00:00,  3.51it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.671  acc 0.581: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 1 loss 0.671  acc 0.582: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 1 loss 0.671  acc 0.582: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 1 loss 0.671  acc 0.582: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 1 loss 0.671  acc 0.582: 100%|██████████| 410/410 [01:55<00:00,  3.82it/s]
epoch 1 loss 0.671  acc 0.582: 100%|██████████| 410/410 [01:55<00:00,  3.54it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.656  acc 0.595: 100%|█████████▉| 408/410 [01:54<00:00,  3.54it/s]
epoch 2 loss 0.656  acc 0.595: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 2 loss 0.656  acc 0.595: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 2 loss 0.656  acc 0.595: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 2 loss 0.656  acc 0.595: 100%|██████████| 410/410 [01:55<00:00,  3.83it/s]
epoch 2 loss 0.656  acc 0.595: 100%|██████████| 410/410 [01:55<00:00,  3.55it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.643  acc 0.621: 100%|█████████▉| 408/410 [01:54<00:00,  3.54it/s]
epoch 3 loss 0.643  acc 0.62: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s] 
epoch 3 loss 0.643  acc 0.62: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 3 loss 0.643  acc 0.62: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 3 loss 0.643  acc 0.62: 100%|██████████| 410/410 [01:55<00:00,  3.84it/s]
epoch 3 loss 0.643  acc 0.62: 100%|██████████| 410/410 [01:55<00:00,  3.55it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.634  acc 0.624: 100%|█████████▉| 408/410 [01:54<00:00,  3.55it/s]
epoch 4 loss 0.634  acc 0.624: 100%|█████████▉| 408/410 [01:55<00:00,  3.55it/s]
epoch 4 loss 0.634  acc 0.624: 100%|█████████▉| 409/410 [01:55<00:00,  3.55it/s]
epoch 4 loss 0.634  acc 0.625: 100%|█████████▉| 409/410 [01:55<00:00,  3.55it/s]
epoch 4 loss 0.634  acc 0.625: 100%|██████████| 410/410 [01:55<00:00,  3.83it/s]
epoch 4 loss 0.634  acc 0.625: 100%|██████████| 410/410 [01:55<00:00,  3.55it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 19:34:03,967 >> Trainable Ratio: 1042193/125097233=0.833106%
[INFO|(OpenDelta)basemodel:702]2024-04-20 19:34:03,967 >> Delta Parameter Ratio: 450832/125097233=0.360385%
[INFO|(OpenDelta)basemodel:704]2024-04-20 19:34:03,967 >> Static Memory 0.49 GB, Max Memory 9.88 GB

 ==> Fitness ( Validation acc ) :   0.602 


 ==> Delta parameters :   [0, 0, 0, 0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [24, 16, 64], 'non_linearity': 'relu'}, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.687  acc 0.546: 100%|█████████▉| 408/410 [01:32<00:00,  4.39it/s]
epoch 0 loss 0.687  acc 0.547: 100%|█████████▉| 408/410 [01:32<00:00,  4.39it/s]
epoch 0 loss 0.687  acc 0.547: 100%|█████████▉| 409/410 [01:32<00:00,  4.39it/s]
epoch 0 loss 0.687  acc 0.547: 100%|█████████▉| 409/410 [01:32<00:00,  4.39it/s]
epoch 0 loss 0.687  acc 0.547: 100%|██████████| 410/410 [01:32<00:00,  4.74it/s]
epoch 0 loss 0.687  acc 0.547: 100%|██████████| 410/410 [01:32<00:00,  4.43it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.678  acc 0.566: 100%|█████████▉| 408/410 [01:31<00:00,  4.49it/s]
epoch 1 loss 0.678  acc 0.566: 100%|█████████▉| 408/410 [01:31<00:00,  4.49it/s]
epoch 1 loss 0.678  acc 0.566: 100%|█████████▉| 409/410 [01:31<00:00,  4.49it/s]
epoch 1 loss 0.678  acc 0.565: 100%|█████████▉| 409/410 [01:32<00:00,  4.49it/s]
epoch 1 loss 0.678  acc 0.565: 100%|██████████| 410/410 [01:32<00:00,  4.85it/s]
epoch 1 loss 0.678  acc 0.565: 100%|██████████| 410/410 [01:32<00:00,  4.45it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.667  acc 0.58: 100%|█████████▉| 408/410 [01:30<00:00,  4.49it/s]
epoch 2 loss 0.668  acc 0.58: 100%|█████████▉| 408/410 [01:31<00:00,  4.49it/s]
epoch 2 loss 0.668  acc 0.58: 100%|█████████▉| 409/410 [01:31<00:00,  4.48it/s]
epoch 2 loss 0.667  acc 0.58: 100%|█████████▉| 409/410 [01:31<00:00,  4.48it/s]
epoch 2 loss 0.667  acc 0.58: 100%|██████████| 410/410 [01:31<00:00,  4.84it/s]
epoch 2 loss 0.667  acc 0.58: 100%|██████████| 410/410 [01:31<00:00,  4.49it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.66  acc 0.59: 100%|█████████▉| 408/410 [01:31<00:00,  4.47it/s]
epoch 3 loss 0.66  acc 0.59: 100%|█████████▉| 408/410 [01:31<00:00,  4.47it/s]
epoch 3 loss 0.66  acc 0.59: 100%|█████████▉| 409/410 [01:31<00:00,  4.47it/s]
epoch 3 loss 0.66  acc 0.591: 100%|█████████▉| 409/410 [01:31<00:00,  4.47it/s]
epoch 3 loss 0.66  acc 0.591: 100%|██████████| 410/410 [01:31<00:00,  4.84it/s]
epoch 3 loss 0.66  acc 0.591: 100%|██████████| 410/410 [01:31<00:00,  4.47it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.658  acc 0.595: 100%|█████████▉| 408/410 [01:31<00:00,  4.48it/s]
epoch 4 loss 0.658  acc 0.596: 100%|█████████▉| 408/410 [01:31<00:00,  4.48it/s]
epoch 4 loss 0.658  acc 0.596: 100%|█████████▉| 409/410 [01:31<00:00,  4.48it/s]
epoch 4 loss 0.658  acc 0.596: 100%|█████████▉| 409/410 [01:31<00:00,  4.48it/s]
epoch 4 loss 0.658  acc 0.596: 100%|██████████| 410/410 [01:31<00:00,  4.84it/s]
epoch 4 loss 0.658  acc 0.596: 100%|██████████| 410/410 [01:31<00:00,  4.47it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 19:42:35,871 >> Trainable Ratio: 4273881/128328921=3.330411%
[INFO|(OpenDelta)basemodel:702]2024-04-20 19:42:35,871 >> Delta Parameter Ratio: 3682520/128328921=2.869595%
[INFO|(OpenDelta)basemodel:704]2024-04-20 19:42:35,871 >> Static Memory 0.95 GB, Max Memory 9.88 GB
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 19:42:37,307 >> Trainable Ratio: 2270209/126325249=1.797114%
[INFO|(OpenDelta)basemodel:702]2024-04-20 19:42:37,307 >> Delta Parameter Ratio: 1678848/126325249=1.328988%
[INFO|(OpenDelta)basemodel:704]2024-04-20 19:42:37,307 >> Static Memory 1.43 GB, Max Memory 9.88 GB

 ==> Fitness ( Validation acc ) :   0.608 


 ==> Delta parameters :   [{'insert_modules': ['output', 'intermediate', 'attention'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'attention.self'], 'bottleneck_dim': [64, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [32, 32, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention'], 'bottleneck_dim': [64, 64, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [128, 32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [16, 32, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [16, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self', 'attention'], 'bottleneck_dim': [24, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'intermediate'], 'bottleneck_dim': [24, 16, 32], 'non_linearity': 'gelu_new'}] 


 ==> Delta parameters :   [{'insert_modules': ['attention', 'output', 'intermediate'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'intermediate', 'attention.self'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention', 'attention.self', 'output'], 'bottleneck_dim': [32, 24], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [64, 32], 'non_linearity': 'relu'}, {'insert_modules': ['attention'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, {'insert_modules': ['attention', 'output', 'attention.self'], 'bottleneck_dim': [32, 32], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention.self', 'intermediate', 'output'], 'bottleneck_dim': [64], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [16, 64], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'output', 'intermediate'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.535: 100%|█████████▉| 408/410 [02:07<00:00,  3.17it/s]
epoch 0 loss 0.69  acc 0.535: 100%|█████████▉| 408/410 [02:07<00:00,  3.17it/s]
epoch 0 loss 0.69  acc 0.535: 100%|█████████▉| 409/410 [02:07<00:00,  3.17it/s]
epoch 0 loss 0.689  acc 0.536: 100%|█████████▉| 409/410 [02:08<00:00,  3.17it/s]
epoch 0 loss 0.689  acc 0.536: 100%|██████████| 410/410 [02:08<00:00,  3.42it/s]
epoch 0 loss 0.689  acc 0.536: 100%|██████████| 410/410 [02:08<00:00,  3.20it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.674  acc 0.565: 100%|█████████▉| 408/410 [02:07<00:00,  3.23it/s]
epoch 1 loss 0.675  acc 0.565: 100%|█████████▉| 408/410 [02:07<00:00,  3.23it/s]
epoch 1 loss 0.675  acc 0.565: 100%|█████████▉| 409/410 [02:07<00:00,  3.24it/s]
epoch 1 loss 0.675  acc 0.564: 100%|█████████▉| 409/410 [02:08<00:00,  3.24it/s]
epoch 1 loss 0.675  acc 0.564: 100%|██████████| 410/410 [02:08<00:00,  3.50it/s]
epoch 1 loss 0.675  acc 0.564: 100%|██████████| 410/410 [02:08<00:00,  3.20it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.658  acc 0.592: 100%|█████████▉| 408/410 [02:06<00:00,  3.23it/s]
epoch 2 loss 0.658  acc 0.592: 100%|█████████▉| 408/410 [02:06<00:00,  3.23it/s]
epoch 2 loss 0.658  acc 0.592: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 2 loss 0.659  acc 0.591: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 2 loss 0.659  acc 0.591: 100%|██████████| 410/410 [02:06<00:00,  3.49it/s]
epoch 2 loss 0.659  acc 0.591: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.644  acc 0.614: 100%|█████████▉| 408/410 [02:06<00:00,  3.22it/s]
epoch 3 loss 0.644  acc 0.614: 100%|█████████▉| 408/410 [02:07<00:00,  3.22it/s]
epoch 3 loss 0.644  acc 0.614: 100%|█████████▉| 409/410 [02:07<00:00,  3.22it/s]
epoch 3 loss 0.645  acc 0.613: 100%|█████████▉| 409/410 [02:07<00:00,  3.22it/s]
epoch 3 loss 0.645  acc 0.613: 100%|██████████| 410/410 [02:07<00:00,  3.48it/s]
epoch 3 loss 0.645  acc 0.613: 100%|██████████| 410/410 [02:07<00:00,  3.22it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.629  acc 0.634: 100%|█████████▉| 408/410 [02:06<00:00,  3.23it/s]
epoch 4 loss 0.629  acc 0.634: 100%|█████████▉| 408/410 [02:06<00:00,  3.23it/s]
epoch 4 loss 0.629  acc 0.634: 100%|█████████▉| 409/410 [02:06<00:00,  3.23it/s]
epoch 4 loss 0.629  acc 0.634: 100%|█████████▉| 409/410 [02:07<00:00,  3.23it/s]
epoch 4 loss 0.629  acc 0.634: 100%|██████████| 410/410 [02:07<00:00,  3.49it/s]
epoch 4 loss 0.629  acc 0.634: 100%|██████████| 410/410 [02:07<00:00,  3.22it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 19:54:08,962 >> Trainable Ratio: 2126713/126181753=1.685436%
[INFO|(OpenDelta)basemodel:702]2024-04-20 19:54:08,962 >> Delta Parameter Ratio: 1535352/126181753=1.216778%
[INFO|(OpenDelta)basemodel:704]2024-04-20 19:54:08,962 >> Static Memory 0.49 GB, Max Memory 9.88 GB

 ==> Fitness ( Validation acc ) :   0.596 


 ==> Delta parameters :   [0, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, 0, {'insert_modules': ['intermediate', 'output', 'attention.self'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['attention', 'intermediate', 'attention.self'], 'bottleneck_dim': [128, 128], 'non_linearity': 'gelu_new'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.538: 100%|█████████▉| 408/410 [01:57<00:00,  3.45it/s]
epoch 0 loss 0.69  acc 0.538: 100%|█████████▉| 408/410 [01:57<00:00,  3.45it/s]
epoch 0 loss 0.69  acc 0.538: 100%|█████████▉| 409/410 [01:57<00:00,  3.45it/s]
epoch 0 loss 0.69  acc 0.538: 100%|█████████▉| 409/410 [01:58<00:00,  3.45it/s]
epoch 0 loss 0.69  acc 0.538: 100%|██████████| 410/410 [01:58<00:00,  3.73it/s]
epoch 0 loss 0.69  acc 0.538: 100%|██████████| 410/410 [01:58<00:00,  3.47it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.68  acc 0.567: 100%|█████████▉| 408/410 [01:58<00:00,  3.47it/s]
epoch 1 loss 0.681  acc 0.567: 100%|█████████▉| 408/410 [01:58<00:00,  3.47it/s]
epoch 1 loss 0.681  acc 0.567: 100%|█████████▉| 409/410 [01:58<00:00,  3.47it/s]
epoch 1 loss 0.68  acc 0.567: 100%|█████████▉| 409/410 [01:58<00:00,  3.47it/s] 
epoch 1 loss 0.68  acc 0.567: 100%|██████████| 410/410 [01:58<00:00,  3.75it/s]
epoch 1 loss 0.68  acc 0.567: 100%|██████████| 410/410 [01:58<00:00,  3.46it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.663  acc 0.59: 100%|█████████▉| 408/410 [01:57<00:00,  3.47it/s]
epoch 2 loss 0.663  acc 0.59: 100%|█████████▉| 408/410 [01:57<00:00,  3.47it/s]
epoch 2 loss 0.663  acc 0.59: 100%|█████████▉| 409/410 [01:57<00:00,  3.47it/s]
epoch 2 loss 0.663  acc 0.59: 100%|█████████▉| 409/410 [01:57<00:00,  3.47it/s]
epoch 2 loss 0.663  acc 0.59: 100%|██████████| 410/410 [01:57<00:00,  3.75it/s]
epoch 2 loss 0.663  acc 0.59: 100%|██████████| 410/410 [01:58<00:00,  3.47it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.651  acc 0.603: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 3 loss 0.651  acc 0.603: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 3 loss 0.651  acc 0.603: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 3 loss 0.651  acc 0.603: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 3 loss 0.651  acc 0.603: 100%|██████████| 410/410 [01:57<00:00,  3.78it/s]
epoch 3 loss 0.651  acc 0.603: 100%|██████████| 410/410 [01:57<00:00,  3.48it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.641  acc 0.62: 100%|█████████▉| 408/410 [01:57<00:00,  3.50it/s]
epoch 4 loss 0.641  acc 0.619: 100%|█████████▉| 408/410 [01:57<00:00,  3.50it/s]
epoch 4 loss 0.641  acc 0.619: 100%|█████████▉| 409/410 [01:57<00:00,  3.50it/s]
epoch 4 loss 0.641  acc 0.62: 100%|█████████▉| 409/410 [01:57<00:00,  3.50it/s] 
epoch 4 loss 0.641  acc 0.62: 100%|██████████| 410/410 [01:57<00:00,  3.78it/s]
epoch 4 loss 0.641  acc 0.62: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 20:04:51,825 >> Trainable Ratio: 1124201/125179241=0.898073%
[INFO|(OpenDelta)basemodel:702]2024-04-20 20:04:51,825 >> Delta Parameter Ratio: 532840/125179241=0.425662%
[INFO|(OpenDelta)basemodel:704]2024-04-20 20:04:51,825 >> Static Memory 0.96 GB, Max Memory 9.88 GB

 ==> Fitness ( Validation acc ) :   0.61 


 ==> Delta parameters :   [0, 0, 0, 0, 0, 0, 0, 0, {'insert_modules': ['attention.self'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [128, 64], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.688  acc 0.543: 100%|█████████▉| 408/410 [01:17<00:00,  5.19it/s]
epoch 0 loss 0.688  acc 0.543: 100%|█████████▉| 408/410 [01:17<00:00,  5.19it/s]
epoch 0 loss 0.688  acc 0.543: 100%|█████████▉| 409/410 [01:17<00:00,  5.20it/s]
epoch 0 loss 0.688  acc 0.543: 100%|█████████▉| 409/410 [01:17<00:00,  5.20it/s]
epoch 0 loss 0.688  acc 0.543: 100%|██████████| 410/410 [01:17<00:00,  5.62it/s]
epoch 0 loss 0.688  acc 0.543: 100%|██████████| 410/410 [01:17<00:00,  5.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.678  acc 0.57: 100%|█████████▉| 408/410 [01:18<00:00,  5.27it/s]
epoch 1 loss 0.678  acc 0.57: 100%|█████████▉| 408/410 [01:18<00:00,  5.27it/s]
epoch 1 loss 0.678  acc 0.57: 100%|█████████▉| 409/410 [01:18<00:00,  5.28it/s]
epoch 1 loss 0.678  acc 0.57: 100%|█████████▉| 409/410 [01:18<00:00,  5.28it/s]
epoch 1 loss 0.678  acc 0.57: 100%|██████████| 410/410 [01:18<00:00,  5.69it/s]
epoch 1 loss 0.678  acc 0.57: 100%|██████████| 410/410 [01:18<00:00,  5.23it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.671  acc 0.576: 100%|█████████▉| 408/410 [01:17<00:00,  5.26it/s]
epoch 2 loss 0.671  acc 0.576: 100%|█████████▉| 408/410 [01:17<00:00,  5.26it/s]
epoch 2 loss 0.671  acc 0.576: 100%|█████████▉| 409/410 [01:17<00:00,  5.27it/s]
epoch 2 loss 0.671  acc 0.576: 100%|█████████▉| 409/410 [01:17<00:00,  5.27it/s]
epoch 2 loss 0.671  acc 0.576: 100%|██████████| 410/410 [01:17<00:00,  5.69it/s]
epoch 2 loss 0.671  acc 0.576: 100%|██████████| 410/410 [01:17<00:00,  5.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.66  acc 0.591: 100%|█████████▉| 408/410 [01:17<00:00,  5.30it/s]
epoch 3 loss 0.661  acc 0.59: 100%|█████████▉| 408/410 [01:17<00:00,  5.30it/s]
epoch 3 loss 0.661  acc 0.59: 100%|█████████▉| 409/410 [01:17<00:00,  5.29it/s]
epoch 3 loss 0.661  acc 0.59: 100%|█████████▉| 409/410 [01:17<00:00,  5.29it/s]
epoch 3 loss 0.661  acc 0.59: 100%|██████████| 410/410 [01:17<00:00,  5.71it/s]
epoch 3 loss 0.661  acc 0.59: 100%|██████████| 410/410 [01:17<00:00,  5.28it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.654  acc 0.597: 100%|█████████▉| 408/410 [01:17<00:00,  5.31it/s]
epoch 4 loss 0.653  acc 0.597: 100%|█████████▉| 408/410 [01:17<00:00,  5.31it/s]
epoch 4 loss 0.653  acc 0.597: 100%|█████████▉| 409/410 [01:17<00:00,  5.30it/s]
epoch 4 loss 0.653  acc 0.597: 100%|█████████▉| 409/410 [01:17<00:00,  5.30it/s]
epoch 4 loss 0.653  acc 0.597: 100%|██████████| 410/410 [01:17<00:00,  5.72it/s]
epoch 4 loss 0.653  acc 0.597: 100%|██████████| 410/410 [01:17<00:00,  5.29it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 20:12:13,516 >> Trainable Ratio: 3251913/127306953=2.554388%
[INFO|(OpenDelta)basemodel:702]2024-04-20 20:12:13,516 >> Delta Parameter Ratio: 2660552/127306953=2.089872%
[INFO|(OpenDelta)basemodel:704]2024-04-20 20:12:13,516 >> Static Memory 0.48 GB, Max Memory 9.88 GB

 ==> Fitness ( Validation acc ) :   0.588 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.692  acc 0.527: 100%|█████████▉| 408/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.692  acc 0.528: 100%|█████████▉| 408/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.692  acc 0.528: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.692  acc 0.528: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.692  acc 0.528: 100%|██████████| 410/410 [02:10<00:00,  3.42it/s]
epoch 0 loss 0.692  acc 0.528: 100%|██████████| 410/410 [02:10<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.674  acc 0.575: 100%|█████████▉| 408/410 [02:08<00:00,  3.16it/s]
epoch 1 loss 0.674  acc 0.575: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 1 loss 0.674  acc 0.575: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 1 loss 0.674  acc 0.576: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 1 loss 0.674  acc 0.576: 100%|██████████| 410/410 [02:09<00:00,  3.42it/s]
epoch 1 loss 0.674  acc 0.576: 100%|██████████| 410/410 [02:09<00:00,  3.17it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.66  acc 0.591: 100%|█████████▉| 408/410 [02:08<00:00,  3.16it/s]
epoch 2 loss 0.66  acc 0.59: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s] 
epoch 2 loss 0.66  acc 0.59: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 2 loss 0.66  acc 0.591: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 2 loss 0.66  acc 0.591: 100%|██████████| 410/410 [02:09<00:00,  3.42it/s]
epoch 2 loss 0.66  acc 0.591: 100%|██████████| 410/410 [02:09<00:00,  3.17it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.644  acc 0.611: 100%|█████████▉| 408/410 [02:08<00:00,  3.17it/s]
epoch 3 loss 0.644  acc 0.611: 100%|█████████▉| 408/410 [02:09<00:00,  3.17it/s]
epoch 3 loss 0.644  acc 0.611: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 3 loss 0.644  acc 0.61: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s] 
epoch 3 loss 0.644  acc 0.61: 100%|██████████| 410/410 [02:09<00:00,  3.42it/s]
epoch 3 loss 0.644  acc 0.61: 100%|██████████| 410/410 [02:09<00:00,  3.17it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.63  acc 0.629: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.63  acc 0.629: 100%|█████████▉| 408/410 [02:09<00:00,  3.18it/s]
epoch 4 loss 0.63  acc 0.629: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 4 loss 0.63  acc 0.629: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 4 loss 0.63  acc 0.629: 100%|██████████| 410/410 [02:09<00:00,  3.43it/s]
epoch 4 loss 0.63  acc 0.629: 100%|██████████| 410/410 [02:09<00:00,  3.17it/s]
INFO:name:Cycle 1, the best candidate in pop has score 0.62, and best in the run 0.62
INFO:name:Mean 0.5947333333333334 and standard deviation 0.01111735380185212 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 20:23:56,152 >> Trainable Ratio: 1280457/125335497=1.021624%
[INFO|(OpenDelta)basemodel:702]2024-04-20 20:23:56,153 >> Delta Parameter Ratio: 689096/125335497=0.549801%
[INFO|(OpenDelta)basemodel:704]2024-04-20 20:23:56,153 >> Static Memory 0.96 GB, Max Memory 9.88 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.538: 100%|█████████▉| 408/410 [01:57<00:00,  3.50it/s]
epoch 0 loss 0.689  acc 0.538: 100%|█████████▉| 408/410 [01:57<00:00,  3.50it/s]
epoch 0 loss 0.689  acc 0.538: 100%|█████████▉| 409/410 [01:57<00:00,  3.50it/s]
epoch 0 loss 0.689  acc 0.537: 100%|█████████▉| 409/410 [01:57<00:00,  3.50it/s]
epoch 0 loss 0.689  acc 0.537: 100%|██████████| 410/410 [01:57<00:00,  3.78it/s]
epoch 0 loss 0.689  acc 0.537: 100%|██████████| 410/410 [01:57<00:00,  3.48it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.676  acc 0.572: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 1 loss 0.676  acc 0.572: 100%|█████████▉| 408/410 [01:56<00:00,  3.53it/s]
epoch 1 loss 0.676  acc 0.572: 100%|█████████▉| 409/410 [01:56<00:00,  3.53it/s]
epoch 1 loss 0.676  acc 0.572: 100%|█████████▉| 409/410 [01:56<00:00,  3.53it/s]
epoch 1 loss 0.676  acc 0.572: 100%|██████████| 410/410 [01:56<00:00,  3.81it/s]
epoch 1 loss 0.676  acc 0.572: 100%|██████████| 410/410 [01:56<00:00,  3.52it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.665  acc 0.583: 100%|█████████▉| 408/410 [01:55<00:00,  3.52it/s]
epoch 2 loss 0.665  acc 0.583: 100%|█████████▉| 408/410 [01:55<00:00,  3.52it/s]
epoch 2 loss 0.665  acc 0.583: 100%|█████████▉| 409/410 [01:55<00:00,  3.52it/s]
epoch 2 loss 0.665  acc 0.582: 100%|█████████▉| 409/410 [01:56<00:00,  3.52it/s]
epoch 2 loss 0.665  acc 0.582: 100%|██████████| 410/410 [01:56<00:00,  3.80it/s]
epoch 2 loss 0.665  acc 0.582: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.657  acc 0.6: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 3 loss 0.657  acc 0.6: 100%|█████████▉| 408/410 [01:56<00:00,  3.53it/s]
epoch 3 loss 0.657  acc 0.6: 100%|█████████▉| 409/410 [01:56<00:00,  3.53it/s]
epoch 3 loss 0.657  acc 0.6: 100%|█████████▉| 409/410 [01:56<00:00,  3.53it/s]
epoch 3 loss 0.657  acc 0.6: 100%|██████████| 410/410 [01:56<00:00,  3.82it/s]
epoch 3 loss 0.657  acc 0.6: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.646  acc 0.609: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 4 loss 0.646  acc 0.609: 100%|█████████▉| 408/410 [01:56<00:00,  3.54it/s]
epoch 4 loss 0.646  acc 0.609: 100%|█████████▉| 409/410 [01:56<00:00,  3.54it/s]
epoch 4 loss 0.646  acc 0.609: 100%|█████████▉| 409/410 [01:56<00:00,  3.54it/s]
epoch 4 loss 0.646  acc 0.609: 100%|██████████| 410/410 [01:56<00:00,  3.82it/s]
epoch 4 loss 0.646  acc 0.609: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]
INFO:name:Cycle 2, the best candidate in pop has score 0.62, and best in the run 0.62
INFO:name:Mean 0.5953333333333334 and standard deviation 0.010674996747332327 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 20:34:31,998 >> Trainable Ratio: 4064153/128119193=3.172166%
[INFO|(OpenDelta)basemodel:702]2024-04-20 20:34:31,998 >> Delta Parameter Ratio: 3472792/128119193=2.710595%
[INFO|(OpenDelta)basemodel:704]2024-04-20 20:34:31,998 >> Static Memory 0.95 GB, Max Memory 9.88 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.544: 100%|█████████▉| 408/410 [02:06<00:00,  3.26it/s]
epoch 0 loss 0.689  acc 0.544: 100%|█████████▉| 408/410 [02:06<00:00,  3.26it/s]
epoch 0 loss 0.689  acc 0.544: 100%|█████████▉| 409/410 [02:06<00:00,  3.26it/s]
epoch 0 loss 0.689  acc 0.545: 100%|█████████▉| 409/410 [02:06<00:00,  3.26it/s]
epoch 0 loss 0.689  acc 0.545: 100%|██████████| 410/410 [02:06<00:00,  3.52it/s]
epoch 0 loss 0.689  acc 0.545: 100%|██████████| 410/410 [02:06<00:00,  3.23it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.675  acc 0.566: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.675  acc 0.566: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.675  acc 0.566: 100%|█████████▉| 409/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.675  acc 0.565: 100%|█████████▉| 409/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.675  acc 0.565: 100%|██████████| 410/410 [02:05<00:00,  3.53it/s]
epoch 1 loss 0.675  acc 0.565: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.657  acc 0.595: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 2 loss 0.657  acc 0.595: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 2 loss 0.657  acc 0.595: 100%|█████████▉| 409/410 [02:05<00:00,  3.26it/s]
epoch 2 loss 0.657  acc 0.595: 100%|█████████▉| 409/410 [02:05<00:00,  3.26it/s]
epoch 2 loss 0.657  acc 0.595: 100%|██████████| 410/410 [02:05<00:00,  3.52it/s]
epoch 2 loss 0.657  acc 0.595: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.642  acc 0.614: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 3 loss 0.642  acc 0.614: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 3 loss 0.642  acc 0.614: 100%|█████████▉| 409/410 [02:05<00:00,  3.25it/s]
epoch 3 loss 0.641  acc 0.614: 100%|█████████▉| 409/410 [02:05<00:00,  3.25it/s]
epoch 3 loss 0.641  acc 0.614: 100%|██████████| 410/410 [02:05<00:00,  3.52it/s]
epoch 3 loss 0.641  acc 0.614: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.628  acc 0.632: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.628  acc 0.632: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.628  acc 0.632: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.628  acc 0.632: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.628  acc 0.632: 100%|██████████| 410/410 [02:05<00:00,  3.53it/s]
epoch 4 loss 0.628  acc 0.632: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]
INFO:name:Cycle 3, the best candidate in pop has score 0.62, and best in the run 0.62
INFO:name:Mean 0.5945666666666667 and standard deviation 0.010635422992162673 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 20:45:56,866 >> Trainable Ratio: 2126713/126181753=1.685436%
[INFO|(OpenDelta)basemodel:702]2024-04-20 20:45:56,866 >> Delta Parameter Ratio: 1535352/126181753=1.216778%
[INFO|(OpenDelta)basemodel:704]2024-04-20 20:45:56,866 >> Static Memory 1.43 GB, Max Memory 9.88 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.691  acc 0.529: 100%|█████████▉| 408/410 [01:58<00:00,  3.46it/s]
epoch 0 loss 0.691  acc 0.529: 100%|█████████▉| 408/410 [01:58<00:00,  3.46it/s]
epoch 0 loss 0.691  acc 0.529: 100%|█████████▉| 409/410 [01:58<00:00,  3.47it/s]
epoch 0 loss 0.691  acc 0.529: 100%|█████████▉| 409/410 [01:58<00:00,  3.47it/s]
epoch 0 loss 0.691  acc 0.529: 100%|██████████| 410/410 [01:58<00:00,  3.75it/s]
epoch 0 loss 0.691  acc 0.529: 100%|██████████| 410/410 [01:58<00:00,  3.45it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.677  acc 0.566: 100%|█████████▉| 408/410 [01:56<00:00,  3.50it/s]
epoch 1 loss 0.676  acc 0.567: 100%|█████████▉| 408/410 [01:57<00:00,  3.50it/s]
epoch 1 loss 0.676  acc 0.567: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 1 loss 0.677  acc 0.566: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 1 loss 0.677  acc 0.566: 100%|██████████| 410/410 [01:57<00:00,  3.78it/s]
epoch 1 loss 0.677  acc 0.566: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.663  acc 0.588: 100%|█████████▉| 408/410 [01:56<00:00,  3.49it/s]
epoch 2 loss 0.663  acc 0.588: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 2 loss 0.663  acc 0.588: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 2 loss 0.663  acc 0.587: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 2 loss 0.663  acc 0.587: 100%|██████████| 410/410 [01:57<00:00,  3.77it/s]
epoch 2 loss 0.663  acc 0.587: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.652  acc 0.604: 100%|█████████▉| 408/410 [01:56<00:00,  3.49it/s]
epoch 3 loss 0.653  acc 0.603: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 3 loss 0.653  acc 0.603: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 3 loss 0.652  acc 0.604: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 3 loss 0.652  acc 0.604: 100%|██████████| 410/410 [01:57<00:00,  3.78it/s]
epoch 3 loss 0.652  acc 0.604: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.641  acc 0.622: 100%|█████████▉| 408/410 [01:56<00:00,  3.49it/s]
epoch 4 loss 0.642  acc 0.622: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 4 loss 0.642  acc 0.622: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 4 loss 0.641  acc 0.622: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 4 loss 0.641  acc 0.622: 100%|██████████| 410/410 [01:57<00:00,  3.77it/s]
epoch 4 loss 0.641  acc 0.622: 100%|██████████| 410/410 [01:57<00:00,  3.50it/s]
INFO:name:Cycle 4, the best candidate in pop has score 0.62, and best in the run 0.62
INFO:name:Mean 0.5947333333333333 and standard deviation 0.010633072097104505 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 20:56:38,250 >> Trainable Ratio: 2036241/126091281=1.614894%
[INFO|(OpenDelta)basemodel:702]2024-04-20 20:56:38,251 >> Delta Parameter Ratio: 1444880/126091281=1.145900%
[INFO|(OpenDelta)basemodel:704]2024-04-20 20:56:38,251 >> Static Memory 0.95 GB, Max Memory 9.88 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.692  acc 0.527: 100%|█████████▉| 408/410 [01:38<00:00,  4.10it/s]
epoch 0 loss 0.692  acc 0.527: 100%|█████████▉| 408/410 [01:38<00:00,  4.10it/s]
epoch 0 loss 0.692  acc 0.527: 100%|█████████▉| 409/410 [01:38<00:00,  4.10it/s]
epoch 0 loss 0.692  acc 0.527: 100%|█████████▉| 409/410 [01:39<00:00,  4.10it/s]
epoch 0 loss 0.692  acc 0.527: 100%|██████████| 410/410 [01:39<00:00,  4.43it/s]
epoch 0 loss 0.692  acc 0.527: 100%|██████████| 410/410 [01:39<00:00,  4.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.678  acc 0.568: 100%|█████████▉| 408/410 [01:38<00:00,  4.17it/s]
epoch 1 loss 0.678  acc 0.568: 100%|█████████▉| 408/410 [01:38<00:00,  4.17it/s]
epoch 1 loss 0.678  acc 0.568: 100%|█████████▉| 409/410 [01:38<00:00,  4.17it/s]
epoch 1 loss 0.678  acc 0.569: 100%|█████████▉| 409/410 [01:38<00:00,  4.17it/s]
epoch 1 loss 0.678  acc 0.569: 100%|██████████| 410/410 [01:38<00:00,  4.50it/s]
epoch 1 loss 0.678  acc 0.569: 100%|██████████| 410/410 [01:38<00:00,  4.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.665  acc 0.586: 100%|█████████▉| 408/410 [01:37<00:00,  4.18it/s]
epoch 2 loss 0.665  acc 0.585: 100%|█████████▉| 408/410 [01:38<00:00,  4.18it/s]
epoch 2 loss 0.665  acc 0.585: 100%|█████████▉| 409/410 [01:38<00:00,  4.18it/s]
epoch 2 loss 0.665  acc 0.585: 100%|█████████▉| 409/410 [01:38<00:00,  4.18it/s]
epoch 2 loss 0.665  acc 0.585: 100%|██████████| 410/410 [01:38<00:00,  4.51it/s]
epoch 2 loss 0.665  acc 0.585: 100%|██████████| 410/410 [01:38<00:00,  4.17it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.649  acc 0.609: 100%|█████████▉| 408/410 [01:37<00:00,  4.16it/s]
epoch 3 loss 0.649  acc 0.609: 100%|█████████▉| 408/410 [01:38<00:00,  4.16it/s]
epoch 3 loss 0.649  acc 0.609: 100%|█████████▉| 409/410 [01:38<00:00,  4.16it/s]
epoch 3 loss 0.649  acc 0.61: 100%|█████████▉| 409/410 [01:38<00:00,  4.16it/s] 
epoch 3 loss 0.649  acc 0.61: 100%|██████████| 410/410 [01:38<00:00,  4.50it/s]
epoch 3 loss 0.649  acc 0.61: 100%|██████████| 410/410 [01:38<00:00,  4.17it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.638  acc 0.614: 100%|█████████▉| 408/410 [01:38<00:00,  4.17it/s]
epoch 4 loss 0.638  acc 0.614: 100%|█████████▉| 408/410 [01:38<00:00,  4.17it/s]
epoch 4 loss 0.638  acc 0.614: 100%|█████████▉| 409/410 [01:38<00:00,  4.17it/s]
epoch 4 loss 0.638  acc 0.613: 100%|█████████▉| 409/410 [01:38<00:00,  4.17it/s]
epoch 4 loss 0.638  acc 0.613: 100%|██████████| 410/410 [01:38<00:00,  4.50it/s]
epoch 4 loss 0.638  acc 0.613: 100%|██████████| 410/410 [01:38<00:00,  4.16it/s]
INFO:name:Cycle 5, the best candidate in pop has score 0.62, and best in the run 0.62
INFO:name:Mean 0.5937666666666667 and standard deviation 0.009854553375076031 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 21:05:45,351 >> Trainable Ratio: 3886641/127941681=3.037822%
[INFO|(OpenDelta)basemodel:702]2024-04-20 21:05:45,351 >> Delta Parameter Ratio: 3295280/127941681=2.575611%
[INFO|(OpenDelta)basemodel:704]2024-04-20 21:05:45,351 >> Static Memory 0.95 GB, Max Memory 9.88 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.541: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 0 loss 0.689  acc 0.541: 100%|█████████▉| 408/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.689  acc 0.541: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.689  acc 0.54: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s] 
epoch 0 loss 0.689  acc 0.54: 100%|██████████| 410/410 [02:10<00:00,  3.42it/s]
epoch 0 loss 0.689  acc 0.54: 100%|██████████| 410/410 [02:10<00:00,  3.15it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.673  acc 0.573: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.573: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.573: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.573: 100%|█████████▉| 409/410 [02:09<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.573: 100%|██████████| 410/410 [02:09<00:00,  3.44it/s]
epoch 1 loss 0.673  acc 0.573: 100%|██████████| 410/410 [02:09<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.659  acc 0.587: 100%|█████████▉| 408/410 [02:08<00:00,  3.17it/s]
epoch 2 loss 0.659  acc 0.587: 100%|█████████▉| 408/410 [02:08<00:00,  3.17it/s]
epoch 2 loss 0.659  acc 0.587: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.659  acc 0.587: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.659  acc 0.587: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 2 loss 0.659  acc 0.587: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.639  acc 0.618: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.639  acc 0.618: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.639  acc 0.618: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.639  acc 0.618: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.639  acc 0.618: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 3 loss 0.639  acc 0.618: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.623  acc 0.644: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.623  acc 0.645: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.623  acc 0.645: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.623  acc 0.644: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.623  acc 0.644: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 4 loss 0.623  acc 0.644: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]
INFO:name:Cycle 6, the best candidate in pop has score 0.628, and best in the run 0.628
INFO:name:Mean 0.5949333333333334 and standard deviation 0.011610148817114955 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 21:17:26,152 >> Trainable Ratio: 4151961/128207001=3.238482%
[INFO|(OpenDelta)basemodel:702]2024-04-20 21:17:26,152 >> Delta Parameter Ratio: 3560600/128207001=2.777227%
[INFO|(OpenDelta)basemodel:704]2024-04-20 21:17:26,152 >> Static Memory 1.43 GB, Max Memory 9.91 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.691  acc 0.531: 100%|█████████▉| 408/410 [02:12<00:00,  3.09it/s]
epoch 0 loss 0.691  acc 0.531: 100%|█████████▉| 408/410 [02:12<00:00,  3.09it/s]
epoch 0 loss 0.691  acc 0.531: 100%|█████████▉| 409/410 [02:12<00:00,  3.09it/s]
epoch 0 loss 0.691  acc 0.531: 100%|█████████▉| 409/410 [02:12<00:00,  3.09it/s]
epoch 0 loss 0.691  acc 0.531: 100%|██████████| 410/410 [02:12<00:00,  3.34it/s]
epoch 0 loss 0.691  acc 0.531: 100%|██████████| 410/410 [02:12<00:00,  3.08it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.675  acc 0.576: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 1 loss 0.675  acc 0.576: 100%|█████████▉| 408/410 [02:11<00:00,  3.12it/s]
epoch 1 loss 0.675  acc 0.576: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 1 loss 0.675  acc 0.576: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 1 loss 0.675  acc 0.576: 100%|██████████| 410/410 [02:11<00:00,  3.37it/s]
epoch 1 loss 0.675  acc 0.576: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.66  acc 0.589: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 2 loss 0.66  acc 0.589: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 2 loss 0.66  acc 0.589: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 2 loss 0.66  acc 0.589: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 2 loss 0.66  acc 0.589: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 2 loss 0.66  acc 0.589: 100%|██████████| 410/410 [02:11<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.646  acc 0.614: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.646  acc 0.614: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.646  acc 0.614: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.646  acc 0.614: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 3 loss 0.646  acc 0.614: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 3 loss 0.646  acc 0.614: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.63  acc 0.629: 100%|█████████▉| 408/410 [02:10<00:00,  3.11it/s]
epoch 4 loss 0.63  acc 0.629: 100%|█████████▉| 408/410 [02:11<00:00,  3.11it/s]
epoch 4 loss 0.63  acc 0.629: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 4 loss 0.631  acc 0.629: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 4 loss 0.631  acc 0.629: 100%|██████████| 410/410 [02:11<00:00,  3.37it/s]
epoch 4 loss 0.631  acc 0.629: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]
INFO:name:Cycle 7, the best candidate in pop has score 0.628, and best in the run 0.628
INFO:name:Mean 0.5951666666666668 and standard deviation 0.011457409053630862 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 21:29:19,253 >> Trainable Ratio: 2126713/126181753=1.685436%
[INFO|(OpenDelta)basemodel:702]2024-04-20 21:29:19,253 >> Delta Parameter Ratio: 1535352/126181753=1.216778%
[INFO|(OpenDelta)basemodel:704]2024-04-20 21:29:19,253 >> Static Memory 0.96 GB, Max Memory 9.91 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.54: 100%|█████████▉| 408/410 [01:58<00:00,  3.46it/s]
epoch 0 loss 0.69  acc 0.54: 100%|█████████▉| 408/410 [01:58<00:00,  3.46it/s]
epoch 0 loss 0.69  acc 0.54: 100%|█████████▉| 409/410 [01:58<00:00,  3.45it/s]
epoch 0 loss 0.69  acc 0.54: 100%|█████████▉| 409/410 [01:58<00:00,  3.45it/s]
epoch 0 loss 0.69  acc 0.54: 100%|██████████| 410/410 [01:58<00:00,  3.73it/s]
epoch 0 loss 0.69  acc 0.54: 100%|██████████| 410/410 [01:58<00:00,  3.45it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.678  acc 0.566: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 1 loss 0.678  acc 0.566: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 1 loss 0.678  acc 0.566: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 1 loss 0.678  acc 0.567: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 1 loss 0.678  acc 0.567: 100%|██████████| 410/410 [01:57<00:00,  3.77it/s]
epoch 1 loss 0.678  acc 0.567: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.663  acc 0.588: 100%|█████████▉| 408/410 [01:56<00:00,  3.51it/s]
epoch 2 loss 0.663  acc 0.588: 100%|█████████▉| 408/410 [01:56<00:00,  3.51it/s]
epoch 2 loss 0.663  acc 0.588: 100%|█████████▉| 409/410 [01:56<00:00,  3.50it/s]
epoch 2 loss 0.663  acc 0.588: 100%|█████████▉| 409/410 [01:57<00:00,  3.50it/s]
epoch 2 loss 0.663  acc 0.588: 100%|██████████| 410/410 [01:57<00:00,  3.79it/s]
epoch 2 loss 0.663  acc 0.588: 100%|██████████| 410/410 [01:57<00:00,  3.50it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.654  acc 0.605: 100%|█████████▉| 408/410 [01:56<00:00,  3.50it/s]
epoch 3 loss 0.654  acc 0.605: 100%|█████████▉| 408/410 [01:57<00:00,  3.50it/s]
epoch 3 loss 0.654  acc 0.605: 100%|█████████▉| 409/410 [01:57<00:00,  3.50it/s]
epoch 3 loss 0.654  acc 0.605: 100%|█████████▉| 409/410 [01:57<00:00,  3.50it/s]
epoch 3 loss 0.654  acc 0.605: 100%|██████████| 410/410 [01:57<00:00,  3.78it/s]
epoch 3 loss 0.654  acc 0.605: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.645  acc 0.611: 100%|█████████▉| 408/410 [01:56<00:00,  3.49it/s]
epoch 4 loss 0.645  acc 0.611: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 4 loss 0.645  acc 0.611: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 4 loss 0.644  acc 0.611: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 4 loss 0.644  acc 0.611: 100%|██████████| 410/410 [01:57<00:00,  3.77it/s]
epoch 4 loss 0.644  acc 0.611: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]
INFO:name:Cycle 8, the best candidate in pop has score 0.628, and best in the run 0.628
INFO:name:Mean 0.5961333333333333 and standard deviation 0.011777756813398543 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 21:40:00,620 >> Trainable Ratio: 2910905/126965945=2.292666%
[INFO|(OpenDelta)basemodel:702]2024-04-20 21:40:00,620 >> Delta Parameter Ratio: 2319544/126965945=1.826902%
[INFO|(OpenDelta)basemodel:704]2024-04-20 21:40:00,620 >> Static Memory 1.43 GB, Max Memory 9.91 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.688  acc 0.542: 100%|█████████▉| 408/410 [02:09<00:00,  3.18it/s]
epoch 0 loss 0.688  acc 0.542: 100%|█████████▉| 408/410 [02:09<00:00,  3.18it/s]
epoch 0 loss 0.688  acc 0.542: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 0 loss 0.688  acc 0.542: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 0 loss 0.688  acc 0.542: 100%|██████████| 410/410 [02:09<00:00,  3.44it/s]
epoch 0 loss 0.688  acc 0.542: 100%|██████████| 410/410 [02:09<00:00,  3.16it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.675  acc 0.566: 100%|█████████▉| 408/410 [02:07<00:00,  3.20it/s]
epoch 1 loss 0.675  acc 0.566: 100%|█████████▉| 408/410 [02:08<00:00,  3.20it/s]
epoch 1 loss 0.675  acc 0.566: 100%|█████████▉| 409/410 [02:08<00:00,  3.20it/s]
epoch 1 loss 0.675  acc 0.566: 100%|█████████▉| 409/410 [02:08<00:00,  3.20it/s]
epoch 1 loss 0.675  acc 0.566: 100%|██████████| 410/410 [02:08<00:00,  3.46it/s]
epoch 1 loss 0.675  acc 0.566: 100%|██████████| 410/410 [02:08<00:00,  3.19it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.663  acc 0.586: 100%|█████████▉| 408/410 [02:07<00:00,  3.19it/s]
epoch 2 loss 0.663  acc 0.586: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.663  acc 0.586: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.663  acc 0.586: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.663  acc 0.586: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 2 loss 0.663  acc 0.586: 100%|██████████| 410/410 [02:08<00:00,  3.20it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.644  acc 0.617: 100%|█████████▉| 408/410 [02:07<00:00,  3.20it/s]
epoch 3 loss 0.644  acc 0.617: 100%|█████████▉| 408/410 [02:08<00:00,  3.20it/s]
epoch 3 loss 0.644  acc 0.617: 100%|█████████▉| 409/410 [02:08<00:00,  3.20it/s]
epoch 3 loss 0.644  acc 0.617: 100%|█████████▉| 409/410 [02:08<00:00,  3.20it/s]
epoch 3 loss 0.644  acc 0.617: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 3 loss 0.644  acc 0.617: 100%|██████████| 410/410 [02:08<00:00,  3.19it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.635  acc 0.627: 100%|█████████▉| 408/410 [02:07<00:00,  3.19it/s]
epoch 4 loss 0.635  acc 0.627: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.635  acc 0.627: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.635  acc 0.627: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.635  acc 0.627: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 4 loss 0.635  acc 0.627: 100%|██████████| 410/410 [02:08<00:00,  3.19it/s]
INFO:name:Cycle 9, the best candidate in pop has score 0.628, and best in the run 0.628
INFO:name:Mean 0.5951666666666667 and standard deviation 0.010939479979515592 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 21:51:38,415 >> Trainable Ratio: 4074849/128129889=3.180249%
[INFO|(OpenDelta)basemodel:702]2024-04-20 21:51:38,415 >> Delta Parameter Ratio: 3483488/128129889=2.718716%
[INFO|(OpenDelta)basemodel:704]2024-04-20 21:51:38,415 >> Static Memory 0.96 GB, Max Memory 9.91 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.688  acc 0.54: 100%|█████████▉| 408/410 [02:11<00:00,  3.13it/s]
epoch 0 loss 0.688  acc 0.54: 100%|█████████▉| 408/410 [02:11<00:00,  3.13it/s]
epoch 0 loss 0.688  acc 0.54: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 0 loss 0.688  acc 0.539: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 0 loss 0.688  acc 0.539: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 0 loss 0.688  acc 0.539: 100%|██████████| 410/410 [02:12<00:00,  3.11it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.673  acc 0.57: 100%|█████████▉| 408/410 [02:09<00:00,  3.14it/s]
epoch 1 loss 0.673  acc 0.57: 100%|█████████▉| 408/410 [02:10<00:00,  3.14it/s]
epoch 1 loss 0.673  acc 0.57: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 1 loss 0.674  acc 0.57: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 1 loss 0.674  acc 0.57: 100%|██████████| 410/410 [02:10<00:00,  3.39it/s]
epoch 1 loss 0.674  acc 0.57: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.662  acc 0.588: 100%|█████████▉| 408/410 [02:09<00:00,  3.15it/s]
epoch 2 loss 0.662  acc 0.588: 100%|█████████▉| 408/410 [02:10<00:00,  3.15it/s]
epoch 2 loss 0.662  acc 0.588: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 2 loss 0.662  acc 0.588: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 2 loss 0.662  acc 0.588: 100%|██████████| 410/410 [02:10<00:00,  3.40it/s]
epoch 2 loss 0.662  acc 0.588: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.643  acc 0.618: 100%|█████████▉| 408/410 [02:09<00:00,  3.14it/s]
epoch 3 loss 0.643  acc 0.618: 100%|█████████▉| 408/410 [02:09<00:00,  3.14it/s]
epoch 3 loss 0.643  acc 0.618: 100%|█████████▉| 409/410 [02:09<00:00,  3.15it/s]
epoch 3 loss 0.643  acc 0.618: 100%|█████████▉| 409/410 [02:10<00:00,  3.15it/s]
epoch 3 loss 0.643  acc 0.618: 100%|██████████| 410/410 [02:10<00:00,  3.40it/s]
epoch 3 loss 0.643  acc 0.618: 100%|██████████| 410/410 [02:10<00:00,  3.15it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.63  acc 0.632: 100%|█████████▉| 408/410 [02:09<00:00,  3.15it/s]
epoch 4 loss 0.63  acc 0.632: 100%|█████████▉| 408/410 [02:10<00:00,  3.15it/s]
epoch 4 loss 0.63  acc 0.632: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 4 loss 0.63  acc 0.632: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 4 loss 0.63  acc 0.632: 100%|██████████| 410/410 [02:10<00:00,  3.40it/s]
epoch 4 loss 0.63  acc 0.632: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]
INFO:name:Cycle 10, the best candidate in pop has score 0.628, and best in the run 0.628
INFO:name:Mean 0.5955 and standard deviation 0.010959774936861929 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 22:03:26,712 >> Trainable Ratio: 3886641/127941681=3.037822%
[INFO|(OpenDelta)basemodel:702]2024-04-20 22:03:26,712 >> Delta Parameter Ratio: 3295280/127941681=2.575611%
[INFO|(OpenDelta)basemodel:704]2024-04-20 22:03:26,712 >> Static Memory 1.44 GB, Max Memory 10.04 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.691  acc 0.537: 100%|█████████▉| 408/410 [02:09<00:00,  3.14it/s]
epoch 0 loss 0.691  acc 0.537: 100%|█████████▉| 408/410 [02:09<00:00,  3.14it/s]
epoch 0 loss 0.691  acc 0.537: 100%|█████████▉| 409/410 [02:09<00:00,  3.14it/s]
epoch 0 loss 0.691  acc 0.537: 100%|█████████▉| 409/410 [02:09<00:00,  3.14it/s]
epoch 0 loss 0.691  acc 0.537: 100%|██████████| 410/410 [02:09<00:00,  3.39it/s]
epoch 0 loss 0.691  acc 0.537: 100%|██████████| 410/410 [02:09<00:00,  3.16it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.675  acc 0.568: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 1 loss 0.675  acc 0.568: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 1 loss 0.675  acc 0.568: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 1 loss 0.675  acc 0.567: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 1 loss 0.675  acc 0.567: 100%|██████████| 410/410 [02:09<00:00,  3.42it/s]
epoch 1 loss 0.675  acc 0.567: 100%|██████████| 410/410 [02:09<00:00,  3.15it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.66  acc 0.59: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.659  acc 0.591: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.659  acc 0.591: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.659  acc 0.591: 100%|█████████▉| 409/410 [02:09<00:00,  3.19it/s]
epoch 2 loss 0.659  acc 0.591: 100%|██████████| 410/410 [02:09<00:00,  3.45it/s]
epoch 2 loss 0.659  acc 0.591: 100%|██████████| 410/410 [02:09<00:00,  3.17it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.641  acc 0.615: 100%|█████████▉| 408/410 [02:08<00:00,  3.17it/s]
epoch 3 loss 0.641  acc 0.615: 100%|█████████▉| 408/410 [02:08<00:00,  3.17it/s]
epoch 3 loss 0.641  acc 0.615: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.64  acc 0.615: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s] 
epoch 3 loss 0.64  acc 0.615: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 3 loss 0.64  acc 0.615: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.629  acc 0.636: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.628  acc 0.636: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.628  acc 0.636: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.628  acc 0.637: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 4 loss 0.628  acc 0.637: 100%|██████████| 410/410 [02:09<00:00,  3.44it/s]
epoch 4 loss 0.628  acc 0.637: 100%|██████████| 410/410 [02:09<00:00,  3.18it/s]
INFO:name:Cycle 11, the best candidate in pop has score 0.628, and best in the run 0.628
INFO:name:Mean 0.5962666666666666 and standard deviation 0.011413247663229886 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 22:15:08,257 >> Trainable Ratio: 3886641/127941681=3.037822%
[INFO|(OpenDelta)basemodel:702]2024-04-20 22:15:08,257 >> Delta Parameter Ratio: 3295280/127941681=2.575611%
[INFO|(OpenDelta)basemodel:704]2024-04-20 22:15:08,257 >> Static Memory 0.96 GB, Max Memory 10.04 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.69  acc 0.536: 100%|█████████▉| 408/410 [02:09<00:00,  3.13it/s]
epoch 0 loss 0.69  acc 0.537: 100%|█████████▉| 408/410 [02:09<00:00,  3.13it/s]
epoch 0 loss 0.69  acc 0.537: 100%|█████████▉| 409/410 [02:09<00:00,  3.13it/s]
epoch 0 loss 0.69  acc 0.537: 100%|█████████▉| 409/410 [02:09<00:00,  3.13it/s]
epoch 0 loss 0.69  acc 0.537: 100%|██████████| 410/410 [02:09<00:00,  3.39it/s]
epoch 0 loss 0.69  acc 0.537: 100%|██████████| 410/410 [02:09<00:00,  3.16it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.673  acc 0.571: 100%|█████████▉| 408/410 [02:09<00:00,  3.12it/s]
epoch 1 loss 0.673  acc 0.571: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 1 loss 0.673  acc 0.571: 100%|█████████▉| 409/410 [02:10<00:00,  3.12it/s]
epoch 1 loss 0.673  acc 0.571: 100%|█████████▉| 409/410 [02:10<00:00,  3.12it/s]
epoch 1 loss 0.673  acc 0.571: 100%|██████████| 410/410 [02:10<00:00,  3.37it/s]
epoch 1 loss 0.673  acc 0.571: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.659  acc 0.596: 100%|█████████▉| 408/410 [02:10<00:00,  3.17it/s]
epoch 2 loss 0.66  acc 0.596: 100%|█████████▉| 408/410 [02:10<00:00,  3.17it/s] 
epoch 2 loss 0.66  acc 0.596: 100%|█████████▉| 409/410 [02:10<00:00,  3.17it/s]
epoch 2 loss 0.66  acc 0.595: 100%|█████████▉| 409/410 [02:10<00:00,  3.17it/s]
epoch 2 loss 0.66  acc 0.595: 100%|██████████| 410/410 [02:10<00:00,  3.42it/s]
epoch 2 loss 0.66  acc 0.595: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.642  acc 0.618: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.642  acc 0.618: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.642  acc 0.618: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.642  acc 0.618: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 3 loss 0.642  acc 0.618: 100%|██████████| 410/410 [02:09<00:00,  3.44it/s]
epoch 3 loss 0.642  acc 0.618: 100%|██████████| 410/410 [02:09<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.631  acc 0.628: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.63  acc 0.629: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s] 
epoch 4 loss 0.63  acc 0.629: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.631  acc 0.629: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.631  acc 0.629: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 4 loss 0.631  acc 0.629: 100%|██████████| 410/410 [02:09<00:00,  3.18it/s]
INFO:name:Cycle 12, the best candidate in pop has score 0.628, and best in the run 0.628
INFO:name:Mean 0.5969666666666668 and standard deviation 0.012349044047572637 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 22:26:52,349 >> Trainable Ratio: 3886641/127941681=3.037822%
[INFO|(OpenDelta)basemodel:702]2024-04-20 22:26:52,349 >> Delta Parameter Ratio: 3295280/127941681=2.575611%
[INFO|(OpenDelta)basemodel:704]2024-04-20 22:26:52,349 >> Static Memory 1.44 GB, Max Memory 10.04 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.688  acc 0.544: 100%|█████████▉| 408/410 [02:09<00:00,  3.17it/s]
epoch 0 loss 0.688  acc 0.544: 100%|█████████▉| 408/410 [02:09<00:00,  3.17it/s]
epoch 0 loss 0.688  acc 0.544: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 0 loss 0.688  acc 0.544: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.688  acc 0.544: 100%|██████████| 410/410 [02:10<00:00,  3.42it/s]
epoch 0 loss 0.688  acc 0.544: 100%|██████████| 410/410 [02:10<00:00,  3.15it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.672  acc 0.574: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.672  acc 0.574: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.672  acc 0.574: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.672  acc 0.575: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.672  acc 0.575: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 1 loss 0.672  acc 0.575: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.661  acc 0.588: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.661  acc 0.588: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.661  acc 0.588: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.661  acc 0.588: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.661  acc 0.588: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 2 loss 0.661  acc 0.588: 100%|██████████| 410/410 [02:08<00:00,  3.19it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.643  acc 0.617: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.643  acc 0.618: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.643  acc 0.618: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.643  acc 0.617: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.643  acc 0.617: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 3 loss 0.643  acc 0.617: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.626  acc 0.638: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.626  acc 0.638: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.626  acc 0.638: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.626  acc 0.638: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.626  acc 0.638: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 4 loss 0.626  acc 0.638: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]
INFO:name:Cycle 13, the best candidate in pop has score 0.635, and best in the run 0.635
INFO:name:Mean 0.5980000000000001 and standard deviation 0.014071247279470301 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 22:38:32,715 >> Trainable Ratio: 3886641/127941681=3.037822%
[INFO|(OpenDelta)basemodel:702]2024-04-20 22:38:32,715 >> Delta Parameter Ratio: 3295280/127941681=2.575611%
[INFO|(OpenDelta)basemodel:704]2024-04-20 22:38:32,715 >> Static Memory 0.96 GB, Max Memory 10.04 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.688  acc 0.536: 100%|█████████▉| 408/410 [02:09<00:00,  3.14it/s]
epoch 0 loss 0.688  acc 0.536: 100%|█████████▉| 408/410 [02:10<00:00,  3.14it/s]
epoch 0 loss 0.688  acc 0.536: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 0 loss 0.688  acc 0.536: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 0 loss 0.688  acc 0.536: 100%|██████████| 410/410 [02:10<00:00,  3.40it/s]
epoch 0 loss 0.688  acc 0.536: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.676  acc 0.571: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.676  acc 0.571: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.676  acc 0.571: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.676  acc 0.571: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.676  acc 0.571: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 1 loss 0.676  acc 0.571: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.656  acc 0.593: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.594: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.594: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.594: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.594: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 2 loss 0.656  acc 0.594: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.644  acc 0.615: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.644  acc 0.614: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.644  acc 0.614: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.644  acc 0.614: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.644  acc 0.614: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 3 loss 0.644  acc 0.614: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.631  acc 0.626: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.631  acc 0.626: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.631  acc 0.626: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.631  acc 0.626: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.631  acc 0.626: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 4 loss 0.631  acc 0.626: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]
INFO:name:Cycle 14, the best candidate in pop has score 0.635, and best in the run 0.635
INFO:name:Mean 0.5993 and standard deviation 0.014690473103341511 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 22:50:13,134 >> Trainable Ratio: 3886641/127941681=3.037822%
[INFO|(OpenDelta)basemodel:702]2024-04-20 22:50:13,135 >> Delta Parameter Ratio: 3295280/127941681=2.575611%
[INFO|(OpenDelta)basemodel:704]2024-04-20 22:50:13,135 >> Static Memory 1.44 GB, Max Memory 10.04 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.688  acc 0.533: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 0 loss 0.688  acc 0.533: 100%|█████████▉| 408/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.688  acc 0.533: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.688  acc 0.533: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.688  acc 0.533: 100%|██████████| 410/410 [02:10<00:00,  3.42it/s]
epoch 0 loss 0.688  acc 0.533: 100%|██████████| 410/410 [02:10<00:00,  3.15it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.673  acc 0.568: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.569: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.569: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.672  acc 0.57: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s] 
epoch 1 loss 0.672  acc 0.57: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 1 loss 0.672  acc 0.57: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.657  acc 0.596: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.657  acc 0.596: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.657  acc 0.596: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.657  acc 0.596: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 2 loss 0.657  acc 0.596: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 2 loss 0.657  acc 0.596: 100%|██████████| 410/410 [02:08<00:00,  3.19it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.639  acc 0.623: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.639  acc 0.623: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.639  acc 0.623: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.639  acc 0.623: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.639  acc 0.623: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 3 loss 0.639  acc 0.623: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.625  acc 0.64: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.625  acc 0.64: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.625  acc 0.64: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.624  acc 0.641: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.624  acc 0.641: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 4 loss 0.624  acc 0.641: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]
INFO:name:Cycle 15, the best candidate in pop has score 0.635, and best in the run 0.635
INFO:name:Mean 0.5999666666666668 and standard deviation 0.015065376050917414 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 23:01:53,418 >> Trainable Ratio: 3886641/127941681=3.037822%
[INFO|(OpenDelta)basemodel:702]2024-04-20 23:01:53,418 >> Delta Parameter Ratio: 3295280/127941681=2.575611%
[INFO|(OpenDelta)basemodel:704]2024-04-20 23:01:53,418 >> Static Memory 0.96 GB, Max Memory 10.04 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.689  acc 0.545: 100%|█████████▉| 408/410 [02:09<00:00,  3.15it/s]
epoch 0 loss 0.689  acc 0.545: 100%|█████████▉| 408/410 [02:10<00:00,  3.15it/s]
epoch 0 loss 0.689  acc 0.545: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.689  acc 0.545: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.689  acc 0.545: 100%|██████████| 410/410 [02:10<00:00,  3.41it/s]
epoch 0 loss 0.689  acc 0.545: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.677  acc 0.564: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.676  acc 0.564: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.676  acc 0.564: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.677  acc 0.564: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 1 loss 0.677  acc 0.564: 100%|██████████| 410/410 [02:09<00:00,  3.44it/s]
epoch 1 loss 0.677  acc 0.564: 100%|██████████| 410/410 [02:09<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.658  acc 0.591: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.658  acc 0.591: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.658  acc 0.591: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.657  acc 0.591: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.657  acc 0.591: 100%|██████████| 410/410 [02:08<00:00,  3.42it/s]
epoch 2 loss 0.657  acc 0.591: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.645  acc 0.615: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.645  acc 0.615: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.645  acc 0.615: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.645  acc 0.615: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.645  acc 0.615: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 3 loss 0.645  acc 0.615: 100%|██████████| 410/410 [02:09<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.628  acc 0.638: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.628  acc 0.638: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.628  acc 0.638: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.628  acc 0.638: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.628  acc 0.638: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 4 loss 0.628  acc 0.638: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]
INFO:name:Cycle 16, the best candidate in pop has score 0.635, and best in the run 0.635
INFO:name:Mean 0.6006333333333332 and standard deviation 0.015140416844401037 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 23:13:34,380 >> Trainable Ratio: 3924297/127979337=3.066352%
[INFO|(OpenDelta)basemodel:702]2024-04-20 23:13:34,380 >> Delta Parameter Ratio: 3332936/127979337=2.604277%
[INFO|(OpenDelta)basemodel:704]2024-04-20 23:13:34,380 >> Static Memory 1.44 GB, Max Memory 10.04 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.693  acc 0.537: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 0 loss 0.693  acc 0.537: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 0 loss 0.693  acc 0.537: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 0 loss 0.693  acc 0.537: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 0 loss 0.693  acc 0.537: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 0 loss 0.693  acc 0.537: 100%|██████████| 410/410 [02:11<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.677  acc 0.564: 100%|█████████▉| 408/410 [02:09<00:00,  3.17it/s]
epoch 1 loss 0.676  acc 0.564: 100%|█████████▉| 408/410 [02:09<00:00,  3.17it/s]
epoch 1 loss 0.676  acc 0.564: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 1 loss 0.676  acc 0.564: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 1 loss 0.676  acc 0.564: 100%|██████████| 410/410 [02:09<00:00,  3.42it/s]
epoch 1 loss 0.676  acc 0.564: 100%|██████████| 410/410 [02:10<00:00,  3.15it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.66  acc 0.596: 100%|█████████▉| 408/410 [02:09<00:00,  3.17it/s]
epoch 2 loss 0.66  acc 0.596: 100%|█████████▉| 408/410 [02:09<00:00,  3.17it/s]
epoch 2 loss 0.66  acc 0.596: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 2 loss 0.66  acc 0.595: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 2 loss 0.66  acc 0.595: 100%|██████████| 410/410 [02:09<00:00,  3.42it/s]
epoch 2 loss 0.66  acc 0.595: 100%|██████████| 410/410 [02:09<00:00,  3.16it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.645  acc 0.613: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.644  acc 0.613: 100%|█████████▉| 408/410 [02:09<00:00,  3.18it/s]
epoch 3 loss 0.644  acc 0.613: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 3 loss 0.644  acc 0.614: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 3 loss 0.644  acc 0.614: 100%|██████████| 410/410 [02:09<00:00,  3.43it/s]
epoch 3 loss 0.644  acc 0.614: 100%|██████████| 410/410 [02:09<00:00,  3.17it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.627  acc 0.636: 100%|█████████▉| 408/410 [02:08<00:00,  3.17it/s]
epoch 4 loss 0.627  acc 0.637: 100%|█████████▉| 408/410 [02:09<00:00,  3.17it/s]
epoch 4 loss 0.627  acc 0.637: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 4 loss 0.627  acc 0.637: 100%|█████████▉| 409/410 [02:09<00:00,  3.17it/s]
epoch 4 loss 0.627  acc 0.637: 100%|██████████| 410/410 [02:09<00:00,  3.43it/s]
epoch 4 loss 0.627  acc 0.637: 100%|██████████| 410/410 [02:09<00:00,  3.17it/s]
INFO:name:Cycle 17, the best candidate in pop has score 0.635, and best in the run 0.635
INFO:name:Mean 0.6013999999999999 and standard deviation 0.015215343133385692 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 23:25:19,079 >> Trainable Ratio: 3886641/127941681=3.037822%
[INFO|(OpenDelta)basemodel:702]2024-04-20 23:25:19,079 >> Delta Parameter Ratio: 3295280/127941681=2.575611%
[INFO|(OpenDelta)basemodel:704]2024-04-20 23:25:19,079 >> Static Memory 0.96 GB, Max Memory 10.04 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.691  acc 0.537: 100%|█████████▉| 408/410 [02:09<00:00,  3.15it/s]
epoch 0 loss 0.691  acc 0.537: 100%|█████████▉| 408/410 [02:10<00:00,  3.15it/s]
epoch 0 loss 0.691  acc 0.537: 100%|█████████▉| 409/410 [02:10<00:00,  3.15it/s]
epoch 0 loss 0.691  acc 0.538: 100%|█████████▉| 409/410 [02:10<00:00,  3.15it/s]
epoch 0 loss 0.691  acc 0.538: 100%|██████████| 410/410 [02:10<00:00,  3.41it/s]
epoch 0 loss 0.691  acc 0.538: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.674  acc 0.571: 100%|█████████▉| 408/410 [02:08<00:00,  3.17it/s]
epoch 1 loss 0.674  acc 0.571: 100%|█████████▉| 408/410 [02:08<00:00,  3.17it/s]
epoch 1 loss 0.674  acc 0.571: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.674  acc 0.571: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 1 loss 0.674  acc 0.571: 100%|██████████| 410/410 [02:09<00:00,  3.43it/s]
epoch 1 loss 0.674  acc 0.571: 100%|██████████| 410/410 [02:09<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.656  acc 0.599: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.599: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.599: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.599: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.599: 100%|██████████| 410/410 [02:08<00:00,  3.42it/s]
epoch 2 loss 0.656  acc 0.599: 100%|██████████| 410/410 [02:09<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.642  acc 0.614: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.642  acc 0.613: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.642  acc 0.613: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.642  acc 0.614: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.642  acc 0.614: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 3 loss 0.642  acc 0.614: 100%|██████████| 410/410 [02:09<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.624  acc 0.639: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.624  acc 0.639: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.624  acc 0.639: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.624  acc 0.639: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.624  acc 0.639: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 4 loss 0.624  acc 0.639: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]
INFO:name:Cycle 18, the best candidate in pop has score 0.635, and best in the run 0.635
INFO:name:Mean 0.6023 and standard deviation 0.014774189205051727 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 23:37:00,845 >> Trainable Ratio: 3886641/127941681=3.037822%
[INFO|(OpenDelta)basemodel:702]2024-04-20 23:37:00,845 >> Delta Parameter Ratio: 3295280/127941681=2.575611%
[INFO|(OpenDelta)basemodel:704]2024-04-20 23:37:00,845 >> Static Memory 0.96 GB, Max Memory 10.04 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.688  acc 0.544: 100%|█████████▉| 408/410 [02:09<00:00,  3.15it/s]
epoch 0 loss 0.688  acc 0.544: 100%|█████████▉| 408/410 [02:10<00:00,  3.15it/s]
epoch 0 loss 0.688  acc 0.544: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.688  acc 0.544: 100%|█████████▉| 409/410 [02:10<00:00,  3.16it/s]
epoch 0 loss 0.688  acc 0.544: 100%|██████████| 410/410 [02:10<00:00,  3.41it/s]
epoch 0 loss 0.688  acc 0.544: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.678  acc 0.554: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.678  acc 0.554: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.678  acc 0.554: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.677  acc 0.555: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 1 loss 0.677  acc 0.555: 100%|██████████| 410/410 [02:09<00:00,  3.43it/s]
epoch 1 loss 0.677  acc 0.555: 100%|██████████| 410/410 [02:09<00:00,  3.17it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.657  acc 0.598: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.598: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.598: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.598: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.656  acc 0.598: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 2 loss 0.656  acc 0.598: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.643  acc 0.612: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.644  acc 0.612: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.644  acc 0.612: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.644  acc 0.612: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.644  acc 0.612: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 3 loss 0.644  acc 0.612: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.624  acc 0.643: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.623  acc 0.643: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.623  acc 0.643: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.623  acc 0.644: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 4 loss 0.623  acc 0.644: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 4 loss 0.623  acc 0.644: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]
INFO:name:Cycle 19, the best candidate in pop has score 0.635, and best in the run 0.635
INFO:name:Mean 0.6035 and standard deviation 0.015023869896490289 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--graphcodebert-base/snapshots/2b0488a7bb0eefc7041f1bb2cad1ab26b0da269d/pytorch_model.bin
Some weights of the model checkpoint at microsoft/graphcodebert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 23:48:41,783 >> Trainable Ratio: 3886641/127941681=3.037822%
[INFO|(OpenDelta)basemodel:702]2024-04-20 23:48:41,784 >> Delta Parameter Ratio: 3295280/127941681=2.575611%
[INFO|(OpenDelta)basemodel:704]2024-04-20 23:48:41,784 >> Static Memory 1.44 GB, Max Memory 10.04 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.687  acc 0.558: 100%|█████████▉| 408/410 [02:09<00:00,  3.17it/s]
epoch 0 loss 0.687  acc 0.558: 100%|█████████▉| 408/410 [02:10<00:00,  3.17it/s]
epoch 0 loss 0.687  acc 0.558: 100%|█████████▉| 409/410 [02:10<00:00,  3.17it/s]
epoch 0 loss 0.687  acc 0.558: 100%|█████████▉| 409/410 [02:10<00:00,  3.17it/s]
epoch 0 loss 0.687  acc 0.558: 100%|██████████| 410/410 [02:10<00:00,  3.42it/s]
epoch 0 loss 0.687  acc 0.558: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.674  acc 0.572: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.572: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.572: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.572: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 1 loss 0.673  acc 0.572: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 1 loss 0.673  acc 0.572: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.659  acc 0.598: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.659  acc 0.598: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.659  acc 0.598: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 2 loss 0.66  acc 0.597: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s] 
epoch 2 loss 0.66  acc 0.597: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 2 loss 0.66  acc 0.597: 100%|██████████| 410/410 [02:09<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.648  acc 0.611: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.648  acc 0.611: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.648  acc 0.611: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.648  acc 0.611: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 3 loss 0.648  acc 0.611: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 3 loss 0.648  acc 0.611: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.631  acc 0.636: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.631  acc 0.636: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.631  acc 0.636: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.631  acc 0.635: 100%|█████████▉| 409/410 [02:08<00:00,  3.18it/s]
epoch 4 loss 0.631  acc 0.635: 100%|██████████| 410/410 [02:08<00:00,  3.44it/s]
epoch 4 loss 0.631  acc 0.635: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]
INFO:name:Cycle 20, the best candidate in pop has score 0.635, and best in the run 0.635
INFO:name:Mean 0.605 and standard deviation 0.014848119970779707 of score in the population
Best of all candidates :([{'insert_modules': ('output', 'attention.self'), 'bottleneck_dim': (24, 128, 32), 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ('attention',), 'bottleneck_dim': (16, 128), 'non_linearity': 'gelu_new'}, {'insert_modules': ('attention', 'attention.self'), 'bottleneck_dim': (32, 64, 24), 'non_linearity': 'gelu_new'}, {'insert_modules': ('attention', 'intermediate', 'attention.self'), 'bottleneck_dim': (16, 32), 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ('intermediate', 'attention.self', 'output'), 'bottleneck_dim': (128,), 'non_linearity': 'gelu_new'}, {'insert_modules': ('output', 'attention.self'), 'bottleneck_dim': (64,), 'non_linearity': 'gelu_new'}, {'insert_modules': ('output', 'attention.self'), 'bottleneck_dim': (24, 64), 'non_linearity': 'gelu_new'}, {'insert_modules': ('attention', 'intermediate', 'attention.self'), 'bottleneck_dim': (128, 128), 'non_linearity': 'gelu_new'}, 0], 0.635)
