/opt/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
/opt/miniconda3/envs/adapter/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
INFO:name:device: cuda:0, n_gpu: 1
DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/codebert-base/resolve/main/config.json HTTP/1.1" 200 0
INFO:name:Initialization of the population, may take a while ...
DEBUG:urllib3.connectionpool:https://huggingface.co:443 "HEAD /microsoft/codebert-base/resolve/main/vocab.json HTTP/1.1" 200 0
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
[INFO|(OpenDelta)basemodel:700]2024-04-19 19:07:56,255 >> Trainable Ratio: 842881/124897921=0.674856%
[INFO|(OpenDelta)basemodel:702]2024-04-19 19:07:56,255 >> Delta Parameter Ratio: 251520/124897921=0.201380%
[INFO|(OpenDelta)basemodel:704]2024-04-19 19:07:56,256 >> Static Memory 0.00 GB, Max Memory 0.00 GB

 ==> Delta parameters :   [0, 0, 0, 0, {'insert_modules': ['attention'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['attention', 'output', 'attention.self'], 'bottleneck_dim': [64, 32, 24], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.694  acc 0.52: 100%|█████████▉| 408/410 [01:36<00:00,  4.17it/s]
epoch 0 loss 0.695  acc 0.519: 100%|█████████▉| 408/410 [01:36<00:00,  4.17it/s]
epoch 0 loss 0.695  acc 0.519: 100%|█████████▉| 409/410 [01:36<00:00,  4.17it/s]
epoch 0 loss 0.695  acc 0.519: 100%|█████████▉| 409/410 [01:37<00:00,  4.17it/s]
epoch 0 loss 0.695  acc 0.519: 100%|██████████| 410/410 [01:37<00:00,  4.51it/s]
epoch 0 loss 0.695  acc 0.519: 100%|██████████| 410/410 [01:37<00:00,  4.22it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.692  acc 0.532: 100%|█████████▉| 408/410 [01:37<00:00,  4.17it/s]
epoch 1 loss 0.692  acc 0.531: 100%|█████████▉| 408/410 [01:38<00:00,  4.17it/s]
epoch 1 loss 0.692  acc 0.531: 100%|█████████▉| 409/410 [01:38<00:00,  4.18it/s]
epoch 1 loss 0.692  acc 0.532: 100%|█████████▉| 409/410 [01:38<00:00,  4.18it/s]
epoch 1 loss 0.692  acc 0.532: 100%|██████████| 410/410 [01:38<00:00,  4.52it/s]
epoch 1 loss 0.692  acc 0.532: 100%|██████████| 410/410 [01:38<00:00,  4.17it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.685  acc 0.547: 100%|█████████▉| 408/410 [01:37<00:00,  4.19it/s]
epoch 2 loss 0.685  acc 0.547: 100%|█████████▉| 408/410 [01:37<00:00,  4.19it/s]
epoch 2 loss 0.685  acc 0.547: 100%|█████████▉| 409/410 [01:37<00:00,  4.19it/s]
epoch 2 loss 0.685  acc 0.547: 100%|█████████▉| 409/410 [01:38<00:00,  4.19it/s]
epoch 2 loss 0.685  acc 0.547: 100%|██████████| 410/410 [01:38<00:00,  4.53it/s]
epoch 2 loss 0.685  acc 0.547: 100%|██████████| 410/410 [01:38<00:00,  4.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.679  acc 0.566: 100%|█████████▉| 408/410 [01:37<00:00,  4.20it/s]
epoch 3 loss 0.679  acc 0.566: 100%|█████████▉| 408/410 [01:37<00:00,  4.20it/s]
epoch 3 loss 0.679  acc 0.566: 100%|█████████▉| 409/410 [01:37<00:00,  4.20it/s]
epoch 3 loss 0.679  acc 0.566: 100%|█████████▉| 409/410 [01:37<00:00,  4.20it/s]
epoch 3 loss 0.679  acc 0.566: 100%|██████████| 410/410 [01:37<00:00,  4.54it/s]
epoch 3 loss 0.679  acc 0.566: 100%|██████████| 410/410 [01:37<00:00,  4.19it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.676  acc 0.568: 100%|█████████▉| 408/410 [01:36<00:00,  4.22it/s]
epoch 4 loss 0.676  acc 0.567: 100%|█████████▉| 408/410 [01:37<00:00,  4.22it/s]
epoch 4 loss 0.676  acc 0.567: 100%|█████████▉| 409/410 [01:37<00:00,  4.21it/s]
epoch 4 loss 0.676  acc 0.567: 100%|█████████▉| 409/410 [01:37<00:00,  4.21it/s]
epoch 4 loss 0.676  acc 0.567: 100%|██████████| 410/410 [01:37<00:00,  4.56it/s]
epoch 4 loss 0.676  acc 0.567: 100%|██████████| 410/410 [01:37<00:00,  4.21it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 19:16:57,337 >> Trainable Ratio: 1046121/125101161=0.836220%
[INFO|(OpenDelta)basemodel:702]2024-04-19 19:16:57,338 >> Delta Parameter Ratio: 454760/125101161=0.363514%
[INFO|(OpenDelta)basemodel:704]2024-04-19 19:16:57,338 >> Static Memory 0.48 GB, Max Memory 5.49 GB

 ==> Fitness ( Validation acc ) :   0.554 


 ==> Delta parameters :   [{'insert_modules': ['output'], 'bottleneck_dim': [64, 128], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [16, 24], 'non_linearity': 'relu'}, 0, 0, 0, {'insert_modules': ['output', 'attention.self', 'attention'], 'bottleneck_dim': [16, 64, 64], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.693  acc 0.527: 100%|█████████▉| 408/410 [01:57<00:00,  3.45it/s]
epoch 0 loss 0.693  acc 0.528: 100%|█████████▉| 408/410 [01:57<00:00,  3.45it/s]
epoch 0 loss 0.693  acc 0.528: 100%|█████████▉| 409/410 [01:57<00:00,  3.45it/s]
epoch 0 loss 0.693  acc 0.529: 100%|█████████▉| 409/410 [01:57<00:00,  3.45it/s]
epoch 0 loss 0.693  acc 0.529: 100%|██████████| 410/410 [01:57<00:00,  3.72it/s]
epoch 0 loss 0.693  acc 0.529: 100%|██████████| 410/410 [01:57<00:00,  3.48it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.693  acc 0.534: 100%|█████████▉| 408/410 [01:57<00:00,  3.47it/s]
epoch 1 loss 0.693  acc 0.534: 100%|█████████▉| 408/410 [01:58<00:00,  3.47it/s]
epoch 1 loss 0.693  acc 0.534: 100%|█████████▉| 409/410 [01:58<00:00,  3.47it/s]
epoch 1 loss 0.693  acc 0.534: 100%|█████████▉| 409/410 [01:58<00:00,  3.47it/s]
epoch 1 loss 0.693  acc 0.534: 100%|██████████| 410/410 [01:58<00:00,  3.76it/s]
epoch 1 loss 0.693  acc 0.534: 100%|██████████| 410/410 [01:58<00:00,  3.47it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.689  acc 0.545: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 2 loss 0.689  acc 0.546: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 2 loss 0.689  acc 0.546: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 2 loss 0.689  acc 0.545: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 2 loss 0.689  acc 0.545: 100%|██████████| 410/410 [01:57<00:00,  3.77it/s]
epoch 2 loss 0.689  acc 0.545: 100%|██████████| 410/410 [01:57<00:00,  3.48it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.682  acc 0.552: 100%|█████████▉| 408/410 [01:56<00:00,  3.49it/s]
epoch 3 loss 0.683  acc 0.552: 100%|█████████▉| 408/410 [01:57<00:00,  3.49it/s]
epoch 3 loss 0.683  acc 0.552: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 3 loss 0.683  acc 0.551: 100%|█████████▉| 409/410 [01:57<00:00,  3.49it/s]
epoch 3 loss 0.683  acc 0.551: 100%|██████████| 410/410 [01:57<00:00,  3.77it/s]
epoch 3 loss 0.683  acc 0.551: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.682  acc 0.558: 100%|█████████▉| 408/410 [01:57<00:00,  3.48it/s]
epoch 4 loss 0.682  acc 0.558: 100%|█████████▉| 408/410 [01:57<00:00,  3.48it/s]
epoch 4 loss 0.682  acc 0.558: 100%|█████████▉| 409/410 [01:57<00:00,  3.48it/s]
epoch 4 loss 0.682  acc 0.558: 100%|█████████▉| 409/410 [01:57<00:00,  3.48it/s]
epoch 4 loss 0.682  acc 0.558: 100%|██████████| 410/410 [01:57<00:00,  3.76it/s]
epoch 4 loss 0.682  acc 0.558: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 19:27:38,002 >> Trainable Ratio: 3680921/127735961=2.881664%
[INFO|(OpenDelta)basemodel:702]2024-04-19 19:27:38,003 >> Delta Parameter Ratio: 3089560/127735961=2.418708%
[INFO|(OpenDelta)basemodel:704]2024-04-19 19:27:38,003 >> Static Memory 0.48 GB, Max Memory 8.45 GB

 ==> Fitness ( Validation acc ) :   0.552 


 ==> Delta parameters :   [{'insert_modules': ['attention', 'output', 'attention.self'], 'bottleneck_dim': [64, 16, 24], 'non_linearity': 'relu'}, 0, {'insert_modules': ['intermediate', 'output', 'attention.self'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [24, 24], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention', 'intermediate'], 'bottleneck_dim': [128, 64], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention.self', 'intermediate'], 'bottleneck_dim': [24, 24, 128], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'attention', 'output'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [16, 24, 128], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention.self', 'attention'], 'bottleneck_dim': [64, 64, 32], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [32, 32], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention.self', 'attention'], 'bottleneck_dim': [128, 128, 24], 'non_linearity': 'relu'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 408/410 [02:09<00:00,  3.14it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 408/410 [02:09<00:00,  3.14it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 409/410 [02:09<00:00,  3.13it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 0 loss 0.696  acc 0.527: 100%|██████████| 410/410 [02:10<00:00,  3.37it/s]
epoch 0 loss 0.696  acc 0.527: 100%|██████████| 410/410 [02:10<00:00,  3.15it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.691  acc 0.542: 100%|█████████▉| 408/410 [02:09<00:00,  3.15it/s]
epoch 1 loss 0.691  acc 0.542: 100%|█████████▉| 408/410 [02:10<00:00,  3.15it/s]
epoch 1 loss 0.691  acc 0.542: 100%|█████████▉| 409/410 [02:10<00:00,  3.15it/s]
epoch 1 loss 0.691  acc 0.542: 100%|█████████▉| 409/410 [02:10<00:00,  3.15it/s]
epoch 1 loss 0.691  acc 0.542: 100%|██████████| 410/410 [02:10<00:00,  3.40it/s]
epoch 1 loss 0.691  acc 0.542: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.678  acc 0.562: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 2 loss 0.678  acc 0.562: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 2 loss 0.678  acc 0.562: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 2 loss 0.678  acc 0.562: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 2 loss 0.678  acc 0.562: 100%|██████████| 410/410 [02:09<00:00,  3.41it/s]
epoch 2 loss 0.678  acc 0.562: 100%|██████████| 410/410 [02:09<00:00,  3.15it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.672  acc 0.572: 100%|█████████▉| 408/410 [02:09<00:00,  3.15it/s]
epoch 3 loss 0.672  acc 0.573: 100%|█████████▉| 408/410 [02:09<00:00,  3.15it/s]
epoch 3 loss 0.672  acc 0.573: 100%|█████████▉| 409/410 [02:09<00:00,  3.15it/s]
epoch 3 loss 0.672  acc 0.572: 100%|█████████▉| 409/410 [02:09<00:00,  3.15it/s]
epoch 3 loss 0.672  acc 0.572: 100%|██████████| 410/410 [02:09<00:00,  3.40it/s]
epoch 3 loss 0.672  acc 0.572: 100%|██████████| 410/410 [02:09<00:00,  3.16it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.666  acc 0.58: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 4 loss 0.666  acc 0.579: 100%|█████████▉| 408/410 [02:09<00:00,  3.16it/s]
epoch 4 loss 0.666  acc 0.579: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 4 loss 0.666  acc 0.579: 100%|█████████▉| 409/410 [02:09<00:00,  3.16it/s]
epoch 4 loss 0.666  acc 0.579: 100%|██████████| 410/410 [02:09<00:00,  3.41it/s]
epoch 4 loss 0.666  acc 0.579: 100%|██████████| 410/410 [02:09<00:00,  3.16it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 19:39:21,667 >> Trainable Ratio: 1792513/125847553=1.424353%
[INFO|(OpenDelta)basemodel:702]2024-04-19 19:39:21,667 >> Delta Parameter Ratio: 1201152/125847553=0.954450%
[INFO|(OpenDelta)basemodel:704]2024-04-19 19:39:21,667 >> Static Memory 0.96 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.565 


 ==> Delta parameters :   [{'insert_modules': ['attention'], 'bottleneck_dim': [128, 128], 'non_linearity': 'relu'}, 0, {'insert_modules': ['intermediate', 'attention'], 'bottleneck_dim': [128], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.525: 100%|█████████▉| 408/410 [01:59<00:00,  3.40it/s]
epoch 0 loss 0.696  acc 0.525: 100%|█████████▉| 408/410 [01:59<00:00,  3.40it/s]
epoch 0 loss 0.696  acc 0.525: 100%|█████████▉| 409/410 [01:59<00:00,  3.40it/s]
epoch 0 loss 0.696  acc 0.525: 100%|█████████▉| 409/410 [01:59<00:00,  3.40it/s]
epoch 0 loss 0.696  acc 0.525: 100%|██████████| 410/410 [01:59<00:00,  3.69it/s]
epoch 0 loss 0.696  acc 0.525: 100%|██████████| 410/410 [01:59<00:00,  3.43it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.691  acc 0.539: 100%|█████████▉| 408/410 [01:59<00:00,  3.42it/s]
epoch 1 loss 0.691  acc 0.539: 100%|█████████▉| 408/410 [01:59<00:00,  3.42it/s]
epoch 1 loss 0.691  acc 0.539: 100%|█████████▉| 409/410 [01:59<00:00,  3.42it/s]
epoch 1 loss 0.691  acc 0.54: 100%|█████████▉| 409/410 [01:59<00:00,  3.42it/s] 
epoch 1 loss 0.691  acc 0.54: 100%|██████████| 410/410 [01:59<00:00,  3.70it/s]
epoch 1 loss 0.691  acc 0.54: 100%|██████████| 410/410 [02:00<00:00,  3.42it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 408/410 [01:58<00:00,  3.44it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 408/410 [01:59<00:00,  3.44it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 409/410 [01:59<00:00,  3.44it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 409/410 [01:59<00:00,  3.44it/s]
epoch 2 loss 0.684  acc 0.553: 100%|██████████| 410/410 [01:59<00:00,  3.72it/s]
epoch 2 loss 0.684  acc 0.553: 100%|██████████| 410/410 [01:59<00:00,  3.43it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.677  acc 0.568: 100%|█████████▉| 408/410 [01:58<00:00,  3.46it/s]
epoch 3 loss 0.677  acc 0.567: 100%|█████████▉| 408/410 [01:58<00:00,  3.46it/s]
epoch 3 loss 0.677  acc 0.567: 100%|█████████▉| 409/410 [01:58<00:00,  3.46it/s]
epoch 3 loss 0.677  acc 0.568: 100%|█████████▉| 409/410 [01:58<00:00,  3.46it/s]
epoch 3 loss 0.677  acc 0.568: 100%|██████████| 410/410 [01:58<00:00,  3.74it/s]
epoch 3 loss 0.677  acc 0.568: 100%|██████████| 410/410 [01:58<00:00,  3.45it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.665  acc 0.587: 100%|█████████▉| 408/410 [01:58<00:00,  3.44it/s]
epoch 4 loss 0.665  acc 0.587: 100%|█████████▉| 408/410 [01:58<00:00,  3.44it/s]
epoch 4 loss 0.665  acc 0.587: 100%|█████████▉| 409/410 [01:58<00:00,  3.44it/s]
epoch 4 loss 0.665  acc 0.587: 100%|█████████▉| 409/410 [01:59<00:00,  3.44it/s]
epoch 4 loss 0.665  acc 0.587: 100%|██████████| 410/410 [01:59<00:00,  3.72it/s]
epoch 4 loss 0.665  acc 0.587: 100%|██████████| 410/410 [01:59<00:00,  3.44it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 19:50:10,609 >> Trainable Ratio: 4064153/128119193=3.172166%
[INFO|(OpenDelta)basemodel:702]2024-04-19 19:50:10,609 >> Delta Parameter Ratio: 3472792/128119193=2.710595%
[INFO|(OpenDelta)basemodel:704]2024-04-19 19:50:10,609 >> Static Memory 0.49 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.575 


 ==> Delta parameters :   [0, {'insert_modules': ['intermediate', 'output', 'attention.self'], 'bottleneck_dim': [128, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'output'], 'bottleneck_dim': [24, 24, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'attention.self', 'output'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate'], 'bottleneck_dim': [64, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [128, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'attention.self', 'intermediate'], 'bottleneck_dim': [24, 128, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 408/410 [02:05<00:00,  3.23it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 408/410 [02:05<00:00,  3.23it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 409/410 [02:05<00:00,  3.24it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 409/410 [02:05<00:00,  3.24it/s]
epoch 0 loss 0.696  acc 0.527: 100%|██████████| 410/410 [02:05<00:00,  3.50it/s]
epoch 0 loss 0.696  acc 0.527: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.687  acc 0.551: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.687  acc 0.551: 100%|█████████▉| 408/410 [02:06<00:00,  3.26it/s]
epoch 1 loss 0.687  acc 0.551: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 1 loss 0.687  acc 0.552: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 1 loss 0.687  acc 0.552: 100%|██████████| 410/410 [02:06<00:00,  3.52it/s]
epoch 1 loss 0.687  acc 0.552: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.674  acc 0.568: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 2 loss 0.674  acc 0.568: 100%|█████████▉| 408/410 [02:05<00:00,  3.28it/s]
epoch 2 loss 0.674  acc 0.568: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 2 loss 0.673  acc 0.569: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 2 loss 0.673  acc 0.569: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 2 loss 0.673  acc 0.569: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.66  acc 0.586: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 3 loss 0.66  acc 0.586: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 3 loss 0.66  acc 0.586: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 3 loss 0.66  acc 0.586: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 3 loss 0.66  acc 0.586: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 3 loss 0.66  acc 0.586: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.655  acc 0.595: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 4 loss 0.655  acc 0.595: 100%|█████████▉| 408/410 [02:05<00:00,  3.28it/s]
epoch 4 loss 0.655  acc 0.595: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.655  acc 0.595: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.655  acc 0.595: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 4 loss 0.655  acc 0.595: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 20:01:32,696 >> Trainable Ratio: 1562809/125617849=1.244098%
[INFO|(OpenDelta)basemodel:702]2024-04-19 20:01:32,696 >> Delta Parameter Ratio: 971448/125617849=0.773336%
[INFO|(OpenDelta)basemodel:704]2024-04-19 20:01:32,696 >> Static Memory 0.97 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.595 


 ==> Delta parameters :   [{'insert_modules': ['attention', 'attention.self'], 'bottleneck_dim': [16, 24], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, 0, 0, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [128, 128, 32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'attention', 'output'], 'bottleneck_dim': [24, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'output', 'intermediate'], 'bottleneck_dim': [16], 'non_linearity': 'gelu_new'}, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.694  acc 0.524: 100%|█████████▉| 408/410 [02:02<00:00,  3.29it/s]
epoch 0 loss 0.694  acc 0.525: 100%|█████████▉| 408/410 [02:02<00:00,  3.29it/s]
epoch 0 loss 0.694  acc 0.525: 100%|█████████▉| 409/410 [02:03<00:00,  3.27it/s]
epoch 0 loss 0.694  acc 0.525: 100%|█████████▉| 409/410 [02:03<00:00,  3.27it/s]
epoch 0 loss 0.694  acc 0.525: 100%|██████████| 410/410 [02:03<00:00,  3.53it/s]
epoch 0 loss 0.694  acc 0.525: 100%|██████████| 410/410 [02:03<00:00,  3.33it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.688  acc 0.54: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 1 loss 0.688  acc 0.54: 100%|█████████▉| 408/410 [02:03<00:00,  3.33it/s]
epoch 1 loss 0.688  acc 0.54: 100%|█████████▉| 409/410 [02:03<00:00,  3.32it/s]
epoch 1 loss 0.688  acc 0.54: 100%|█████████▉| 409/410 [02:03<00:00,  3.32it/s]
epoch 1 loss 0.688  acc 0.54: 100%|██████████| 410/410 [02:03<00:00,  3.60it/s]
epoch 1 loss 0.688  acc 0.54: 100%|██████████| 410/410 [02:03<00:00,  3.32it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.681  acc 0.554: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 2 loss 0.68  acc 0.554: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s] 
epoch 2 loss 0.68  acc 0.554: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 2 loss 0.681  acc 0.555: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 2 loss 0.681  acc 0.555: 100%|██████████| 410/410 [02:02<00:00,  3.63it/s]
epoch 2 loss 0.681  acc 0.555: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.673  acc 0.573: 100%|█████████▉| 408/410 [02:01<00:00,  3.35it/s]
epoch 3 loss 0.673  acc 0.573: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.673  acc 0.573: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.673  acc 0.573: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.673  acc 0.573: 100%|██████████| 410/410 [02:02<00:00,  3.62it/s]
epoch 3 loss 0.673  acc 0.573: 100%|██████████| 410/410 [02:02<00:00,  3.35it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.669  acc 0.57: 100%|█████████▉| 408/410 [02:01<00:00,  3.35it/s]
epoch 4 loss 0.669  acc 0.57: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.669  acc 0.57: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.669  acc 0.571: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.669  acc 0.571: 100%|██████████| 410/410 [02:02<00:00,  3.62it/s]
epoch 4 loss 0.669  acc 0.571: 100%|██████████| 410/410 [02:02<00:00,  3.35it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 20:12:38,906 >> Trainable Ratio: 1889433/125944473=1.500211%
[INFO|(OpenDelta)basemodel:702]2024-04-19 20:12:38,906 >> Delta Parameter Ratio: 1298072/125944473=1.030670%
[INFO|(OpenDelta)basemodel:704]2024-04-19 20:12:38,906 >> Static Memory 0.48 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.559 


 ==> Delta parameters :   [0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [32, 64, 32], 'non_linearity': 'relu'}, 0, {'insert_modules': ['output', 'intermediate', 'attention'], 'bottleneck_dim': [64, 24, 24], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'intermediate', 'attention.self'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, 0, 0, 0, 0, {'insert_modules': ['attention', 'attention.self', 'output'], 'bottleneck_dim': [24, 128, 128], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [16, 128], 'non_linearity': 'relu'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.525: 100%|█████████▉| 408/410 [01:51<00:00,  3.66it/s]
epoch 0 loss 0.696  acc 0.525: 100%|█████████▉| 408/410 [01:51<00:00,  3.66it/s]
epoch 0 loss 0.696  acc 0.525: 100%|█████████▉| 409/410 [01:51<00:00,  3.65it/s]
epoch 0 loss 0.696  acc 0.525: 100%|█████████▉| 409/410 [01:51<00:00,  3.65it/s]
epoch 0 loss 0.696  acc 0.525: 100%|██████████| 410/410 [01:51<00:00,  3.96it/s]
epoch 0 loss 0.696  acc 0.525: 100%|██████████| 410/410 [01:51<00:00,  3.68it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.691  acc 0.544: 100%|█████████▉| 408/410 [01:51<00:00,  3.66it/s]
epoch 1 loss 0.691  acc 0.543: 100%|█████████▉| 408/410 [01:51<00:00,  3.66it/s]
epoch 1 loss 0.691  acc 0.543: 100%|█████████▉| 409/410 [01:51<00:00,  3.66it/s]
epoch 1 loss 0.691  acc 0.544: 100%|█████████▉| 409/410 [01:52<00:00,  3.66it/s]
epoch 1 loss 0.691  acc 0.544: 100%|██████████| 410/410 [01:52<00:00,  3.96it/s]
epoch 1 loss 0.691  acc 0.544: 100%|██████████| 410/410 [01:52<00:00,  3.66it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.68  acc 0.56: 100%|█████████▉| 408/410 [01:51<00:00,  3.69it/s]
epoch 2 loss 0.68  acc 0.56: 100%|█████████▉| 408/410 [01:51<00:00,  3.69it/s]
epoch 2 loss 0.68  acc 0.56: 100%|█████████▉| 409/410 [01:51<00:00,  3.68it/s]
epoch 2 loss 0.68  acc 0.56: 100%|█████████▉| 409/410 [01:51<00:00,  3.68it/s]
epoch 2 loss 0.68  acc 0.56: 100%|██████████| 410/410 [01:51<00:00,  3.98it/s]
epoch 2 loss 0.68  acc 0.56: 100%|██████████| 410/410 [01:51<00:00,  3.67it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.675  acc 0.569: 100%|█████████▉| 408/410 [01:50<00:00,  3.69it/s]
epoch 3 loss 0.675  acc 0.569: 100%|█████████▉| 408/410 [01:51<00:00,  3.69it/s]
epoch 3 loss 0.675  acc 0.569: 100%|█████████▉| 409/410 [01:51<00:00,  3.69it/s]
epoch 3 loss 0.675  acc 0.569: 100%|█████████▉| 409/410 [01:51<00:00,  3.69it/s]
epoch 3 loss 0.675  acc 0.569: 100%|██████████| 410/410 [01:51<00:00,  3.98it/s]
epoch 3 loss 0.675  acc 0.569: 100%|██████████| 410/410 [01:51<00:00,  3.68it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.671  acc 0.57: 100%|█████████▉| 408/410 [01:50<00:00,  3.68it/s]
epoch 4 loss 0.671  acc 0.571: 100%|█████████▉| 408/410 [01:51<00:00,  3.68it/s]
epoch 4 loss 0.671  acc 0.571: 100%|█████████▉| 409/410 [01:51<00:00,  3.68it/s]
epoch 4 loss 0.671  acc 0.571: 100%|█████████▉| 409/410 [01:51<00:00,  3.68it/s]
epoch 4 loss 0.671  acc 0.571: 100%|██████████| 410/410 [01:51<00:00,  3.98it/s]
epoch 4 loss 0.671  acc 0.571: 100%|██████████| 410/410 [01:51<00:00,  3.68it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 20:22:48,777 >> Trainable Ratio: 1588513/125643553=1.264301%
[INFO|(OpenDelta)basemodel:702]2024-04-19 20:22:48,777 >> Delta Parameter Ratio: 997152/125643553=0.793636%
[INFO|(OpenDelta)basemodel:704]2024-04-19 20:22:48,777 >> Static Memory 0.96 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.548 


 ==> Delta parameters :   [{'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['output'], 'bottleneck_dim': [32, 128, 128], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['attention.self'], 'bottleneck_dim': [16, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [16], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['intermediate', 'output', 'attention'], 'bottleneck_dim': [32, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention'], 'bottleneck_dim': [24, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.695  acc 0.527: 100%|█████████▉| 408/410 [02:02<00:00,  3.30it/s]
epoch 0 loss 0.695  acc 0.527: 100%|█████████▉| 408/410 [02:03<00:00,  3.30it/s]
epoch 0 loss 0.695  acc 0.527: 100%|█████████▉| 409/410 [02:03<00:00,  3.30it/s]
epoch 0 loss 0.695  acc 0.527: 100%|█████████▉| 409/410 [02:03<00:00,  3.30it/s]
epoch 0 loss 0.695  acc 0.527: 100%|██████████| 410/410 [02:03<00:00,  3.57it/s]
epoch 0 loss 0.695  acc 0.527: 100%|██████████| 410/410 [02:03<00:00,  3.33it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.689  acc 0.547: 100%|█████████▉| 408/410 [02:03<00:00,  3.31it/s]
epoch 1 loss 0.689  acc 0.547: 100%|█████████▉| 408/410 [02:03<00:00,  3.31it/s]
epoch 1 loss 0.689  acc 0.547: 100%|█████████▉| 409/410 [02:03<00:00,  3.31it/s]
epoch 1 loss 0.689  acc 0.546: 100%|█████████▉| 409/410 [02:03<00:00,  3.31it/s]
epoch 1 loss 0.689  acc 0.546: 100%|██████████| 410/410 [02:03<00:00,  3.58it/s]
epoch 1 loss 0.689  acc 0.546: 100%|██████████| 410/410 [02:03<00:00,  3.31it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.677  acc 0.569: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 2 loss 0.677  acc 0.57: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s] 
epoch 2 loss 0.677  acc 0.57: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 2 loss 0.677  acc 0.57: 100%|█████████▉| 409/410 [02:03<00:00,  3.34it/s]
epoch 2 loss 0.677  acc 0.57: 100%|██████████| 410/410 [02:03<00:00,  3.61it/s]
epoch 2 loss 0.677  acc 0.57: 100%|██████████| 410/410 [02:03<00:00,  3.33it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.671  acc 0.569: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 3 loss 0.671  acc 0.569: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 3 loss 0.671  acc 0.569: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.671  acc 0.568: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.671  acc 0.568: 100%|██████████| 410/410 [02:02<00:00,  3.62it/s]
epoch 3 loss 0.671  acc 0.568: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.665  acc 0.585: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 4 loss 0.665  acc 0.585: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 4 loss 0.665  acc 0.585: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 4 loss 0.666  acc 0.585: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 4 loss 0.666  acc 0.585: 100%|██████████| 410/410 [02:02<00:00,  3.60it/s]
epoch 4 loss 0.666  acc 0.585: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 20:33:56,370 >> Trainable Ratio: 1284353/125339393=1.024700%
[INFO|(OpenDelta)basemodel:702]2024-04-19 20:33:56,371 >> Delta Parameter Ratio: 692992/125339393=0.552892%
[INFO|(OpenDelta)basemodel:704]2024-04-19 20:33:56,371 >> Static Memory 0.48 GB, Max Memory 9.36 GB
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 20:33:57,783 >> Trainable Ratio: 4151961/128207001=3.238482%
[INFO|(OpenDelta)basemodel:702]2024-04-19 20:33:57,783 >> Delta Parameter Ratio: 3560600/128207001=2.777227%
[INFO|(OpenDelta)basemodel:704]2024-04-19 20:33:57,783 >> Static Memory 0.95 GB, Max Memory 9.36 GB

 ==> Fitness ( Validation acc ) :   0.56 


 ==> Delta parameters :   [0, 0, 0, 0, {'insert_modules': ['intermediate', 'attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention.self'], 'bottleneck_dim': [128], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0] 


 ==> Delta parameters :   [{'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [24, 128, 32], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['attention'], 'bottleneck_dim': [16, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'attention.self'], 'bottleneck_dim': [32, 64, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'intermediate', 'attention.self'], 'bottleneck_dim': [16, 32], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['intermediate', 'attention.self', 'output'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [24, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention', 'attention.self'], 'bottleneck_dim': [128, 64, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'intermediate', 'attention'], 'bottleneck_dim': [24, 32], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.697  acc 0.525: 100%|█████████▉| 408/410 [02:10<00:00,  3.10it/s]
epoch 0 loss 0.697  acc 0.525: 100%|█████████▉| 408/410 [02:11<00:00,  3.10it/s]
epoch 0 loss 0.697  acc 0.525: 100%|█████████▉| 409/410 [02:11<00:00,  3.09it/s]
epoch 0 loss 0.697  acc 0.525: 100%|█████████▉| 409/410 [02:11<00:00,  3.09it/s]
epoch 0 loss 0.697  acc 0.525: 100%|██████████| 410/410 [02:11<00:00,  3.34it/s]
epoch 0 loss 0.697  acc 0.525: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.688  acc 0.545: 100%|█████████▉| 408/410 [02:11<00:00,  3.11it/s]
epoch 1 loss 0.688  acc 0.545: 100%|█████████▉| 408/410 [02:11<00:00,  3.11it/s]
epoch 1 loss 0.688  acc 0.545: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 1 loss 0.688  acc 0.545: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 1 loss 0.688  acc 0.545: 100%|██████████| 410/410 [02:11<00:00,  3.37it/s]
epoch 1 loss 0.688  acc 0.545: 100%|██████████| 410/410 [02:11<00:00,  3.11it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.679  acc 0.567: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 2 loss 0.678  acc 0.567: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 2 loss 0.678  acc 0.567: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 2 loss 0.679  acc 0.566: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 2 loss 0.679  acc 0.566: 100%|██████████| 410/410 [02:11<00:00,  3.39it/s]
epoch 2 loss 0.679  acc 0.566: 100%|██████████| 410/410 [02:11<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.668  acc 0.578: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.668  acc 0.578: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.668  acc 0.578: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.668  acc 0.577: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.668  acc 0.577: 100%|██████████| 410/410 [02:10<00:00,  3.38it/s]
epoch 3 loss 0.668  acc 0.577: 100%|██████████| 410/410 [02:11<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.66  acc 0.585: 100%|█████████▉| 408/410 [02:10<00:00,  3.14it/s]
epoch 4 loss 0.66  acc 0.586: 100%|█████████▉| 408/410 [02:10<00:00,  3.14it/s]
epoch 4 loss 0.66  acc 0.586: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 4 loss 0.66  acc 0.586: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 4 loss 0.66  acc 0.586: 100%|██████████| 410/410 [02:10<00:00,  3.39it/s]
epoch 4 loss 0.66  acc 0.586: 100%|██████████| 410/410 [02:11<00:00,  3.13it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 20:45:47,963 >> Trainable Ratio: 1017409/125072449=0.813456%
[INFO|(OpenDelta)basemodel:702]2024-04-19 20:45:47,963 >> Delta Parameter Ratio: 426048/125072449=0.340641%
[INFO|(OpenDelta)basemodel:704]2024-04-19 20:45:47,963 >> Static Memory 0.49 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.579 


 ==> Delta parameters :   [0, 0, 0, 0, 0, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [128, 16, 32], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.695  acc 0.534: 100%|█████████▉| 408/410 [01:32<00:00,  4.36it/s]
epoch 0 loss 0.695  acc 0.534: 100%|█████████▉| 408/410 [01:32<00:00,  4.36it/s]
epoch 0 loss 0.695  acc 0.534: 100%|█████████▉| 409/410 [01:32<00:00,  4.36it/s]
epoch 0 loss 0.695  acc 0.534: 100%|█████████▉| 409/410 [01:32<00:00,  4.36it/s]
epoch 0 loss 0.695  acc 0.534: 100%|██████████| 410/410 [01:32<00:00,  4.71it/s]
epoch 0 loss 0.695  acc 0.534: 100%|██████████| 410/410 [01:32<00:00,  4.41it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.694  acc 0.534: 100%|█████████▉| 408/410 [01:33<00:00,  4.38it/s]
epoch 1 loss 0.694  acc 0.534: 100%|█████████▉| 408/410 [01:33<00:00,  4.38it/s]
epoch 1 loss 0.694  acc 0.534: 100%|█████████▉| 409/410 [01:33<00:00,  4.38it/s]
epoch 1 loss 0.694  acc 0.534: 100%|█████████▉| 409/410 [01:33<00:00,  4.38it/s]
epoch 1 loss 0.694  acc 0.534: 100%|██████████| 410/410 [01:33<00:00,  4.74it/s]
epoch 1 loss 0.694  acc 0.534: 100%|██████████| 410/410 [01:33<00:00,  4.37it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.682  acc 0.555: 100%|█████████▉| 408/410 [01:32<00:00,  4.40it/s]
epoch 2 loss 0.682  acc 0.555: 100%|█████████▉| 408/410 [01:33<00:00,  4.40it/s]
epoch 2 loss 0.682  acc 0.555: 100%|█████████▉| 409/410 [01:33<00:00,  4.40it/s]
epoch 2 loss 0.682  acc 0.555: 100%|█████████▉| 409/410 [01:33<00:00,  4.40it/s]
epoch 2 loss 0.682  acc 0.555: 100%|██████████| 410/410 [01:33<00:00,  4.76it/s]
epoch 2 loss 0.682  acc 0.555: 100%|██████████| 410/410 [01:33<00:00,  4.39it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.675  acc 0.568: 100%|█████████▉| 408/410 [01:32<00:00,  4.40it/s]
epoch 3 loss 0.675  acc 0.568: 100%|█████████▉| 408/410 [01:33<00:00,  4.40it/s]
epoch 3 loss 0.675  acc 0.568: 100%|█████████▉| 409/410 [01:33<00:00,  4.41it/s]
epoch 3 loss 0.675  acc 0.567: 100%|█████████▉| 409/410 [01:33<00:00,  4.41it/s]
epoch 3 loss 0.675  acc 0.567: 100%|██████████| 410/410 [01:33<00:00,  4.76it/s]
epoch 3 loss 0.675  acc 0.567: 100%|██████████| 410/410 [01:33<00:00,  4.40it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.67  acc 0.576: 100%|█████████▉| 408/410 [01:32<00:00,  4.42it/s]
epoch 4 loss 0.67  acc 0.576: 100%|█████████▉| 408/410 [01:32<00:00,  4.42it/s]
epoch 4 loss 0.67  acc 0.576: 100%|█████████▉| 409/410 [01:32<00:00,  4.41it/s]
epoch 4 loss 0.669  acc 0.577: 100%|█████████▉| 409/410 [01:32<00:00,  4.41it/s]
epoch 4 loss 0.669  acc 0.577: 100%|██████████| 410/410 [01:32<00:00,  4.77it/s]
epoch 4 loss 0.669  acc 0.577: 100%|██████████| 410/410 [01:33<00:00,  4.41it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 20:54:25,412 >> Trainable Ratio: 1400993/125456033=1.116720%
[INFO|(OpenDelta)basemodel:702]2024-04-19 20:54:25,412 >> Delta Parameter Ratio: 809632/125456033=0.645351%
[INFO|(OpenDelta)basemodel:704]2024-04-19 20:54:25,412 >> Static Memory 0.96 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.563 


 ==> Delta parameters :   [0, {'insert_modules': ['attention.self', 'attention', 'output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, 0, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [128, 128, 32], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.695  acc 0.522: 100%|█████████▉| 408/410 [01:54<00:00,  3.53it/s]
epoch 0 loss 0.695  acc 0.522: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 0 loss 0.695  acc 0.522: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 0 loss 0.695  acc 0.522: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 0 loss 0.695  acc 0.522: 100%|██████████| 410/410 [01:55<00:00,  3.83it/s]
epoch 0 loss 0.695  acc 0.522: 100%|██████████| 410/410 [01:55<00:00,  3.55it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.691  acc 0.539: 100%|█████████▉| 408/410 [01:55<00:00,  3.55it/s]
epoch 1 loss 0.691  acc 0.539: 100%|█████████▉| 408/410 [01:55<00:00,  3.55it/s]
epoch 1 loss 0.691  acc 0.539: 100%|█████████▉| 409/410 [01:55<00:00,  3.55it/s]
epoch 1 loss 0.691  acc 0.538: 100%|█████████▉| 409/410 [01:55<00:00,  3.55it/s]
epoch 1 loss 0.691  acc 0.538: 100%|██████████| 410/410 [01:55<00:00,  3.84it/s]
epoch 1 loss 0.691  acc 0.538: 100%|██████████| 410/410 [01:55<00:00,  3.54it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.682  acc 0.557: 100%|█████████▉| 408/410 [01:54<00:00,  3.56it/s]
epoch 2 loss 0.682  acc 0.557: 100%|█████████▉| 408/410 [01:55<00:00,  3.56it/s]
epoch 2 loss 0.682  acc 0.557: 100%|█████████▉| 409/410 [01:55<00:00,  3.56it/s]
epoch 2 loss 0.682  acc 0.558: 100%|█████████▉| 409/410 [01:55<00:00,  3.56it/s]
epoch 2 loss 0.682  acc 0.558: 100%|██████████| 410/410 [01:55<00:00,  3.84it/s]
epoch 2 loss 0.682  acc 0.558: 100%|██████████| 410/410 [01:55<00:00,  3.55it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.677  acc 0.568: 100%|█████████▉| 408/410 [01:54<00:00,  3.57it/s]
epoch 3 loss 0.677  acc 0.568: 100%|█████████▉| 408/410 [01:54<00:00,  3.57it/s]
epoch 3 loss 0.677  acc 0.568: 100%|█████████▉| 409/410 [01:54<00:00,  3.57it/s]
epoch 3 loss 0.677  acc 0.568: 100%|█████████▉| 409/410 [01:54<00:00,  3.57it/s]
epoch 3 loss 0.677  acc 0.568: 100%|██████████| 410/410 [01:54<00:00,  3.85it/s]
epoch 3 loss 0.677  acc 0.568: 100%|██████████| 410/410 [01:54<00:00,  3.57it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.671  acc 0.578: 100%|█████████▉| 408/410 [01:54<00:00,  3.55it/s]
epoch 4 loss 0.671  acc 0.578: 100%|█████████▉| 408/410 [01:54<00:00,  3.55it/s]
epoch 4 loss 0.671  acc 0.578: 100%|█████████▉| 409/410 [01:54<00:00,  3.56it/s]
epoch 4 loss 0.671  acc 0.578: 100%|█████████▉| 409/410 [01:55<00:00,  3.56it/s]
epoch 4 loss 0.671  acc 0.578: 100%|██████████| 410/410 [01:55<00:00,  3.85it/s]
epoch 4 loss 0.671  acc 0.578: 100%|██████████| 410/410 [01:55<00:00,  3.56it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 21:04:53,484 >> Trainable Ratio: 1168761/125223801=0.933338%
[INFO|(OpenDelta)basemodel:702]2024-04-19 21:04:53,484 >> Delta Parameter Ratio: 577400/125223801=0.461094%
[INFO|(OpenDelta)basemodel:704]2024-04-19 21:04:53,484 >> Static Memory 0.48 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.547 


 ==> Delta parameters :   [0, 0, {'insert_modules': ['attention', 'output'], 'bottleneck_dim': [24, 24], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention', 'attention.self', 'output'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, 0, 0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [32], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.517: 100%|█████████▉| 408/410 [01:49<00:00,  3.69it/s]
epoch 0 loss 0.696  acc 0.518: 100%|█████████▉| 408/410 [01:49<00:00,  3.69it/s]
epoch 0 loss 0.696  acc 0.518: 100%|█████████▉| 409/410 [01:49<00:00,  3.69it/s]
epoch 0 loss 0.696  acc 0.518: 100%|█████████▉| 409/410 [01:50<00:00,  3.69it/s]
epoch 0 loss 0.696  acc 0.518: 100%|██████████| 410/410 [01:50<00:00,  3.99it/s]
epoch 0 loss 0.696  acc 0.518: 100%|██████████| 410/410 [01:50<00:00,  3.72it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.688  acc 0.547: 100%|█████████▉| 408/410 [01:50<00:00,  3.71it/s]
epoch 1 loss 0.689  acc 0.547: 100%|█████████▉| 408/410 [01:50<00:00,  3.71it/s]
epoch 1 loss 0.689  acc 0.547: 100%|█████████▉| 409/410 [01:50<00:00,  3.71it/s]
epoch 1 loss 0.689  acc 0.547: 100%|█████████▉| 409/410 [01:50<00:00,  3.71it/s]
epoch 1 loss 0.689  acc 0.547: 100%|██████████| 410/410 [01:50<00:00,  4.01it/s]
epoch 1 loss 0.689  acc 0.547: 100%|██████████| 410/410 [01:50<00:00,  3.70it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.677  acc 0.568: 100%|█████████▉| 408/410 [01:49<00:00,  3.73it/s]
epoch 2 loss 0.677  acc 0.568: 100%|█████████▉| 408/410 [01:49<00:00,  3.73it/s]
epoch 2 loss 0.677  acc 0.568: 100%|█████████▉| 409/410 [01:49<00:00,  3.73it/s]
epoch 2 loss 0.677  acc 0.568: 100%|█████████▉| 409/410 [01:50<00:00,  3.73it/s]
epoch 2 loss 0.677  acc 0.568: 100%|██████████| 410/410 [01:50<00:00,  4.04it/s]
epoch 2 loss 0.677  acc 0.568: 100%|██████████| 410/410 [01:50<00:00,  3.72it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 408/410 [01:49<00:00,  3.74it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 408/410 [01:49<00:00,  3.74it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 409/410 [01:49<00:00,  3.74it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 409/410 [01:49<00:00,  3.74it/s]
epoch 3 loss 0.672  acc 0.57: 100%|██████████| 410/410 [01:49<00:00,  4.04it/s]
epoch 3 loss 0.672  acc 0.57: 100%|██████████| 410/410 [01:49<00:00,  3.73it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.666  acc 0.576: 100%|█████████▉| 408/410 [01:49<00:00,  3.73it/s]
epoch 4 loss 0.666  acc 0.576: 100%|█████████▉| 408/410 [01:49<00:00,  3.73it/s]
epoch 4 loss 0.666  acc 0.576: 100%|█████████▉| 409/410 [01:49<00:00,  3.73it/s]
epoch 4 loss 0.666  acc 0.576: 100%|█████████▉| 409/410 [01:49<00:00,  3.73it/s]
epoch 4 loss 0.666  acc 0.576: 100%|██████████| 410/410 [01:49<00:00,  4.03it/s]
epoch 4 loss 0.666  acc 0.576: 100%|██████████| 410/410 [01:49<00:00,  3.73it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 21:14:55,704 >> Trainable Ratio: 2664697/126719737=2.102827%
[INFO|(OpenDelta)basemodel:702]2024-04-19 21:14:55,705 >> Delta Parameter Ratio: 2073336/126719737=1.636159%
[INFO|(OpenDelta)basemodel:704]2024-04-19 21:14:55,705 >> Static Memory 0.95 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.551 


 ==> Delta parameters :   [0, {'insert_modules': ['output'], 'bottleneck_dim': [32, 24], 'non_linearity': 'relu'}, 0, 0, 0, 0, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [128, 128], 'non_linearity': 'relu'}, {'insert_modules': ['attention', 'intermediate', 'attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, 0, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [24, 64], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [128, 32], 'non_linearity': 'relu'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.697  acc 0.527: 100%|█████████▉| 408/410 [01:55<00:00,  3.49it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 408/410 [01:56<00:00,  3.49it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 409/410 [01:56<00:00,  3.49it/s]
epoch 0 loss 0.697  acc 0.527: 100%|█████████▉| 409/410 [01:56<00:00,  3.49it/s]
epoch 0 loss 0.697  acc 0.527: 100%|██████████| 410/410 [01:56<00:00,  3.77it/s]
epoch 0 loss 0.697  acc 0.527: 100%|██████████| 410/410 [01:56<00:00,  3.52it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.687  acc 0.549: 100%|█████████▉| 408/410 [01:56<00:00,  3.51it/s]
epoch 1 loss 0.687  acc 0.549: 100%|█████████▉| 408/410 [01:56<00:00,  3.51it/s]
epoch 1 loss 0.687  acc 0.549: 100%|█████████▉| 409/410 [01:56<00:00,  3.52it/s]
epoch 1 loss 0.687  acc 0.549: 100%|█████████▉| 409/410 [01:56<00:00,  3.52it/s]
epoch 1 loss 0.687  acc 0.549: 100%|██████████| 410/410 [01:56<00:00,  3.80it/s]
epoch 1 loss 0.687  acc 0.549: 100%|██████████| 410/410 [01:56<00:00,  3.51it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.674  acc 0.575: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 2 loss 0.674  acc 0.575: 100%|█████████▉| 408/410 [01:56<00:00,  3.53it/s]
epoch 2 loss 0.674  acc 0.575: 100%|█████████▉| 409/410 [01:56<00:00,  3.53it/s]
epoch 2 loss 0.674  acc 0.574: 100%|█████████▉| 409/410 [01:56<00:00,  3.53it/s]
epoch 2 loss 0.674  acc 0.574: 100%|██████████| 410/410 [01:56<00:00,  3.82it/s]
epoch 2 loss 0.674  acc 0.574: 100%|██████████| 410/410 [01:56<00:00,  3.52it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.669  acc 0.576: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 3 loss 0.669  acc 0.576: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 3 loss 0.669  acc 0.576: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 3 loss 0.669  acc 0.576: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 3 loss 0.669  acc 0.576: 100%|██████████| 410/410 [01:55<00:00,  3.82it/s]
epoch 3 loss 0.669  acc 0.576: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.661  acc 0.592: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 4 loss 0.661  acc 0.592: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 4 loss 0.661  acc 0.592: 100%|█████████▉| 409/410 [01:55<00:00,  3.53it/s]
epoch 4 loss 0.661  acc 0.592: 100%|█████████▉| 409/410 [01:56<00:00,  3.53it/s]
epoch 4 loss 0.661  acc 0.592: 100%|██████████| 410/410 [01:56<00:00,  3.81it/s]
epoch 4 loss 0.661  acc 0.592: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 21:25:29,924 >> Trainable Ratio: 1386577/125441617=1.105356%
[INFO|(OpenDelta)basemodel:702]2024-04-19 21:25:29,924 >> Delta Parameter Ratio: 795216/125441617=0.633933%
[INFO|(OpenDelta)basemodel:704]2024-04-19 21:25:29,924 >> Static Memory 0.49 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.569 


 ==> Delta parameters :   [{'insert_modules': ['attention.self'], 'bottleneck_dim': [16, 128, 32], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [64, 64, 16], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [16, 32, 128], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.697  acc 0.521: 100%|█████████▉| 408/410 [02:00<00:00,  3.36it/s]
epoch 0 loss 0.696  acc 0.522: 100%|█████████▉| 408/410 [02:00<00:00,  3.36it/s]
epoch 0 loss 0.696  acc 0.522: 100%|█████████▉| 409/410 [02:00<00:00,  3.36it/s]
epoch 0 loss 0.696  acc 0.521: 100%|█████████▉| 409/410 [02:01<00:00,  3.36it/s]
epoch 0 loss 0.696  acc 0.521: 100%|██████████| 410/410 [02:01<00:00,  3.63it/s]
epoch 0 loss 0.696  acc 0.521: 100%|██████████| 410/410 [02:01<00:00,  3.38it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.691  acc 0.54: 100%|█████████▉| 408/410 [02:01<00:00,  3.37it/s]
epoch 1 loss 0.691  acc 0.54: 100%|█████████▉| 408/410 [02:01<00:00,  3.37it/s]
epoch 1 loss 0.691  acc 0.54: 100%|█████████▉| 409/410 [02:01<00:00,  3.37it/s]
epoch 1 loss 0.691  acc 0.541: 100%|█████████▉| 409/410 [02:01<00:00,  3.37it/s]
epoch 1 loss 0.691  acc 0.541: 100%|██████████| 410/410 [02:01<00:00,  3.65it/s]
epoch 1 loss 0.691  acc 0.541: 100%|██████████| 410/410 [02:01<00:00,  3.37it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.684  acc 0.55: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 2 loss 0.685  acc 0.55: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 2 loss 0.685  acc 0.55: 100%|█████████▉| 409/410 [02:00<00:00,  3.40it/s]
epoch 2 loss 0.685  acc 0.55: 100%|█████████▉| 409/410 [02:01<00:00,  3.40it/s]
epoch 2 loss 0.685  acc 0.55: 100%|██████████| 410/410 [02:01<00:00,  3.68it/s]
epoch 2 loss 0.685  acc 0.55: 100%|██████████| 410/410 [02:01<00:00,  3.39it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.678  acc 0.57: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 3 loss 0.678  acc 0.57: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 3 loss 0.678  acc 0.57: 100%|█████████▉| 409/410 [02:00<00:00,  3.40it/s]
epoch 3 loss 0.678  acc 0.57: 100%|█████████▉| 409/410 [02:00<00:00,  3.40it/s]
epoch 3 loss 0.678  acc 0.57: 100%|██████████| 410/410 [02:00<00:00,  3.68it/s]
epoch 3 loss 0.678  acc 0.57: 100%|██████████| 410/410 [02:00<00:00,  3.39it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.677  acc 0.564: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 4 loss 0.677  acc 0.564: 100%|█████████▉| 408/410 [02:00<00:00,  3.40it/s]
epoch 4 loss 0.677  acc 0.564: 100%|█████████▉| 409/410 [02:00<00:00,  3.39it/s]
epoch 4 loss 0.677  acc 0.564: 100%|█████████▉| 409/410 [02:00<00:00,  3.39it/s]
epoch 4 loss 0.677  acc 0.564: 100%|██████████| 410/410 [02:00<00:00,  3.66it/s]
epoch 4 loss 0.677  acc 0.564: 100%|██████████| 410/410 [02:00<00:00,  3.39it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 21:36:27,128 >> Trainable Ratio: 3223073/127278113=2.532307%
[INFO|(OpenDelta)basemodel:702]2024-04-19 21:36:27,128 >> Delta Parameter Ratio: 2631712/127278113=2.067686%
[INFO|(OpenDelta)basemodel:704]2024-04-19 21:36:27,128 >> Static Memory 0.96 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.551 


 ==> Delta parameters :   [{'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [128, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self', 'intermediate'], 'bottleneck_dim': [32, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention.self'], 'bottleneck_dim': [64, 24, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention.self'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [64, 24, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [64, 32], 'non_linearity': 'gelu_new'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.697  acc 0.525: 100%|█████████▉| 408/410 [02:08<00:00,  3.15it/s]
epoch 0 loss 0.697  acc 0.525: 100%|█████████▉| 408/410 [02:08<00:00,  3.15it/s]
epoch 0 loss 0.697  acc 0.525: 100%|█████████▉| 409/410 [02:08<00:00,  3.16it/s]
epoch 0 loss 0.697  acc 0.525: 100%|█████████▉| 409/410 [02:08<00:00,  3.16it/s]
epoch 0 loss 0.697  acc 0.525: 100%|██████████| 410/410 [02:08<00:00,  3.42it/s]
epoch 0 loss 0.697  acc 0.525: 100%|██████████| 410/410 [02:08<00:00,  3.18it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.688  acc 0.547: 100%|█████████▉| 408/410 [02:08<00:00,  3.18it/s]
epoch 1 loss 0.688  acc 0.548: 100%|█████████▉| 408/410 [02:09<00:00,  3.18it/s]
epoch 1 loss 0.688  acc 0.548: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 1 loss 0.688  acc 0.547: 100%|█████████▉| 409/410 [02:09<00:00,  3.18it/s]
epoch 1 loss 0.688  acc 0.547: 100%|██████████| 410/410 [02:09<00:00,  3.43it/s]
epoch 1 loss 0.688  acc 0.547: 100%|██████████| 410/410 [02:09<00:00,  3.17it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.679  acc 0.558: 100%|█████████▉| 408/410 [02:07<00:00,  3.20it/s]
epoch 2 loss 0.679  acc 0.558: 100%|█████████▉| 408/410 [02:08<00:00,  3.20it/s]
epoch 2 loss 0.679  acc 0.558: 100%|█████████▉| 409/410 [02:08<00:00,  3.20it/s]
epoch 2 loss 0.678  acc 0.558: 100%|█████████▉| 409/410 [02:08<00:00,  3.20it/s]
epoch 2 loss 0.678  acc 0.558: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 2 loss 0.678  acc 0.558: 100%|██████████| 410/410 [02:08<00:00,  3.19it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.665  acc 0.582: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.665  acc 0.582: 100%|█████████▉| 408/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.665  acc 0.582: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.665  acc 0.582: 100%|█████████▉| 409/410 [02:08<00:00,  3.19it/s]
epoch 3 loss 0.665  acc 0.582: 100%|██████████| 410/410 [02:08<00:00,  3.45it/s]
epoch 3 loss 0.665  acc 0.582: 100%|██████████| 410/410 [02:08<00:00,  3.19it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.658  acc 0.593: 100%|█████████▉| 408/410 [02:07<00:00,  3.20it/s]
epoch 4 loss 0.658  acc 0.593: 100%|█████████▉| 408/410 [02:08<00:00,  3.20it/s]
epoch 4 loss 0.658  acc 0.593: 100%|█████████▉| 409/410 [02:08<00:00,  3.20it/s]
epoch 4 loss 0.658  acc 0.594: 100%|█████████▉| 409/410 [02:08<00:00,  3.20it/s]
epoch 4 loss 0.658  acc 0.594: 100%|██████████| 410/410 [02:08<00:00,  3.46it/s]
epoch 4 loss 0.658  acc 0.594: 100%|██████████| 410/410 [02:08<00:00,  3.19it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 21:48:04,803 >> Trainable Ratio: 3555721/127610761=2.786380%
[INFO|(OpenDelta)basemodel:702]2024-04-19 21:48:04,803 >> Delta Parameter Ratio: 2964360/127610761=2.322970%
[INFO|(OpenDelta)basemodel:704]2024-04-19 21:48:04,803 >> Static Memory 0.49 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.579 


 ==> Delta parameters :   [{'insert_modules': ['output', 'intermediate', 'attention.self'], 'bottleneck_dim': [24, 24], 'non_linearity': 'relu'}, {'insert_modules': ['intermediate', 'attention.self', 'attention'], 'bottleneck_dim': [32, 64, 16], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [64, 128], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention', 'attention.self'], 'bottleneck_dim': [24, 16], 'non_linearity': 'relu'}, {'insert_modules': ['intermediate'], 'bottleneck_dim': [16, 16], 'non_linearity': 'relu'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [16, 16], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [24, 128], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [32], 'non_linearity': 'relu'}, {'insert_modules': ['attention'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention', 'attention.self'], 'bottleneck_dim': [32], 'non_linearity': 'relu'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [32, 32], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'intermediate', 'attention'], 'bottleneck_dim': [128, 16, 64], 'non_linearity': 'relu'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.697  acc 0.514: 100%|█████████▉| 408/410 [02:10<00:00,  3.11it/s]
epoch 0 loss 0.697  acc 0.515: 100%|█████████▉| 408/410 [02:11<00:00,  3.11it/s]
epoch 0 loss 0.697  acc 0.515: 100%|█████████▉| 409/410 [02:11<00:00,  3.11it/s]
epoch 0 loss 0.697  acc 0.515: 100%|█████████▉| 409/410 [02:11<00:00,  3.11it/s]
epoch 0 loss 0.697  acc 0.515: 100%|██████████| 410/410 [02:11<00:00,  3.35it/s]
epoch 0 loss 0.697  acc 0.515: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.688  acc 0.548: 100%|█████████▉| 408/410 [02:11<00:00,  3.12it/s]
epoch 1 loss 0.688  acc 0.548: 100%|█████████▉| 408/410 [02:11<00:00,  3.12it/s]
epoch 1 loss 0.688  acc 0.548: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 1 loss 0.688  acc 0.548: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 1 loss 0.688  acc 0.548: 100%|██████████| 410/410 [02:11<00:00,  3.37it/s]
epoch 1 loss 0.688  acc 0.548: 100%|██████████| 410/410 [02:11<00:00,  3.11it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.675  acc 0.565: 100%|█████████▉| 408/410 [02:10<00:00,  3.14it/s]
epoch 2 loss 0.675  acc 0.566: 100%|█████████▉| 408/410 [02:10<00:00,  3.14it/s]
epoch 2 loss 0.675  acc 0.566: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 2 loss 0.676  acc 0.565: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 2 loss 0.676  acc 0.565: 100%|██████████| 410/410 [02:10<00:00,  3.39it/s]
epoch 2 loss 0.676  acc 0.565: 100%|██████████| 410/410 [02:10<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.671  acc 0.57: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.672  acc 0.569: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.672  acc 0.569: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.672  acc 0.569: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 3 loss 0.672  acc 0.569: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 3 loss 0.672  acc 0.569: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.661  acc 0.589: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.661  acc 0.589: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.661  acc 0.589: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 4 loss 0.661  acc 0.589: 100%|█████████▉| 409/410 [02:10<00:00,  3.14it/s]
epoch 4 loss 0.661  acc 0.589: 100%|██████████| 410/410 [02:10<00:00,  3.39it/s]
epoch 4 loss 0.661  acc 0.589: 100%|██████████| 410/410 [02:10<00:00,  3.14it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 21:59:54,538 >> Trainable Ratio: 958377/125013417=0.766619%
[INFO|(OpenDelta)basemodel:702]2024-04-19 21:59:54,538 >> Delta Parameter Ratio: 367016/125013417=0.293581%
[INFO|(OpenDelta)basemodel:704]2024-04-19 21:59:54,538 >> Static Memory 0.49 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.559 


 ==> Delta parameters :   [0, 0, 0, 0, 0, {'insert_modules': ['attention.self', 'intermediate', 'attention'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [24, 24, 64], 'non_linearity': 'relu'}, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.697  acc 0.522: 100%|█████████▉| 408/410 [01:32<00:00,  4.35it/s]
epoch 0 loss 0.698  acc 0.521: 100%|█████████▉| 408/410 [01:33<00:00,  4.35it/s]
epoch 0 loss 0.698  acc 0.521: 100%|█████████▉| 409/410 [01:33<00:00,  4.36it/s]
epoch 0 loss 0.697  acc 0.522: 100%|█████████▉| 409/410 [01:33<00:00,  4.36it/s]
epoch 0 loss 0.697  acc 0.522: 100%|██████████| 410/410 [01:33<00:00,  4.72it/s]
epoch 0 loss 0.697  acc 0.522: 100%|██████████| 410/410 [01:33<00:00,  4.39it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.691  acc 0.541: 100%|█████████▉| 408/410 [01:33<00:00,  4.38it/s]
epoch 1 loss 0.692  acc 0.541: 100%|█████████▉| 408/410 [01:33<00:00,  4.38it/s]
epoch 1 loss 0.692  acc 0.541: 100%|█████████▉| 409/410 [01:33<00:00,  4.38it/s]
epoch 1 loss 0.692  acc 0.54: 100%|█████████▉| 409/410 [01:33<00:00,  4.38it/s] 
epoch 1 loss 0.692  acc 0.54: 100%|██████████| 410/410 [01:33<00:00,  4.73it/s]
epoch 1 loss 0.692  acc 0.54: 100%|██████████| 410/410 [01:34<00:00,  4.36it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.683  acc 0.555: 100%|█████████▉| 408/410 [01:33<00:00,  4.40it/s]
epoch 2 loss 0.683  acc 0.554: 100%|█████████▉| 408/410 [01:33<00:00,  4.40it/s]
epoch 2 loss 0.683  acc 0.554: 100%|█████████▉| 409/410 [01:33<00:00,  4.40it/s]
epoch 2 loss 0.683  acc 0.554: 100%|█████████▉| 409/410 [01:33<00:00,  4.40it/s]
epoch 2 loss 0.683  acc 0.554: 100%|██████████| 410/410 [01:33<00:00,  4.75it/s]
epoch 2 loss 0.683  acc 0.554: 100%|██████████| 410/410 [01:33<00:00,  4.38it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.675  acc 0.58: 100%|█████████▉| 408/410 [01:33<00:00,  4.38it/s]
epoch 3 loss 0.675  acc 0.58: 100%|█████████▉| 408/410 [01:33<00:00,  4.38it/s]
epoch 3 loss 0.675  acc 0.58: 100%|█████████▉| 409/410 [01:33<00:00,  4.39it/s]
epoch 3 loss 0.675  acc 0.58: 100%|█████████▉| 409/410 [01:33<00:00,  4.39it/s]
epoch 3 loss 0.675  acc 0.58: 100%|██████████| 410/410 [01:33<00:00,  4.74it/s]
epoch 3 loss 0.675  acc 0.58: 100%|██████████| 410/410 [01:33<00:00,  4.38it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.671  acc 0.58: 100%|█████████▉| 408/410 [01:32<00:00,  4.40it/s]
epoch 4 loss 0.671  acc 0.58: 100%|█████████▉| 408/410 [01:33<00:00,  4.40it/s]
epoch 4 loss 0.671  acc 0.58: 100%|█████████▉| 409/410 [01:33<00:00,  4.40it/s]
epoch 4 loss 0.671  acc 0.58: 100%|█████████▉| 409/410 [01:33<00:00,  4.40it/s]
epoch 4 loss 0.671  acc 0.58: 100%|██████████| 410/410 [01:33<00:00,  4.75it/s]
epoch 4 loss 0.671  acc 0.58: 100%|██████████| 410/410 [01:33<00:00,  4.39it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 22:08:33,535 >> Trainable Ratio: 1158737/125213777=0.925407%
[INFO|(OpenDelta)basemodel:702]2024-04-19 22:08:33,535 >> Delta Parameter Ratio: 567376/125213777=0.453126%
[INFO|(OpenDelta)basemodel:704]2024-04-19 22:08:33,535 >> Static Memory 0.96 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.551 


 ==> Delta parameters :   [0, 0, 0, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, 0, 0, 0, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [32, 32, 32], 'non_linearity': 'gelu_new'}, 0, 0, 0, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [16, 24], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.522: 100%|█████████▉| 408/410 [01:43<00:00,  3.92it/s]
epoch 0 loss 0.696  acc 0.522: 100%|█████████▉| 408/410 [01:43<00:00,  3.92it/s]
epoch 0 loss 0.696  acc 0.522: 100%|█████████▉| 409/410 [01:43<00:00,  3.91it/s]
epoch 0 loss 0.696  acc 0.522: 100%|█████████▉| 409/410 [01:43<00:00,  3.91it/s]
epoch 0 loss 0.696  acc 0.522: 100%|██████████| 410/410 [01:43<00:00,  4.22it/s]
epoch 0 loss 0.696  acc 0.522: 100%|██████████| 410/410 [01:43<00:00,  3.94it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.693  acc 0.528: 100%|█████████▉| 408/410 [01:44<00:00,  3.92it/s]
epoch 1 loss 0.693  acc 0.528: 100%|█████████▉| 408/410 [01:44<00:00,  3.92it/s]
epoch 1 loss 0.693  acc 0.528: 100%|█████████▉| 409/410 [01:44<00:00,  3.93it/s]
epoch 1 loss 0.693  acc 0.528: 100%|█████████▉| 409/410 [01:44<00:00,  3.93it/s]
epoch 1 loss 0.693  acc 0.528: 100%|██████████| 410/410 [01:44<00:00,  4.25it/s]
epoch 1 loss 0.693  acc 0.528: 100%|██████████| 410/410 [01:44<00:00,  3.92it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.686  acc 0.541: 100%|█████████▉| 408/410 [01:43<00:00,  3.95it/s]
epoch 2 loss 0.686  acc 0.54: 100%|█████████▉| 408/410 [01:43<00:00,  3.95it/s] 
epoch 2 loss 0.686  acc 0.54: 100%|█████████▉| 409/410 [01:43<00:00,  3.95it/s]
epoch 2 loss 0.686  acc 0.54: 100%|█████████▉| 409/410 [01:44<00:00,  3.95it/s]
epoch 2 loss 0.686  acc 0.54: 100%|██████████| 410/410 [01:44<00:00,  4.26it/s]
epoch 2 loss 0.686  acc 0.54: 100%|██████████| 410/410 [01:44<00:00,  3.94it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.682  acc 0.563: 100%|█████████▉| 408/410 [01:43<00:00,  3.96it/s]
epoch 3 loss 0.682  acc 0.563: 100%|█████████▉| 408/410 [01:43<00:00,  3.96it/s]
epoch 3 loss 0.682  acc 0.563: 100%|█████████▉| 409/410 [01:43<00:00,  3.96it/s]
epoch 3 loss 0.682  acc 0.563: 100%|█████████▉| 409/410 [01:43<00:00,  3.96it/s]
epoch 3 loss 0.682  acc 0.563: 100%|██████████| 410/410 [01:43<00:00,  4.28it/s]
epoch 3 loss 0.682  acc 0.563: 100%|██████████| 410/410 [01:43<00:00,  3.95it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.677  acc 0.571: 100%|█████████▉| 408/410 [01:43<00:00,  3.95it/s]
epoch 4 loss 0.677  acc 0.571: 100%|█████████▉| 408/410 [01:43<00:00,  3.95it/s]
epoch 4 loss 0.677  acc 0.571: 100%|█████████▉| 409/410 [01:43<00:00,  3.95it/s]
epoch 4 loss 0.677  acc 0.571: 100%|█████████▉| 409/410 [01:43<00:00,  3.95it/s]
epoch 4 loss 0.677  acc 0.571: 100%|██████████| 410/410 [01:43<00:00,  4.26it/s]
epoch 4 loss 0.677  acc 0.571: 100%|██████████| 410/410 [01:43<00:00,  3.95it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 22:18:05,401 >> Trainable Ratio: 1057257/125112297=0.845046%
[INFO|(OpenDelta)basemodel:702]2024-04-19 22:18:05,401 >> Delta Parameter Ratio: 465896/125112297=0.372382%
[INFO|(OpenDelta)basemodel:704]2024-04-19 22:18:05,402 >> Static Memory 0.48 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.564 


 ==> Delta parameters :   [0, 0, 0, 0, 0, 0, 0, {'insert_modules': ['attention.self', 'attention', 'intermediate'], 'bottleneck_dim': [32, 24], 'non_linearity': 'relu'}, 0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [32, 32], 'non_linearity': 'relu'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.698  acc 0.518: 100%|█████████▉| 408/410 [01:22<00:00,  4.91it/s]
epoch 0 loss 0.698  acc 0.518: 100%|█████████▉| 408/410 [01:22<00:00,  4.91it/s]
epoch 0 loss 0.698  acc 0.518: 100%|█████████▉| 409/410 [01:22<00:00,  4.91it/s]
epoch 0 loss 0.698  acc 0.518: 100%|█████████▉| 409/410 [01:22<00:00,  4.91it/s]
epoch 0 loss 0.698  acc 0.518: 100%|██████████| 410/410 [01:22<00:00,  5.30it/s]
epoch 0 loss 0.698  acc 0.518: 100%|██████████| 410/410 [01:22<00:00,  4.94it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.691  acc 0.536: 100%|█████████▉| 408/410 [01:23<00:00,  4.91it/s]
epoch 1 loss 0.691  acc 0.536: 100%|█████████▉| 408/410 [01:23<00:00,  4.91it/s]
epoch 1 loss 0.691  acc 0.536: 100%|█████████▉| 409/410 [01:23<00:00,  4.92it/s]
epoch 1 loss 0.691  acc 0.535: 100%|█████████▉| 409/410 [01:23<00:00,  4.92it/s]
epoch 1 loss 0.691  acc 0.535: 100%|██████████| 410/410 [01:23<00:00,  5.31it/s]
epoch 1 loss 0.691  acc 0.535: 100%|██████████| 410/410 [01:23<00:00,  4.91it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 408/410 [01:22<00:00,  4.93it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 408/410 [01:23<00:00,  4.93it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 409/410 [01:23<00:00,  4.93it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 409/410 [01:23<00:00,  4.93it/s]
epoch 2 loss 0.684  acc 0.553: 100%|██████████| 410/410 [01:23<00:00,  5.33it/s]
epoch 2 loss 0.684  acc 0.553: 100%|██████████| 410/410 [01:23<00:00,  4.92it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.679  acc 0.566: 100%|█████████▉| 408/410 [01:22<00:00,  4.94it/s]
epoch 3 loss 0.679  acc 0.566: 100%|█████████▉| 408/410 [01:22<00:00,  4.94it/s]
epoch 3 loss 0.679  acc 0.566: 100%|█████████▉| 409/410 [01:22<00:00,  4.94it/s]
epoch 3 loss 0.679  acc 0.566: 100%|█████████▉| 409/410 [01:23<00:00,  4.94it/s]
epoch 3 loss 0.679  acc 0.566: 100%|██████████| 410/410 [01:23<00:00,  5.34it/s]
epoch 3 loss 0.679  acc 0.566: 100%|██████████| 410/410 [01:23<00:00,  4.93it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.678  acc 0.568: 100%|█████████▉| 408/410 [01:22<00:00,  4.92it/s]
epoch 4 loss 0.678  acc 0.568: 100%|█████████▉| 408/410 [01:22<00:00,  4.92it/s]
epoch 4 loss 0.678  acc 0.568: 100%|█████████▉| 409/410 [01:22<00:00,  4.92it/s]
epoch 4 loss 0.678  acc 0.567: 100%|█████████▉| 409/410 [01:23<00:00,  4.92it/s]
epoch 4 loss 0.678  acc 0.567: 100%|██████████| 410/410 [01:23<00:00,  5.32it/s]
epoch 4 loss 0.678  acc 0.567: 100%|██████████| 410/410 [01:23<00:00,  4.93it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 22:25:52,623 >> Trainable Ratio: 1238481/125293521=0.988464%
[INFO|(OpenDelta)basemodel:702]2024-04-19 22:25:52,623 >> Delta Parameter Ratio: 647120/125293521=0.516483%
[INFO|(OpenDelta)basemodel:704]2024-04-19 22:25:52,624 >> Static Memory 0.95 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.553 


 ==> Delta parameters :   [0, 0, {'insert_modules': ['attention', 'attention.self'], 'bottleneck_dim': [24, 64, 64], 'non_linearity': 'relu'}, 0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [32, 128], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.698  acc 0.52: 100%|█████████▉| 408/410 [01:48<00:00,  3.74it/s]
epoch 0 loss 0.698  acc 0.52: 100%|█████████▉| 408/410 [01:48<00:00,  3.74it/s]
epoch 0 loss 0.698  acc 0.52: 100%|█████████▉| 409/410 [01:48<00:00,  3.74it/s]
epoch 0 loss 0.698  acc 0.52: 100%|█████████▉| 409/410 [01:48<00:00,  3.74it/s]
epoch 0 loss 0.698  acc 0.52: 100%|██████████| 410/410 [01:48<00:00,  4.04it/s]
epoch 0 loss 0.698  acc 0.52: 100%|██████████| 410/410 [01:48<00:00,  3.77it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.695  acc 0.53: 100%|█████████▉| 408/410 [01:48<00:00,  3.75it/s]
epoch 1 loss 0.695  acc 0.529: 100%|█████████▉| 408/410 [01:49<00:00,  3.75it/s]
epoch 1 loss 0.695  acc 0.529: 100%|█████████▉| 409/410 [01:49<00:00,  3.75it/s]
epoch 1 loss 0.695  acc 0.53: 100%|█████████▉| 409/410 [01:49<00:00,  3.75it/s] 
epoch 1 loss 0.695  acc 0.53: 100%|██████████| 410/410 [01:49<00:00,  4.06it/s]
epoch 1 loss 0.695  acc 0.53: 100%|██████████| 410/410 [01:49<00:00,  3.75it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.689  acc 0.546: 100%|█████████▉| 408/410 [01:48<00:00,  3.77it/s]
epoch 2 loss 0.689  acc 0.547: 100%|█████████▉| 408/410 [01:48<00:00,  3.77it/s]
epoch 2 loss 0.689  acc 0.547: 100%|█████████▉| 409/410 [01:48<00:00,  3.77it/s]
epoch 2 loss 0.689  acc 0.547: 100%|█████████▉| 409/410 [01:48<00:00,  3.77it/s]
epoch 2 loss 0.689  acc 0.547: 100%|██████████| 410/410 [01:48<00:00,  4.07it/s]
epoch 2 loss 0.689  acc 0.547: 100%|██████████| 410/410 [01:49<00:00,  3.76it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.681  acc 0.562: 100%|█████████▉| 408/410 [01:48<00:00,  3.78it/s]
epoch 3 loss 0.681  acc 0.562: 100%|█████████▉| 408/410 [01:48<00:00,  3.78it/s]
epoch 3 loss 0.681  acc 0.562: 100%|█████████▉| 409/410 [01:48<00:00,  3.78it/s]
epoch 3 loss 0.681  acc 0.562: 100%|█████████▉| 409/410 [01:48<00:00,  3.78it/s]
epoch 3 loss 0.681  acc 0.562: 100%|██████████| 410/410 [01:48<00:00,  4.09it/s]
epoch 3 loss 0.681  acc 0.562: 100%|██████████| 410/410 [01:48<00:00,  3.77it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.677  acc 0.574: 100%|█████████▉| 408/410 [01:48<00:00,  3.77it/s]
epoch 4 loss 0.676  acc 0.574: 100%|█████████▉| 408/410 [01:48<00:00,  3.77it/s]
epoch 4 loss 0.676  acc 0.574: 100%|█████████▉| 409/410 [01:48<00:00,  3.77it/s]
epoch 4 loss 0.676  acc 0.575: 100%|█████████▉| 409/410 [01:48<00:00,  3.77it/s]
epoch 4 loss 0.676  acc 0.575: 100%|██████████| 410/410 [01:48<00:00,  4.07it/s]
epoch 4 loss 0.676  acc 0.575: 100%|██████████| 410/410 [01:48<00:00,  3.77it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 22:35:48,454 >> Trainable Ratio: 1897249/125952289=1.506324%
[INFO|(OpenDelta)basemodel:702]2024-04-19 22:35:48,454 >> Delta Parameter Ratio: 1305888/125952289=1.036812%
[INFO|(OpenDelta)basemodel:704]2024-04-19 22:35:48,454 >> Static Memory 0.48 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.554 


 ==> Delta parameters :   [{'insert_modules': ['attention'], 'bottleneck_dim': [128, 64], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['attention.self'], 'bottleneck_dim': [24, 64, 32], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [32, 24], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, {'insert_modules': ['intermediate', 'attention', 'attention.self'], 'bottleneck_dim': [24, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'intermediate'], 'bottleneck_dim': [24, 32, 128], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.695  acc 0.522: 100%|█████████▉| 408/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.695  acc 0.522: 100%|█████████▉| 408/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.695  acc 0.522: 100%|█████████▉| 409/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.695  acc 0.522: 100%|█████████▉| 409/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.695  acc 0.522: 100%|██████████| 410/410 [02:02<00:00,  3.58it/s]
epoch 0 loss 0.695  acc 0.522: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.691  acc 0.541: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 1 loss 0.691  acc 0.54: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s] 
epoch 1 loss 0.691  acc 0.54: 100%|█████████▉| 409/410 [02:02<00:00,  3.33it/s]
epoch 1 loss 0.691  acc 0.541: 100%|█████████▉| 409/410 [02:03<00:00,  3.33it/s]
epoch 1 loss 0.691  acc 0.541: 100%|██████████| 410/410 [02:03<00:00,  3.60it/s]
epoch 1 loss 0.691  acc 0.541: 100%|██████████| 410/410 [02:03<00:00,  3.33it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.684  acc 0.552: 100%|█████████▉| 408/410 [02:02<00:00,  3.36it/s]
epoch 2 loss 0.684  acc 0.551: 100%|█████████▉| 408/410 [02:02<00:00,  3.36it/s]
epoch 2 loss 0.684  acc 0.551: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 2 loss 0.684  acc 0.552: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 2 loss 0.684  acc 0.552: 100%|██████████| 410/410 [02:02<00:00,  3.62it/s]
epoch 2 loss 0.684  acc 0.552: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.675  acc 0.563: 100%|█████████▉| 408/410 [02:01<00:00,  3.35it/s]
epoch 3 loss 0.675  acc 0.563: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.675  acc 0.563: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 3 loss 0.675  acc 0.563: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 3 loss 0.675  acc 0.563: 100%|██████████| 410/410 [02:02<00:00,  3.63it/s]
epoch 3 loss 0.675  acc 0.563: 100%|██████████| 410/410 [02:02<00:00,  3.35it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.673  acc 0.568: 100%|█████████▉| 408/410 [02:01<00:00,  3.35it/s]
epoch 4 loss 0.673  acc 0.568: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.673  acc 0.568: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.672  acc 0.568: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.672  acc 0.568: 100%|██████████| 410/410 [02:02<00:00,  3.63it/s]
epoch 4 loss 0.672  acc 0.568: 100%|██████████| 410/410 [02:02<00:00,  3.35it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 22:46:53,470 >> Trainable Ratio: 2708721/126763761=2.136826%
[INFO|(OpenDelta)basemodel:702]2024-04-19 22:46:53,470 >> Delta Parameter Ratio: 2117360/126763761=1.670320%
[INFO|(OpenDelta)basemodel:704]2024-04-19 22:46:53,470 >> Static Memory 0.95 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.553 


 ==> Delta parameters :   [0, 0, 0, 0, {'insert_modules': ['intermediate', 'output', 'attention.self'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['attention', 'intermediate'], 'bottleneck_dim': [32, 128, 24], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['intermediate', 'attention.self', 'attention'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention', 'intermediate'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.697  acc 0.526: 100%|█████████▉| 408/410 [01:43<00:00,  3.90it/s]
epoch 0 loss 0.697  acc 0.526: 100%|█████████▉| 408/410 [01:44<00:00,  3.90it/s]
epoch 0 loss 0.697  acc 0.526: 100%|█████████▉| 409/410 [01:44<00:00,  3.90it/s]
epoch 0 loss 0.697  acc 0.526: 100%|█████████▉| 409/410 [01:44<00:00,  3.90it/s]
epoch 0 loss 0.697  acc 0.526: 100%|██████████| 410/410 [01:44<00:00,  4.21it/s]
epoch 0 loss 0.697  acc 0.526: 100%|██████████| 410/410 [01:44<00:00,  3.93it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.689  acc 0.543: 100%|█████████▉| 408/410 [01:44<00:00,  3.92it/s]
epoch 1 loss 0.689  acc 0.543: 100%|█████████▉| 408/410 [01:44<00:00,  3.92it/s]
epoch 1 loss 0.689  acc 0.543: 100%|█████████▉| 409/410 [01:44<00:00,  3.92it/s]
epoch 1 loss 0.69  acc 0.543: 100%|█████████▉| 409/410 [01:44<00:00,  3.92it/s] 
epoch 1 loss 0.69  acc 0.543: 100%|██████████| 410/410 [01:44<00:00,  4.24it/s]
epoch 1 loss 0.69  acc 0.543: 100%|██████████| 410/410 [01:44<00:00,  3.91it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.673  acc 0.574: 100%|█████████▉| 408/410 [01:44<00:00,  3.92it/s]
epoch 2 loss 0.673  acc 0.575: 100%|█████████▉| 408/410 [01:44<00:00,  3.92it/s]
epoch 2 loss 0.673  acc 0.575: 100%|█████████▉| 409/410 [01:44<00:00,  3.92it/s]
epoch 2 loss 0.673  acc 0.575: 100%|█████████▉| 409/410 [01:44<00:00,  3.92it/s]
epoch 2 loss 0.673  acc 0.575: 100%|██████████| 410/410 [01:44<00:00,  4.24it/s]
epoch 2 loss 0.673  acc 0.575: 100%|██████████| 410/410 [01:44<00:00,  3.92it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.665  acc 0.587: 100%|█████████▉| 408/410 [01:43<00:00,  3.95it/s]
epoch 3 loss 0.665  acc 0.588: 100%|█████████▉| 408/410 [01:43<00:00,  3.95it/s]
epoch 3 loss 0.665  acc 0.588: 100%|█████████▉| 409/410 [01:43<00:00,  3.95it/s]
epoch 3 loss 0.665  acc 0.587: 100%|█████████▉| 409/410 [01:44<00:00,  3.95it/s]
epoch 3 loss 0.665  acc 0.587: 100%|██████████| 410/410 [01:44<00:00,  4.26it/s]
epoch 3 loss 0.665  acc 0.587: 100%|██████████| 410/410 [01:44<00:00,  3.94it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.659  acc 0.597: 100%|█████████▉| 408/410 [01:43<00:00,  3.93it/s]
epoch 4 loss 0.659  acc 0.597: 100%|█████████▉| 408/410 [01:43<00:00,  3.93it/s]
epoch 4 loss 0.659  acc 0.597: 100%|█████████▉| 409/410 [01:43<00:00,  3.93it/s]
epoch 4 loss 0.658  acc 0.598: 100%|█████████▉| 409/410 [01:44<00:00,  3.93it/s]
epoch 4 loss 0.658  acc 0.598: 100%|██████████| 410/410 [01:44<00:00,  4.25it/s]
epoch 4 loss 0.658  acc 0.598: 100%|██████████| 410/410 [01:44<00:00,  3.94it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 22:56:28,137 >> Trainable Ratio: 1110321/125165361=0.887083%
[INFO|(OpenDelta)basemodel:702]2024-04-19 22:56:28,137 >> Delta Parameter Ratio: 518960/125165361=0.414620%
[INFO|(OpenDelta)basemodel:704]2024-04-19 22:56:28,137 >> Static Memory 0.49 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.562 


 ==> Delta parameters :   [0, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [16, 64, 128], 'non_linearity': 'relu'}, {'insert_modules': ['attention'], 'bottleneck_dim': [128, 64], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.521: 100%|█████████▉| 408/410 [01:53<00:00,  3.57it/s]
epoch 0 loss 0.696  acc 0.521: 100%|█████████▉| 408/410 [01:53<00:00,  3.57it/s]
epoch 0 loss 0.696  acc 0.521: 100%|█████████▉| 409/410 [01:53<00:00,  3.57it/s]
epoch 0 loss 0.696  acc 0.52: 100%|█████████▉| 409/410 [01:53<00:00,  3.57it/s] 
epoch 0 loss 0.696  acc 0.52: 100%|██████████| 410/410 [01:53<00:00,  3.86it/s]
epoch 0 loss 0.696  acc 0.52: 100%|██████████| 410/410 [01:53<00:00,  3.60it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.693  acc 0.533: 100%|█████████▉| 408/410 [01:53<00:00,  3.59it/s]
epoch 1 loss 0.693  acc 0.532: 100%|█████████▉| 408/410 [01:54<00:00,  3.59it/s]
epoch 1 loss 0.693  acc 0.532: 100%|█████████▉| 409/410 [01:54<00:00,  3.59it/s]
epoch 1 loss 0.693  acc 0.532: 100%|█████████▉| 409/410 [01:54<00:00,  3.59it/s]
epoch 1 loss 0.693  acc 0.532: 100%|██████████| 410/410 [01:54<00:00,  3.89it/s]
epoch 1 loss 0.693  acc 0.532: 100%|██████████| 410/410 [01:54<00:00,  3.58it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.687  acc 0.547: 100%|█████████▉| 408/410 [01:53<00:00,  3.61it/s]
epoch 2 loss 0.687  acc 0.547: 100%|█████████▉| 408/410 [01:53<00:00,  3.61it/s]
epoch 2 loss 0.687  acc 0.547: 100%|█████████▉| 409/410 [01:53<00:00,  3.61it/s]
epoch 2 loss 0.687  acc 0.547: 100%|█████████▉| 409/410 [01:53<00:00,  3.61it/s]
epoch 2 loss 0.687  acc 0.547: 100%|██████████| 410/410 [01:53<00:00,  3.90it/s]
epoch 2 loss 0.687  acc 0.547: 100%|██████████| 410/410 [01:53<00:00,  3.60it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.686  acc 0.546: 100%|█████████▉| 408/410 [01:53<00:00,  3.62it/s]
epoch 3 loss 0.686  acc 0.547: 100%|█████████▉| 408/410 [01:53<00:00,  3.62it/s]
epoch 3 loss 0.686  acc 0.547: 100%|█████████▉| 409/410 [01:53<00:00,  3.62it/s]
epoch 3 loss 0.685  acc 0.547: 100%|█████████▉| 409/410 [01:53<00:00,  3.62it/s]
epoch 3 loss 0.685  acc 0.547: 100%|██████████| 410/410 [01:53<00:00,  3.91it/s]
epoch 3 loss 0.685  acc 0.547: 100%|██████████| 410/410 [01:53<00:00,  3.61it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.682  acc 0.552: 100%|█████████▉| 408/410 [01:53<00:00,  3.61it/s]
epoch 4 loss 0.682  acc 0.552: 100%|█████████▉| 408/410 [01:53<00:00,  3.61it/s]
epoch 4 loss 0.682  acc 0.552: 100%|█████████▉| 409/410 [01:53<00:00,  3.61it/s]
epoch 4 loss 0.683  acc 0.552: 100%|█████████▉| 409/410 [01:53<00:00,  3.61it/s]
epoch 4 loss 0.683  acc 0.552: 100%|██████████| 410/410 [01:53<00:00,  3.89it/s]
epoch 4 loss 0.683  acc 0.552: 100%|██████████| 410/410 [01:53<00:00,  3.61it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 23:06:48,966 >> Trainable Ratio: 4720321/128775361=3.665547%
[INFO|(OpenDelta)basemodel:702]2024-04-19 23:06:48,966 >> Delta Parameter Ratio: 4128960/128775361=3.206328%
[INFO|(OpenDelta)basemodel:704]2024-04-19 23:06:48,966 >> Static Memory 0.96 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.546 


 ==> Delta parameters :   [{'insert_modules': ['attention.self'], 'bottleneck_dim': [64, 16, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [128, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [64, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [16, 32, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self', 'intermediate'], 'bottleneck_dim': [24, 128, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [16, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention'], 'bottleneck_dim': [128, 64, 128], 'non_linearity': 'gelu_new'}, 0, {'insert_modules': ['attention.self', 'intermediate', 'attention'], 'bottleneck_dim': [128, 24, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'intermediate', 'output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.699  acc 0.525: 100%|█████████▉| 408/410 [02:13<00:00,  3.04it/s]
epoch 0 loss 0.699  acc 0.525: 100%|█████████▉| 408/410 [02:13<00:00,  3.04it/s]
epoch 0 loss 0.699  acc 0.525: 100%|█████████▉| 409/410 [02:13<00:00,  3.04it/s]
epoch 0 loss 0.699  acc 0.525: 100%|█████████▉| 409/410 [02:13<00:00,  3.04it/s]
epoch 0 loss 0.699  acc 0.525: 100%|██████████| 410/410 [02:13<00:00,  3.28it/s]
epoch 0 loss 0.699  acc 0.525: 100%|██████████| 410/410 [02:13<00:00,  3.06it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.689  acc 0.542: 100%|█████████▉| 408/410 [02:13<00:00,  3.06it/s]
epoch 1 loss 0.689  acc 0.542: 100%|█████████▉| 408/410 [02:14<00:00,  3.06it/s]
epoch 1 loss 0.689  acc 0.542: 100%|█████████▉| 409/410 [02:14<00:00,  3.06it/s]
epoch 1 loss 0.689  acc 0.542: 100%|█████████▉| 409/410 [02:14<00:00,  3.06it/s]
epoch 1 loss 0.689  acc 0.542: 100%|██████████| 410/410 [02:14<00:00,  3.30it/s]
epoch 1 loss 0.689  acc 0.542: 100%|██████████| 410/410 [02:14<00:00,  3.05it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.678  acc 0.549: 100%|█████████▉| 408/410 [02:12<00:00,  3.07it/s]
epoch 2 loss 0.678  acc 0.55: 100%|█████████▉| 408/410 [02:13<00:00,  3.07it/s] 
epoch 2 loss 0.678  acc 0.55: 100%|█████████▉| 409/410 [02:13<00:00,  3.07it/s]
epoch 2 loss 0.678  acc 0.55: 100%|█████████▉| 409/410 [02:13<00:00,  3.07it/s]
epoch 2 loss 0.678  acc 0.55: 100%|██████████| 410/410 [02:13<00:00,  3.32it/s]
epoch 2 loss 0.678  acc 0.55: 100%|██████████| 410/410 [02:13<00:00,  3.07it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.673  acc 0.568: 100%|█████████▉| 408/410 [02:12<00:00,  3.07it/s]
epoch 3 loss 0.673  acc 0.569: 100%|█████████▉| 408/410 [02:13<00:00,  3.07it/s]
epoch 3 loss 0.673  acc 0.569: 100%|█████████▉| 409/410 [02:13<00:00,  3.06it/s]
epoch 3 loss 0.673  acc 0.569: 100%|█████████▉| 409/410 [02:13<00:00,  3.06it/s]
epoch 3 loss 0.673  acc 0.569: 100%|██████████| 410/410 [02:13<00:00,  3.31it/s]
epoch 3 loss 0.673  acc 0.569: 100%|██████████| 410/410 [02:13<00:00,  3.07it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.666  acc 0.581: 100%|█████████▉| 408/410 [02:12<00:00,  3.08it/s]
epoch 4 loss 0.666  acc 0.58: 100%|█████████▉| 408/410 [02:13<00:00,  3.08it/s] 
epoch 4 loss 0.666  acc 0.58: 100%|█████████▉| 409/410 [02:13<00:00,  3.08it/s]
epoch 4 loss 0.666  acc 0.58: 100%|█████████▉| 409/410 [02:13<00:00,  3.08it/s]
epoch 4 loss 0.666  acc 0.58: 100%|██████████| 410/410 [02:13<00:00,  3.32it/s]
epoch 4 loss 0.666  acc 0.58: 100%|██████████| 410/410 [02:13<00:00,  3.07it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 23:18:52,616 >> Trainable Ratio: 4144505/128199545=3.232855%
[INFO|(OpenDelta)basemodel:702]2024-04-19 23:18:52,616 >> Delta Parameter Ratio: 3553144/128199545=2.771573%
[INFO|(OpenDelta)basemodel:704]2024-04-19 23:18:52,616 >> Static Memory 0.50 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.563 


 ==> Delta parameters :   [0, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [128, 128, 32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output', 'intermediate'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output'], 'bottleneck_dim': [16, 24, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output'], 'bottleneck_dim': [16, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output'], 'bottleneck_dim': [24, 16, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self', 'intermediate'], 'bottleneck_dim': [16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output'], 'bottleneck_dim': [16, 24], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'attention.self', 'output'], 'bottleneck_dim': [64, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [64, 128], 'non_linearity': 'gelu_new'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.695  acc 0.528: 100%|█████████▉| 408/410 [02:04<00:00,  3.25it/s]
epoch 0 loss 0.695  acc 0.529: 100%|█████████▉| 408/410 [02:05<00:00,  3.25it/s]
epoch 0 loss 0.695  acc 0.529: 100%|█████████▉| 409/410 [02:05<00:00,  3.25it/s]
epoch 0 loss 0.695  acc 0.528: 100%|█████████▉| 409/410 [02:05<00:00,  3.25it/s]
epoch 0 loss 0.695  acc 0.528: 100%|██████████| 410/410 [02:05<00:00,  3.51it/s]
epoch 0 loss 0.695  acc 0.528: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.69  acc 0.548: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 1 loss 0.69  acc 0.549: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 1 loss 0.69  acc 0.549: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 1 loss 0.69  acc 0.549: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 1 loss 0.69  acc 0.549: 100%|██████████| 410/410 [02:05<00:00,  3.53it/s]
epoch 1 loss 0.69  acc 0.549: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.678  acc 0.559: 100%|█████████▉| 408/410 [02:04<00:00,  3.29it/s]
epoch 2 loss 0.678  acc 0.559: 100%|█████████▉| 408/410 [02:04<00:00,  3.29it/s]
epoch 2 loss 0.678  acc 0.559: 100%|█████████▉| 409/410 [02:04<00:00,  3.29it/s]
epoch 2 loss 0.678  acc 0.559: 100%|█████████▉| 409/410 [02:05<00:00,  3.29it/s]
epoch 2 loss 0.678  acc 0.559: 100%|██████████| 410/410 [02:05<00:00,  3.55it/s]
epoch 2 loss 0.678  acc 0.559: 100%|██████████| 410/410 [02:05<00:00,  3.28it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.667  acc 0.583: 100%|█████████▉| 408/410 [02:04<00:00,  3.29it/s]
epoch 3 loss 0.667  acc 0.583: 100%|█████████▉| 408/410 [02:04<00:00,  3.29it/s]
epoch 3 loss 0.667  acc 0.583: 100%|█████████▉| 409/410 [02:04<00:00,  3.29it/s]
epoch 3 loss 0.667  acc 0.583: 100%|█████████▉| 409/410 [02:04<00:00,  3.29it/s]
epoch 3 loss 0.667  acc 0.583: 100%|██████████| 410/410 [02:04<00:00,  3.55it/s]
epoch 3 loss 0.667  acc 0.583: 100%|██████████| 410/410 [02:04<00:00,  3.28it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.66  acc 0.599: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 4 loss 0.66  acc 0.599: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 4 loss 0.66  acc 0.599: 100%|█████████▉| 409/410 [02:04<00:00,  3.29it/s]
epoch 4 loss 0.66  acc 0.598: 100%|█████████▉| 409/410 [02:04<00:00,  3.29it/s]
epoch 4 loss 0.66  acc 0.598: 100%|██████████| 410/410 [02:04<00:00,  3.55it/s]
epoch 4 loss 0.66  acc 0.598: 100%|██████████| 410/410 [02:04<00:00,  3.28it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 23:30:12,627 >> Trainable Ratio: 1999905/126054945=1.586534%
[INFO|(OpenDelta)basemodel:702]2024-04-19 23:30:12,627 >> Delta Parameter Ratio: 1408544/126054945=1.117405%
[INFO|(OpenDelta)basemodel:704]2024-04-19 23:30:12,627 >> Static Memory 0.50 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.575 


 ==> Delta parameters :   [0, {'insert_modules': ['attention.self'], 'bottleneck_dim': [16, 16], 'non_linearity': 'relu'}, {'insert_modules': ['attention', 'output'], 'bottleneck_dim': [64], 'non_linearity': 'relu'}, 0, 0, {'insert_modules': ['attention', 'attention.self', 'intermediate'], 'bottleneck_dim': [128], 'non_linearity': 'relu'}, 0, 0, 0, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.693  acc 0.532: 100%|█████████▉| 408/410 [01:55<00:00,  3.52it/s]
epoch 0 loss 0.693  acc 0.532: 100%|█████████▉| 408/410 [01:55<00:00,  3.52it/s]
epoch 0 loss 0.693  acc 0.532: 100%|█████████▉| 409/410 [01:55<00:00,  3.53it/s]
epoch 0 loss 0.693  acc 0.533: 100%|█████████▉| 409/410 [01:55<00:00,  3.53it/s]
epoch 0 loss 0.693  acc 0.533: 100%|██████████| 410/410 [01:55<00:00,  3.83it/s]
epoch 0 loss 0.693  acc 0.533: 100%|██████████| 410/410 [01:55<00:00,  3.55it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.685  acc 0.553: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 1 loss 0.685  acc 0.553: 100%|█████████▉| 408/410 [01:55<00:00,  3.53it/s]
epoch 1 loss 0.685  acc 0.553: 100%|█████████▉| 409/410 [01:55<00:00,  3.53it/s]
epoch 1 loss 0.685  acc 0.554: 100%|█████████▉| 409/410 [01:56<00:00,  3.53it/s]
epoch 1 loss 0.685  acc 0.554: 100%|██████████| 410/410 [01:56<00:00,  3.82it/s]
epoch 1 loss 0.685  acc 0.554: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.671  acc 0.578: 100%|█████████▉| 408/410 [01:54<00:00,  3.55it/s]
epoch 2 loss 0.671  acc 0.578: 100%|█████████▉| 408/410 [01:55<00:00,  3.55it/s]
epoch 2 loss 0.671  acc 0.578: 100%|█████████▉| 409/410 [01:55<00:00,  3.55it/s]
epoch 2 loss 0.671  acc 0.578: 100%|█████████▉| 409/410 [01:55<00:00,  3.55it/s]
epoch 2 loss 0.671  acc 0.578: 100%|██████████| 410/410 [01:55<00:00,  3.84it/s]
epoch 2 loss 0.671  acc 0.578: 100%|██████████| 410/410 [01:55<00:00,  3.55it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.662  acc 0.587: 100%|█████████▉| 408/410 [01:54<00:00,  3.56it/s]
epoch 3 loss 0.662  acc 0.587: 100%|█████████▉| 408/410 [01:54<00:00,  3.56it/s]
epoch 3 loss 0.662  acc 0.587: 100%|█████████▉| 409/410 [01:54<00:00,  3.56it/s]
epoch 3 loss 0.662  acc 0.587: 100%|█████████▉| 409/410 [01:54<00:00,  3.56it/s]
epoch 3 loss 0.662  acc 0.587: 100%|██████████| 410/410 [01:54<00:00,  3.85it/s]
epoch 3 loss 0.662  acc 0.587: 100%|██████████| 410/410 [01:55<00:00,  3.56it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.655  acc 0.604: 100%|█████████▉| 408/410 [01:54<00:00,  3.56it/s]
epoch 4 loss 0.655  acc 0.604: 100%|█████████▉| 408/410 [01:54<00:00,  3.56it/s]
epoch 4 loss 0.655  acc 0.604: 100%|█████████▉| 409/410 [01:54<00:00,  3.55it/s]
epoch 4 loss 0.654  acc 0.604: 100%|█████████▉| 409/410 [01:55<00:00,  3.55it/s]
epoch 4 loss 0.654  acc 0.604: 100%|██████████| 410/410 [01:55<00:00,  3.84it/s]
epoch 4 loss 0.654  acc 0.604: 100%|██████████| 410/410 [01:55<00:00,  3.56it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 23:40:41,430 >> Trainable Ratio: 1042193/125097233=0.833106%
[INFO|(OpenDelta)basemodel:702]2024-04-19 23:40:41,430 >> Delta Parameter Ratio: 450832/125097233=0.360385%
[INFO|(OpenDelta)basemodel:704]2024-04-19 23:40:41,430 >> Static Memory 0.97 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.587 


 ==> Delta parameters :   [0, 0, 0, 0, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [24, 16, 64], 'non_linearity': 'relu'}, 0, {'insert_modules': ['intermediate'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, 0, 0, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.692  acc 0.531: 100%|█████████▉| 408/410 [01:31<00:00,  4.45it/s]
epoch 0 loss 0.692  acc 0.532: 100%|█████████▉| 408/410 [01:31<00:00,  4.45it/s]
epoch 0 loss 0.692  acc 0.532: 100%|█████████▉| 409/410 [01:31<00:00,  4.45it/s]
epoch 0 loss 0.692  acc 0.532: 100%|█████████▉| 409/410 [01:31<00:00,  4.45it/s]
epoch 0 loss 0.692  acc 0.532: 100%|██████████| 410/410 [01:31<00:00,  4.81it/s]
epoch 0 loss 0.692  acc 0.532: 100%|██████████| 410/410 [01:31<00:00,  4.48it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.688  acc 0.547: 100%|█████████▉| 408/410 [01:31<00:00,  4.44it/s]
epoch 1 loss 0.688  acc 0.547: 100%|█████████▉| 408/410 [01:32<00:00,  4.44it/s]
epoch 1 loss 0.688  acc 0.547: 100%|█████████▉| 409/410 [01:32<00:00,  4.44it/s]
epoch 1 loss 0.688  acc 0.548: 100%|█████████▉| 409/410 [01:32<00:00,  4.44it/s]
epoch 1 loss 0.688  acc 0.548: 100%|██████████| 410/410 [01:32<00:00,  4.80it/s]
epoch 1 loss 0.688  acc 0.548: 100%|██████████| 410/410 [01:32<00:00,  4.44it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 408/410 [01:31<00:00,  4.45it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 408/410 [01:31<00:00,  4.45it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 409/410 [01:31<00:00,  4.45it/s]
epoch 2 loss 0.684  acc 0.553: 100%|█████████▉| 409/410 [01:32<00:00,  4.45it/s]
epoch 2 loss 0.684  acc 0.553: 100%|██████████| 410/410 [01:32<00:00,  4.80it/s]
epoch 2 loss 0.684  acc 0.553: 100%|██████████| 410/410 [01:32<00:00,  4.45it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.678  acc 0.56: 100%|█████████▉| 408/410 [01:31<00:00,  4.47it/s]
epoch 3 loss 0.678  acc 0.561: 100%|█████████▉| 408/410 [01:31<00:00,  4.47it/s]
epoch 3 loss 0.678  acc 0.561: 100%|█████████▉| 409/410 [01:31<00:00,  4.47it/s]
epoch 3 loss 0.678  acc 0.561: 100%|█████████▉| 409/410 [01:31<00:00,  4.47it/s]
epoch 3 loss 0.678  acc 0.561: 100%|██████████| 410/410 [01:31<00:00,  4.83it/s]
epoch 3 loss 0.678  acc 0.561: 100%|██████████| 410/410 [01:31<00:00,  4.46it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.673  acc 0.571: 100%|█████████▉| 408/410 [01:31<00:00,  4.48it/s]
epoch 4 loss 0.673  acc 0.571: 100%|█████████▉| 408/410 [01:31<00:00,  4.48it/s]
epoch 4 loss 0.673  acc 0.571: 100%|█████████▉| 409/410 [01:31<00:00,  4.48it/s]
epoch 4 loss 0.673  acc 0.571: 100%|█████████▉| 409/410 [01:31<00:00,  4.48it/s]
epoch 4 loss 0.673  acc 0.571: 100%|██████████| 410/410 [01:31<00:00,  4.84it/s]
epoch 4 loss 0.673  acc 0.571: 100%|██████████| 410/410 [01:31<00:00,  4.47it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 23:49:12,489 >> Trainable Ratio: 4273881/128328921=3.330411%
[INFO|(OpenDelta)basemodel:702]2024-04-19 23:49:12,489 >> Delta Parameter Ratio: 3682520/128328921=2.869595%
[INFO|(OpenDelta)basemodel:704]2024-04-19 23:49:12,489 >> Static Memory 0.48 GB, Max Memory 10.11 GB
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-19 23:49:13,868 >> Trainable Ratio: 2270209/126325249=1.797114%
[INFO|(OpenDelta)basemodel:702]2024-04-19 23:49:13,868 >> Delta Parameter Ratio: 1678848/126325249=1.328988%
[INFO|(OpenDelta)basemodel:704]2024-04-19 23:49:13,868 >> Static Memory 0.96 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.545 


 ==> Delta parameters :   [{'insert_modules': ['output', 'intermediate', 'attention'], 'bottleneck_dim': [128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'attention.self'], 'bottleneck_dim': [64, 16], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'intermediate'], 'bottleneck_dim': [32, 32, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention'], 'bottleneck_dim': [64, 64, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [128, 32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self', 'intermediate'], 'bottleneck_dim': [16, 32, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [16, 128], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['intermediate', 'output'], 'bottleneck_dim': [32], 'non_linearity': 'gelu_new'}, {'insert_modules': ['output', 'attention.self', 'attention'], 'bottleneck_dim': [24, 64], 'non_linearity': 'gelu_new'}, {'insert_modules': ['attention', 'intermediate'], 'bottleneck_dim': [24, 16, 32], 'non_linearity': 'gelu_new'}] 


 ==> Delta parameters :   [{'insert_modules': ['attention', 'output', 'intermediate'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'intermediate', 'attention.self'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention', 'attention.self', 'output'], 'bottleneck_dim': [32, 24], 'non_linearity': 'relu'}, {'insert_modules': ['output', 'attention.self'], 'bottleneck_dim': [64, 32], 'non_linearity': 'relu'}, {'insert_modules': ['attention'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, {'insert_modules': ['attention', 'output', 'attention.self'], 'bottleneck_dim': [32, 32], 'non_linearity': 'relu'}, 0, {'insert_modules': ['attention.self', 'intermediate', 'output'], 'bottleneck_dim': [64], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [16, 64], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'output', 'intermediate'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.699  acc 0.518: 100%|█████████▉| 408/410 [02:06<00:00,  3.20it/s]
epoch 0 loss 0.699  acc 0.518: 100%|█████████▉| 408/410 [02:06<00:00,  3.20it/s]
epoch 0 loss 0.699  acc 0.518: 100%|█████████▉| 409/410 [02:06<00:00,  3.20it/s]
epoch 0 loss 0.699  acc 0.519: 100%|█████████▉| 409/410 [02:07<00:00,  3.20it/s]
epoch 0 loss 0.699  acc 0.519: 100%|██████████| 410/410 [02:07<00:00,  3.46it/s]
epoch 0 loss 0.699  acc 0.519: 100%|██████████| 410/410 [02:07<00:00,  3.23it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.686  acc 0.544: 100%|█████████▉| 408/410 [02:07<00:00,  3.22it/s]
epoch 1 loss 0.686  acc 0.544: 100%|█████████▉| 408/410 [02:07<00:00,  3.22it/s]
epoch 1 loss 0.686  acc 0.544: 100%|█████████▉| 409/410 [02:07<00:00,  3.22it/s]
epoch 1 loss 0.686  acc 0.543: 100%|█████████▉| 409/410 [02:07<00:00,  3.22it/s]
epoch 1 loss 0.686  acc 0.543: 100%|██████████| 410/410 [02:07<00:00,  3.48it/s]
epoch 1 loss 0.686  acc 0.543: 100%|██████████| 410/410 [02:07<00:00,  3.21it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.678  acc 0.557: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 2 loss 0.679  acc 0.556: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 2 loss 0.679  acc 0.556: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 2 loss 0.679  acc 0.556: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 2 loss 0.679  acc 0.556: 100%|██████████| 410/410 [02:06<00:00,  3.50it/s]
epoch 2 loss 0.679  acc 0.556: 100%|██████████| 410/410 [02:06<00:00,  3.23it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.669  acc 0.575: 100%|█████████▉| 408/410 [02:05<00:00,  3.24it/s]
epoch 3 loss 0.669  acc 0.575: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 3 loss 0.669  acc 0.575: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 3 loss 0.669  acc 0.574: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 3 loss 0.669  acc 0.574: 100%|██████████| 410/410 [02:06<00:00,  3.51it/s]
epoch 3 loss 0.669  acc 0.574: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.664  acc 0.588: 100%|█████████▉| 408/410 [02:05<00:00,  3.24it/s]
epoch 4 loss 0.664  acc 0.588: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 4 loss 0.664  acc 0.588: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 4 loss 0.664  acc 0.588: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 4 loss 0.664  acc 0.588: 100%|██████████| 410/410 [02:06<00:00,  3.50it/s]
epoch 4 loss 0.664  acc 0.588: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 00:00:41,351 >> Trainable Ratio: 2126713/126181753=1.685436%
[INFO|(OpenDelta)basemodel:702]2024-04-20 00:00:41,351 >> Delta Parameter Ratio: 1535352/126181753=1.216778%
[INFO|(OpenDelta)basemodel:704]2024-04-20 00:00:41,351 >> Static Memory 1.43 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.57 


 ==> Delta parameters :   [0, {'insert_modules': ['attention.self', 'attention'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, 0, 0, 0, 0, 0, {'insert_modules': ['intermediate', 'output', 'attention.self'], 'bottleneck_dim': [24], 'non_linearity': 'gelu_new'}, 0, 0, {'insert_modules': ['attention', 'intermediate', 'attention.self'], 'bottleneck_dim': [128, 128], 'non_linearity': 'gelu_new'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.695  acc 0.527: 100%|█████████▉| 408/410 [01:56<00:00,  3.47it/s]
epoch 0 loss 0.695  acc 0.528: 100%|█████████▉| 408/410 [01:57<00:00,  3.47it/s]
epoch 0 loss 0.695  acc 0.528: 100%|█████████▉| 409/410 [01:57<00:00,  3.48it/s]
epoch 0 loss 0.695  acc 0.527: 100%|█████████▉| 409/410 [01:57<00:00,  3.48it/s]
epoch 0 loss 0.695  acc 0.527: 100%|██████████| 410/410 [01:57<00:00,  3.76it/s]
epoch 0 loss 0.695  acc 0.527: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.69  acc 0.54: 100%|█████████▉| 408/410 [01:57<00:00,  3.48it/s]
epoch 1 loss 0.69  acc 0.539: 100%|█████████▉| 408/410 [01:57<00:00,  3.48it/s]
epoch 1 loss 0.69  acc 0.539: 100%|█████████▉| 409/410 [01:57<00:00,  3.48it/s]
epoch 1 loss 0.69  acc 0.539: 100%|█████████▉| 409/410 [01:58<00:00,  3.48it/s]
epoch 1 loss 0.69  acc 0.539: 100%|██████████| 410/410 [01:58<00:00,  3.76it/s]
epoch 1 loss 0.69  acc 0.539: 100%|██████████| 410/410 [01:58<00:00,  3.47it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.676  acc 0.57: 100%|█████████▉| 408/410 [01:56<00:00,  3.50it/s]
epoch 2 loss 0.676  acc 0.57: 100%|█████████▉| 408/410 [01:57<00:00,  3.50it/s]
epoch 2 loss 0.676  acc 0.57: 100%|█████████▉| 409/410 [01:57<00:00,  3.50it/s]
epoch 2 loss 0.676  acc 0.57: 100%|█████████▉| 409/410 [01:57<00:00,  3.50it/s]
epoch 2 loss 0.676  acc 0.57: 100%|██████████| 410/410 [01:57<00:00,  3.79it/s]
epoch 2 loss 0.676  acc 0.57: 100%|██████████| 410/410 [01:57<00:00,  3.49it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 408/410 [01:56<00:00,  3.51it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 408/410 [01:56<00:00,  3.51it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 409/410 [01:56<00:00,  3.51it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 409/410 [01:57<00:00,  3.51it/s]
epoch 3 loss 0.672  acc 0.57: 100%|██████████| 410/410 [01:57<00:00,  3.79it/s]
epoch 3 loss 0.672  acc 0.57: 100%|██████████| 410/410 [01:57<00:00,  3.50it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.666  acc 0.576: 100%|█████████▉| 408/410 [01:56<00:00,  3.50it/s]
epoch 4 loss 0.666  acc 0.576: 100%|█████████▉| 408/410 [01:56<00:00,  3.50it/s]
epoch 4 loss 0.666  acc 0.576: 100%|█████████▉| 409/410 [01:56<00:00,  3.50it/s]
epoch 4 loss 0.666  acc 0.576: 100%|█████████▉| 409/410 [01:57<00:00,  3.50it/s]
epoch 4 loss 0.666  acc 0.576: 100%|██████████| 410/410 [01:57<00:00,  3.78it/s]
epoch 4 loss 0.666  acc 0.576: 100%|██████████| 410/410 [01:57<00:00,  3.50it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 00:11:20,788 >> Trainable Ratio: 1124201/125179241=0.898073%
[INFO|(OpenDelta)basemodel:702]2024-04-20 00:11:20,788 >> Delta Parameter Ratio: 532840/125179241=0.425662%
[INFO|(OpenDelta)basemodel:704]2024-04-20 00:11:20,788 >> Static Memory 0.49 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.562 


 ==> Delta parameters :   [0, 0, 0, 0, 0, 0, 0, 0, {'insert_modules': ['attention.self'], 'bottleneck_dim': [24], 'non_linearity': 'relu'}, {'insert_modules': ['attention.self', 'output', 'attention'], 'bottleneck_dim': [128, 64], 'non_linearity': 'relu'}, {'insert_modules': ['output'], 'bottleneck_dim': [16], 'non_linearity': 'relu'}, 0] 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.695  acc 0.522: 100%|█████████▉| 408/410 [01:16<00:00,  5.29it/s]
epoch 0 loss 0.695  acc 0.523: 100%|█████████▉| 408/410 [01:17<00:00,  5.29it/s]
epoch 0 loss 0.695  acc 0.523: 100%|█████████▉| 409/410 [01:17<00:00,  5.29it/s]
epoch 0 loss 0.695  acc 0.523: 100%|█████████▉| 409/410 [01:17<00:00,  5.29it/s]
epoch 0 loss 0.695  acc 0.523: 100%|██████████| 410/410 [01:17<00:00,  5.71it/s]
epoch 0 loss 0.695  acc 0.523: 100%|██████████| 410/410 [01:17<00:00,  5.30it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.69  acc 0.544: 100%|█████████▉| 408/410 [01:17<00:00,  5.25it/s]
epoch 1 loss 0.69  acc 0.544: 100%|█████████▉| 408/410 [01:17<00:00,  5.25it/s]
epoch 1 loss 0.69  acc 0.544: 100%|█████████▉| 409/410 [01:17<00:00,  5.25it/s]
epoch 1 loss 0.689  acc 0.544: 100%|█████████▉| 409/410 [01:17<00:00,  5.25it/s]
epoch 1 loss 0.689  acc 0.544: 100%|██████████| 410/410 [01:17<00:00,  5.68it/s]
epoch 1 loss 0.689  acc 0.544: 100%|██████████| 410/410 [01:17<00:00,  5.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.679  acc 0.56: 100%|█████████▉| 408/410 [01:17<00:00,  5.29it/s]
epoch 2 loss 0.679  acc 0.561: 100%|█████████▉| 408/410 [01:17<00:00,  5.29it/s]
epoch 2 loss 0.679  acc 0.561: 100%|█████████▉| 409/410 [01:17<00:00,  5.29it/s]
epoch 2 loss 0.679  acc 0.561: 100%|█████████▉| 409/410 [01:17<00:00,  5.29it/s]
epoch 2 loss 0.679  acc 0.561: 100%|██████████| 410/410 [01:17<00:00,  5.71it/s]
epoch 2 loss 0.679  acc 0.561: 100%|██████████| 410/410 [01:17<00:00,  5.28it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 408/410 [01:17<00:00,  5.30it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 408/410 [01:17<00:00,  5.30it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 409/410 [01:17<00:00,  5.30it/s]
epoch 3 loss 0.672  acc 0.57: 100%|█████████▉| 409/410 [01:17<00:00,  5.30it/s]
epoch 3 loss 0.672  acc 0.57: 100%|██████████| 410/410 [01:17<00:00,  5.72it/s]
epoch 3 loss 0.672  acc 0.57: 100%|██████████| 410/410 [01:17<00:00,  5.30it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.672  acc 0.572: 100%|█████████▉| 408/410 [01:17<00:00,  5.31it/s]
epoch 4 loss 0.671  acc 0.572: 100%|█████████▉| 408/410 [01:17<00:00,  5.31it/s]
epoch 4 loss 0.671  acc 0.572: 100%|█████████▉| 409/410 [01:17<00:00,  5.30it/s]
epoch 4 loss 0.672  acc 0.572: 100%|█████████▉| 409/410 [01:17<00:00,  5.30it/s]
epoch 4 loss 0.672  acc 0.572: 100%|██████████| 410/410 [01:17<00:00,  5.72it/s]
epoch 4 loss 0.672  acc 0.572: 100%|██████████| 410/410 [01:17<00:00,  5.29it/s]
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 00:18:39,951 >> Trainable Ratio: 4064153/128119193=3.172166%
[INFO|(OpenDelta)basemodel:702]2024-04-20 00:18:39,951 >> Delta Parameter Ratio: 3472792/128119193=2.710595%
[INFO|(OpenDelta)basemodel:704]2024-04-20 00:18:39,951 >> Static Memory 0.49 GB, Max Memory 10.11 GB

 ==> Fitness ( Validation acc ) :   0.554 


  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.691  acc 0.535: 100%|█████████▉| 408/410 [02:05<00:00,  3.24it/s]
epoch 0 loss 0.691  acc 0.535: 100%|█████████▉| 408/410 [02:05<00:00,  3.24it/s]
epoch 0 loss 0.691  acc 0.535: 100%|█████████▉| 409/410 [02:05<00:00,  3.23it/s]
epoch 0 loss 0.691  acc 0.536: 100%|█████████▉| 409/410 [02:05<00:00,  3.23it/s]
epoch 0 loss 0.691  acc 0.536: 100%|██████████| 410/410 [02:05<00:00,  3.49it/s]
epoch 0 loss 0.691  acc 0.536: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.687  acc 0.542: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.687  acc 0.542: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.687  acc 0.542: 100%|█████████▉| 409/410 [02:05<00:00,  3.25it/s]
epoch 1 loss 0.687  acc 0.543: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 1 loss 0.687  acc 0.543: 100%|██████████| 410/410 [02:06<00:00,  3.52it/s]
epoch 1 loss 0.687  acc 0.543: 100%|██████████| 410/410 [02:06<00:00,  3.25it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.669  acc 0.571: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 2 loss 0.669  acc 0.571: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 2 loss 0.669  acc 0.571: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 2 loss 0.669  acc 0.57: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s] 
epoch 2 loss 0.669  acc 0.57: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 2 loss 0.669  acc 0.57: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.662  acc 0.59: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 3 loss 0.661  acc 0.59: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 3 loss 0.661  acc 0.59: 100%|█████████▉| 409/410 [02:04<00:00,  3.28it/s]
epoch 3 loss 0.661  acc 0.59: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 3 loss 0.661  acc 0.59: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 3 loss 0.661  acc 0.59: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.651  acc 0.6: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 4 loss 0.651  acc 0.6: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.651  acc 0.6: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.651  acc 0.6: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.651  acc 0.6: 100%|██████████| 410/410 [02:05<00:00,  3.53it/s]
epoch 4 loss 0.651  acc 0.6: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]
INFO:name:Cycle 1, the best candidate in pop has score 0.595, and best in the run 0.595
INFO:name:Mean 0.5625333333333332 and standard deviation 0.012935051432273267 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 00:30:01,975 >> Trainable Ratio: 1999905/126054945=1.586534%
[INFO|(OpenDelta)basemodel:702]2024-04-20 00:30:01,975 >> Delta Parameter Ratio: 1408544/126054945=1.117405%
[INFO|(OpenDelta)basemodel:704]2024-04-20 00:30:01,976 >> Static Memory 0.96 GB, Max Memory 10.11 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.698  acc 0.512: 100%|█████████▉| 408/410 [01:55<00:00,  3.52it/s]
epoch 0 loss 0.698  acc 0.513: 100%|█████████▉| 408/410 [01:55<00:00,  3.52it/s]
epoch 0 loss 0.698  acc 0.513: 100%|█████████▉| 409/410 [01:55<00:00,  3.52it/s]
epoch 0 loss 0.698  acc 0.513: 100%|█████████▉| 409/410 [01:55<00:00,  3.52it/s]
epoch 0 loss 0.698  acc 0.513: 100%|██████████| 410/410 [01:55<00:00,  3.81it/s]
epoch 0 loss 0.698  acc 0.513: 100%|██████████| 410/410 [01:55<00:00,  3.55it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.684  acc 0.555: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 1 loss 0.684  acc 0.555: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 1 loss 0.684  acc 0.555: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 1 loss 0.684  acc 0.555: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 1 loss 0.684  acc 0.555: 100%|██████████| 410/410 [01:55<00:00,  3.82it/s]
epoch 1 loss 0.684  acc 0.555: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.672  acc 0.569: 100%|█████████▉| 408/410 [01:54<00:00,  3.55it/s]
epoch 2 loss 0.672  acc 0.569: 100%|█████████▉| 408/410 [01:55<00:00,  3.55it/s]
epoch 2 loss 0.672  acc 0.569: 100%|█████████▉| 409/410 [01:55<00:00,  3.56it/s]
epoch 2 loss 0.672  acc 0.57: 100%|█████████▉| 409/410 [01:55<00:00,  3.56it/s] 
epoch 2 loss 0.672  acc 0.57: 100%|██████████| 410/410 [01:55<00:00,  3.85it/s]
epoch 2 loss 0.672  acc 0.57: 100%|██████████| 410/410 [01:55<00:00,  3.55it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.658  acc 0.595: 100%|█████████▉| 408/410 [01:54<00:00,  3.57it/s]
epoch 3 loss 0.658  acc 0.595: 100%|█████████▉| 408/410 [01:54<00:00,  3.57it/s]
epoch 3 loss 0.658  acc 0.595: 100%|█████████▉| 409/410 [01:54<00:00,  3.57it/s]
epoch 3 loss 0.658  acc 0.595: 100%|█████████▉| 409/410 [01:55<00:00,  3.57it/s]
epoch 3 loss 0.658  acc 0.595: 100%|██████████| 410/410 [01:55<00:00,  3.85it/s]
epoch 3 loss 0.658  acc 0.595: 100%|██████████| 410/410 [01:55<00:00,  3.56it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.647  acc 0.603: 100%|█████████▉| 408/410 [01:54<00:00,  3.55it/s]
epoch 4 loss 0.646  acc 0.604: 100%|█████████▉| 408/410 [01:54<00:00,  3.55it/s]
epoch 4 loss 0.646  acc 0.604: 100%|█████████▉| 409/410 [01:54<00:00,  3.56it/s]
epoch 4 loss 0.646  acc 0.604: 100%|█████████▉| 409/410 [01:55<00:00,  3.56it/s]
epoch 4 loss 0.646  acc 0.604: 100%|██████████| 410/410 [01:55<00:00,  3.85it/s]
epoch 4 loss 0.646  acc 0.604: 100%|██████████| 410/410 [01:55<00:00,  3.56it/s]
INFO:name:Cycle 2, the best candidate in pop has score 0.595, and best in the run 0.595
INFO:name:Mean 0.5634333333333333 and standard deviation 0.013108987586978434 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 00:40:31,135 >> Trainable Ratio: 4064153/128119193=3.172166%
[INFO|(OpenDelta)basemodel:702]2024-04-20 00:40:31,135 >> Delta Parameter Ratio: 3472792/128119193=2.710595%
[INFO|(OpenDelta)basemodel:704]2024-04-20 00:40:31,135 >> Static Memory 0.96 GB, Max Memory 10.11 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.515: 100%|█████████▉| 408/410 [02:05<00:00,  3.25it/s]
epoch 0 loss 0.696  acc 0.516: 100%|█████████▉| 408/410 [02:05<00:00,  3.25it/s]
epoch 0 loss 0.696  acc 0.516: 100%|█████████▉| 409/410 [02:05<00:00,  3.25it/s]
epoch 0 loss 0.696  acc 0.516: 100%|█████████▉| 409/410 [02:05<00:00,  3.25it/s]
epoch 0 loss 0.696  acc 0.516: 100%|██████████| 410/410 [02:05<00:00,  3.50it/s]
epoch 0 loss 0.696  acc 0.516: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.684  acc 0.551: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.684  acc 0.551: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.684  acc 0.551: 100%|█████████▉| 409/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.684  acc 0.551: 100%|█████████▉| 409/410 [02:06<00:00,  3.26it/s]
epoch 1 loss 0.684  acc 0.551: 100%|██████████| 410/410 [02:06<00:00,  3.52it/s]
epoch 1 loss 0.684  acc 0.551: 100%|██████████| 410/410 [02:06<00:00,  3.25it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.673  acc 0.568: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 2 loss 0.672  acc 0.569: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 2 loss 0.672  acc 0.569: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 2 loss 0.672  acc 0.569: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 2 loss 0.672  acc 0.569: 100%|██████████| 410/410 [02:05<00:00,  3.53it/s]
epoch 2 loss 0.672  acc 0.569: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.66  acc 0.597: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 3 loss 0.66  acc 0.596: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 3 loss 0.66  acc 0.596: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 3 loss 0.66  acc 0.595: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 3 loss 0.66  acc 0.595: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 3 loss 0.66  acc 0.595: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.655  acc 0.601: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 4 loss 0.655  acc 0.601: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.655  acc 0.601: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.655  acc 0.601: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.655  acc 0.601: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 4 loss 0.655  acc 0.601: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]
INFO:name:Cycle 3, the best candidate in pop has score 0.595, and best in the run 0.595
INFO:name:Mean 0.5639000000000001 and standard deviation 0.013402362975734257 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 00:51:53,551 >> Trainable Ratio: 4064153/128119193=3.172166%
[INFO|(OpenDelta)basemodel:702]2024-04-20 00:51:53,552 >> Delta Parameter Ratio: 3472792/128119193=2.710595%
[INFO|(OpenDelta)basemodel:704]2024-04-20 00:51:53,552 >> Static Memory 1.43 GB, Max Memory 10.11 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.518: 100%|█████████▉| 408/410 [02:05<00:00,  3.23it/s]
epoch 0 loss 0.696  acc 0.517: 100%|█████████▉| 408/410 [02:05<00:00,  3.23it/s]
epoch 0 loss 0.696  acc 0.517: 100%|█████████▉| 409/410 [02:05<00:00,  3.23it/s]
epoch 0 loss 0.696  acc 0.517: 100%|█████████▉| 409/410 [02:05<00:00,  3.23it/s]
epoch 0 loss 0.696  acc 0.517: 100%|██████████| 410/410 [02:05<00:00,  3.50it/s]
epoch 0 loss 0.696  acc 0.517: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.688  acc 0.541: 100%|█████████▉| 408/410 [02:05<00:00,  3.25it/s]
epoch 1 loss 0.688  acc 0.541: 100%|█████████▉| 408/410 [02:05<00:00,  3.25it/s]
epoch 1 loss 0.688  acc 0.541: 100%|█████████▉| 409/410 [02:05<00:00,  3.25it/s]
epoch 1 loss 0.688  acc 0.542: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 1 loss 0.688  acc 0.542: 100%|██████████| 410/410 [02:06<00:00,  3.52it/s]
epoch 1 loss 0.688  acc 0.542: 100%|██████████| 410/410 [02:06<00:00,  3.25it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.673  acc 0.567: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 2 loss 0.673  acc 0.567: 100%|█████████▉| 408/410 [02:05<00:00,  3.28it/s]
epoch 2 loss 0.673  acc 0.567: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 2 loss 0.673  acc 0.567: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 2 loss 0.673  acc 0.567: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 2 loss 0.673  acc 0.567: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.663  acc 0.593: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 3 loss 0.663  acc 0.592: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 3 loss 0.663  acc 0.592: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 3 loss 0.663  acc 0.592: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 3 loss 0.663  acc 0.592: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 3 loss 0.663  acc 0.592: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.654  acc 0.604: 100%|█████████▉| 408/410 [02:04<00:00,  3.27it/s]
epoch 4 loss 0.654  acc 0.605: 100%|█████████▉| 408/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.654  acc 0.605: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.654  acc 0.605: 100%|█████████▉| 409/410 [02:05<00:00,  3.27it/s]
epoch 4 loss 0.654  acc 0.605: 100%|██████████| 410/410 [02:05<00:00,  3.54it/s]
epoch 4 loss 0.654  acc 0.605: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]
INFO:name:Cycle 4, the best candidate in pop has score 0.595, and best in the run 0.595
INFO:name:Mean 0.5642333333333335 and standard deviation 0.013792953112207502 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 01:03:16,265 >> Trainable Ratio: 3217905/127272945=2.528350%
[INFO|(OpenDelta)basemodel:702]2024-04-20 01:03:16,265 >> Delta Parameter Ratio: 2626544/127272945=2.063710%
[INFO|(OpenDelta)basemodel:704]2024-04-20 01:03:16,265 >> Static Memory 0.96 GB, Max Memory 10.11 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.697  acc 0.532: 100%|█████████▉| 408/410 [02:01<00:00,  3.32it/s]
epoch 0 loss 0.697  acc 0.531: 100%|█████████▉| 408/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.697  acc 0.531: 100%|█████████▉| 409/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.697  acc 0.531: 100%|█████████▉| 409/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.697  acc 0.531: 100%|██████████| 410/410 [02:02<00:00,  3.59it/s]
epoch 0 loss 0.697  acc 0.531: 100%|██████████| 410/410 [02:02<00:00,  3.35it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.69  acc 0.547: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.69  acc 0.546: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.69  acc 0.546: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.69  acc 0.546: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.69  acc 0.546: 100%|██████████| 410/410 [02:02<00:00,  3.61it/s]
epoch 1 loss 0.69  acc 0.546: 100%|██████████| 410/410 [02:03<00:00,  3.33it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.678  acc 0.564: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s]
epoch 2 loss 0.678  acc 0.564: 100%|█████████▉| 408/410 [02:02<00:00,  3.36it/s]
epoch 2 loss 0.678  acc 0.564: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 2 loss 0.678  acc 0.563: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 2 loss 0.678  acc 0.563: 100%|██████████| 410/410 [02:02<00:00,  3.63it/s]
epoch 2 loss 0.678  acc 0.563: 100%|██████████| 410/410 [02:02<00:00,  3.35it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.667  acc 0.585: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s]
epoch 3 loss 0.667  acc 0.585: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s]
epoch 3 loss 0.667  acc 0.585: 100%|█████████▉| 409/410 [02:01<00:00,  3.36it/s]
epoch 3 loss 0.667  acc 0.585: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 3 loss 0.667  acc 0.585: 100%|██████████| 410/410 [02:02<00:00,  3.64it/s]
epoch 3 loss 0.667  acc 0.585: 100%|██████████| 410/410 [02:02<00:00,  3.36it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.656  acc 0.594: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s]
epoch 4 loss 0.656  acc 0.594: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s]
epoch 4 loss 0.656  acc 0.594: 100%|█████████▉| 409/410 [02:01<00:00,  3.35it/s]
epoch 4 loss 0.656  acc 0.594: 100%|█████████▉| 409/410 [02:02<00:00,  3.35it/s]
epoch 4 loss 0.656  acc 0.594: 100%|██████████| 410/410 [02:02<00:00,  3.62it/s]
epoch 4 loss 0.656  acc 0.594: 100%|██████████| 410/410 [02:02<00:00,  3.36it/s]
INFO:name:Cycle 5, the best candidate in pop has score 0.587, and best in the run 0.595
INFO:name:Mean 0.5637666666666666 and standard deviation 0.012955522203120746 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 01:14:21,426 >> Trainable Ratio: 1999905/126054945=1.586534%
[INFO|(OpenDelta)basemodel:702]2024-04-20 01:14:21,426 >> Delta Parameter Ratio: 1408544/126054945=1.117405%
[INFO|(OpenDelta)basemodel:704]2024-04-20 01:14:21,426 >> Static Memory 1.44 GB, Max Memory 10.11 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.697  acc 0.523: 100%|█████████▉| 408/410 [01:55<00:00,  3.52it/s]
epoch 0 loss 0.697  acc 0.523: 100%|█████████▉| 408/410 [01:55<00:00,  3.52it/s]
epoch 0 loss 0.697  acc 0.523: 100%|█████████▉| 409/410 [01:55<00:00,  3.52it/s]
epoch 0 loss 0.697  acc 0.523: 100%|█████████▉| 409/410 [01:55<00:00,  3.52it/s]
epoch 0 loss 0.697  acc 0.523: 100%|██████████| 410/410 [01:55<00:00,  3.81it/s]
epoch 0 loss 0.697  acc 0.523: 100%|██████████| 410/410 [01:55<00:00,  3.55it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.69  acc 0.541: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s]
epoch 1 loss 0.69  acc 0.54: 100%|█████████▉| 408/410 [01:55<00:00,  3.54it/s] 
epoch 1 loss 0.69  acc 0.54: 100%|█████████▉| 409/410 [01:55<00:00,  3.54it/s]
epoch 1 loss 0.69  acc 0.541: 100%|█████████▉| 409/410 [01:56<00:00,  3.54it/s]
epoch 1 loss 0.69  acc 0.541: 100%|██████████| 410/410 [01:56<00:00,  3.83it/s]
epoch 1 loss 0.69  acc 0.541: 100%|██████████| 410/410 [01:56<00:00,  3.53it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.678  acc 0.565: 100%|█████████▉| 408/410 [01:55<00:00,  3.56it/s]
epoch 2 loss 0.678  acc 0.565: 100%|█████████▉| 408/410 [01:55<00:00,  3.56it/s]
epoch 2 loss 0.678  acc 0.565: 100%|█████████▉| 409/410 [01:55<00:00,  3.57it/s]
epoch 2 loss 0.678  acc 0.565: 100%|█████████▉| 409/410 [01:55<00:00,  3.57it/s]
epoch 2 loss 0.678  acc 0.565: 100%|██████████| 410/410 [01:55<00:00,  3.85it/s]
epoch 2 loss 0.678  acc 0.565: 100%|██████████| 410/410 [01:55<00:00,  3.55it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.663  acc 0.584: 100%|█████████▉| 408/410 [01:54<00:00,  3.58it/s]
epoch 3 loss 0.663  acc 0.585: 100%|█████████▉| 408/410 [01:54<00:00,  3.58it/s]
epoch 3 loss 0.663  acc 0.585: 100%|█████████▉| 409/410 [01:54<00:00,  3.58it/s]
epoch 3 loss 0.663  acc 0.585: 100%|█████████▉| 409/410 [01:55<00:00,  3.58it/s]
epoch 3 loss 0.663  acc 0.585: 100%|██████████| 410/410 [01:55<00:00,  3.87it/s]
epoch 3 loss 0.663  acc 0.585: 100%|██████████| 410/410 [01:55<00:00,  3.56it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.654  acc 0.6: 100%|█████████▉| 408/410 [01:54<00:00,  3.56it/s]
epoch 4 loss 0.654  acc 0.601: 100%|█████████▉| 408/410 [01:54<00:00,  3.56it/s]
epoch 4 loss 0.654  acc 0.601: 100%|█████████▉| 409/410 [01:54<00:00,  3.55it/s]
epoch 4 loss 0.653  acc 0.6: 100%|█████████▉| 409/410 [01:55<00:00,  3.55it/s]  
epoch 4 loss 0.653  acc 0.6: 100%|██████████| 410/410 [01:55<00:00,  3.85it/s]
epoch 4 loss 0.653  acc 0.6: 100%|██████████| 410/410 [01:55<00:00,  3.56it/s]
INFO:name:Cycle 6, the best candidate in pop has score 0.587, and best in the run 0.595
INFO:name:Mean 0.5642666666666668 and standard deviation 0.013051011029375767 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 01:24:50,802 >> Trainable Ratio: 3550321/127605361=2.782266%
[INFO|(OpenDelta)basemodel:702]2024-04-20 01:24:50,803 >> Delta Parameter Ratio: 2958960/127605361=2.318837%
[INFO|(OpenDelta)basemodel:704]2024-04-20 01:24:50,803 >> Static Memory 0.96 GB, Max Memory 10.11 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.697  acc 0.525: 100%|█████████▉| 408/410 [02:01<00:00,  3.32it/s]
epoch 0 loss 0.697  acc 0.525: 100%|█████████▉| 408/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.697  acc 0.525: 100%|█████████▉| 409/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.698  acc 0.525: 100%|█████████▉| 409/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.698  acc 0.525: 100%|██████████| 410/410 [02:02<00:00,  3.59it/s]
epoch 0 loss 0.698  acc 0.525: 100%|██████████| 410/410 [02:02<00:00,  3.35it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.694  acc 0.522: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.694  acc 0.522: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.694  acc 0.522: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.694  acc 0.522: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.694  acc 0.522: 100%|██████████| 410/410 [02:02<00:00,  3.61it/s]
epoch 1 loss 0.694  acc 0.522: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.694  acc 0.521: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s]
epoch 2 loss 0.694  acc 0.52: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s] 
epoch 2 loss 0.694  acc 0.52: 100%|█████████▉| 409/410 [02:01<00:00,  3.37it/s]
epoch 2 loss 0.694  acc 0.52: 100%|█████████▉| 409/410 [02:02<00:00,  3.37it/s]
epoch 2 loss 0.694  acc 0.52: 100%|██████████| 410/410 [02:02<00:00,  3.64it/s]
epoch 2 loss 0.694  acc 0.52: 100%|██████████| 410/410 [02:02<00:00,  3.36it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.682  acc 0.554: 100%|█████████▉| 408/410 [02:01<00:00,  3.37it/s]
epoch 3 loss 0.682  acc 0.554: 100%|█████████▉| 408/410 [02:01<00:00,  3.37it/s]
epoch 3 loss 0.682  acc 0.554: 100%|█████████▉| 409/410 [02:01<00:00,  3.37it/s]
epoch 3 loss 0.682  acc 0.555: 100%|█████████▉| 409/410 [02:01<00:00,  3.37it/s]
epoch 3 loss 0.682  acc 0.555: 100%|██████████| 410/410 [02:01<00:00,  3.64it/s]
epoch 3 loss 0.682  acc 0.555: 100%|██████████| 410/410 [02:01<00:00,  3.36it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.669  acc 0.581: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s]
epoch 4 loss 0.669  acc 0.581: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s]
epoch 4 loss 0.669  acc 0.581: 100%|█████████▉| 409/410 [02:01<00:00,  3.36it/s]
epoch 4 loss 0.669  acc 0.581: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 4 loss 0.669  acc 0.581: 100%|██████████| 410/410 [02:02<00:00,  3.63it/s]
epoch 4 loss 0.669  acc 0.581: 100%|██████████| 410/410 [02:02<00:00,  3.36it/s]
INFO:name:Cycle 7, the best candidate in pop has score 0.587, and best in the run 0.595
INFO:name:Mean 0.5643666666666667 and standard deviation 0.012936984020843287 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 01:35:55,809 >> Trainable Ratio: 4064153/128119193=3.172166%
[INFO|(OpenDelta)basemodel:702]2024-04-20 01:35:55,809 >> Delta Parameter Ratio: 3472792/128119193=2.710595%
[INFO|(OpenDelta)basemodel:704]2024-04-20 01:35:55,809 >> Static Memory 0.96 GB, Max Memory 10.11 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.697  acc 0.524: 100%|█████████▉| 408/410 [02:05<00:00,  3.24it/s]
epoch 0 loss 0.697  acc 0.524: 100%|█████████▉| 408/410 [02:05<00:00,  3.24it/s]
epoch 0 loss 0.697  acc 0.524: 100%|█████████▉| 409/410 [02:05<00:00,  3.24it/s]
epoch 0 loss 0.696  acc 0.525: 100%|█████████▉| 409/410 [02:05<00:00,  3.24it/s]
epoch 0 loss 0.696  acc 0.525: 100%|██████████| 410/410 [02:05<00:00,  3.50it/s]
epoch 0 loss 0.696  acc 0.525: 100%|██████████| 410/410 [02:05<00:00,  3.26it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.686  acc 0.557: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.686  acc 0.557: 100%|█████████▉| 408/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.686  acc 0.557: 100%|█████████▉| 409/410 [02:05<00:00,  3.26it/s]
epoch 1 loss 0.686  acc 0.557: 100%|█████████▉| 409/410 [02:06<00:00,  3.26it/s]
epoch 1 loss 0.686  acc 0.557: 100%|██████████| 410/410 [02:06<00:00,  3.52it/s]
epoch 1 loss 0.686  acc 0.557: 100%|██████████| 410/410 [02:06<00:00,  3.25it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.673  acc 0.569: 100%|█████████▉| 408/410 [02:04<00:00,  3.29it/s]
epoch 2 loss 0.672  acc 0.569: 100%|█████████▉| 408/410 [02:04<00:00,  3.29it/s]
epoch 2 loss 0.672  acc 0.569: 100%|█████████▉| 409/410 [02:04<00:00,  3.29it/s]
epoch 2 loss 0.672  acc 0.569: 100%|█████████▉| 409/410 [02:05<00:00,  3.29it/s]
epoch 2 loss 0.672  acc 0.569: 100%|██████████| 410/410 [02:05<00:00,  3.55it/s]
epoch 2 loss 0.672  acc 0.569: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.661  acc 0.596: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 3 loss 0.661  acc 0.596: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 3 loss 0.661  acc 0.596: 100%|█████████▉| 409/410 [02:04<00:00,  3.28it/s]
epoch 3 loss 0.661  acc 0.596: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 3 loss 0.661  acc 0.596: 100%|██████████| 410/410 [02:05<00:00,  3.55it/s]
epoch 3 loss 0.661  acc 0.596: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.65  acc 0.6: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 4 loss 0.65  acc 0.599: 100%|█████████▉| 408/410 [02:04<00:00,  3.28it/s]
epoch 4 loss 0.65  acc 0.599: 100%|█████████▉| 409/410 [02:04<00:00,  3.28it/s]
epoch 4 loss 0.65  acc 0.599: 100%|█████████▉| 409/410 [02:05<00:00,  3.28it/s]
epoch 4 loss 0.65  acc 0.599: 100%|██████████| 410/410 [02:05<00:00,  3.55it/s]
epoch 4 loss 0.65  acc 0.599: 100%|██████████| 410/410 [02:05<00:00,  3.27it/s]
INFO:name:Cycle 8, the best candidate in pop has score 0.587, and best in the run 0.595
INFO:name:Mean 0.5649 and standard deviation 0.013075039834228641 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 01:47:17,153 >> Trainable Ratio: 3298249/127353289=2.589842%
[INFO|(OpenDelta)basemodel:702]2024-04-20 01:47:17,153 >> Delta Parameter Ratio: 2706888/127353289=2.125495%
[INFO|(OpenDelta)basemodel:704]2024-04-20 01:47:17,153 >> Static Memory 1.44 GB, Max Memory 10.11 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.695  acc 0.526: 100%|█████████▉| 408/410 [02:02<00:00,  3.29it/s]
epoch 0 loss 0.695  acc 0.526: 100%|█████████▉| 408/410 [02:03<00:00,  3.29it/s]
epoch 0 loss 0.695  acc 0.526: 100%|█████████▉| 409/410 [02:03<00:00,  3.30it/s]
epoch 0 loss 0.695  acc 0.525: 100%|█████████▉| 409/410 [02:03<00:00,  3.30it/s]
epoch 0 loss 0.695  acc 0.525: 100%|██████████| 410/410 [02:03<00:00,  3.57it/s]
epoch 0 loss 0.695  acc 0.525: 100%|██████████| 410/410 [02:03<00:00,  3.32it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.688  acc 0.544: 100%|█████████▉| 408/410 [02:03<00:00,  3.32it/s]
epoch 1 loss 0.688  acc 0.545: 100%|█████████▉| 408/410 [02:03<00:00,  3.32it/s]
epoch 1 loss 0.688  acc 0.545: 100%|█████████▉| 409/410 [02:03<00:00,  3.32it/s]
epoch 1 loss 0.688  acc 0.545: 100%|█████████▉| 409/410 [02:03<00:00,  3.32it/s]
epoch 1 loss 0.688  acc 0.545: 100%|██████████| 410/410 [02:03<00:00,  3.59it/s]
epoch 1 loss 0.688  acc 0.545: 100%|██████████| 410/410 [02:03<00:00,  3.31it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.677  acc 0.567: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 2 loss 0.677  acc 0.566: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 2 loss 0.677  acc 0.566: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 2 loss 0.677  acc 0.566: 100%|█████████▉| 409/410 [02:03<00:00,  3.34it/s]
epoch 2 loss 0.677  acc 0.566: 100%|██████████| 410/410 [02:03<00:00,  3.61it/s]
epoch 2 loss 0.677  acc 0.566: 100%|██████████| 410/410 [02:03<00:00,  3.33it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.667  acc 0.583: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 3 loss 0.667  acc 0.583: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 3 loss 0.667  acc 0.583: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 3 loss 0.667  acc 0.582: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 3 loss 0.667  acc 0.582: 100%|██████████| 410/410 [02:02<00:00,  3.60it/s]
epoch 3 loss 0.667  acc 0.582: 100%|██████████| 410/410 [02:02<00:00,  3.33it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.657  acc 0.6: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 4 loss 0.656  acc 0.6: 100%|█████████▉| 408/410 [02:02<00:00,  3.33it/s]
epoch 4 loss 0.656  acc 0.6: 100%|█████████▉| 409/410 [02:02<00:00,  3.33it/s]
epoch 4 loss 0.657  acc 0.599: 100%|█████████▉| 409/410 [02:03<00:00,  3.33it/s]
epoch 4 loss 0.657  acc 0.599: 100%|██████████| 410/410 [02:03<00:00,  3.60it/s]
epoch 4 loss 0.657  acc 0.599: 100%|██████████| 410/410 [02:03<00:00,  3.33it/s]
INFO:name:Cycle 9, the best candidate in pop has score 0.587, and best in the run 0.595
INFO:name:Mean 0.5645666666666666 and standard deviation 0.012836622955002142 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 01:58:27,471 >> Trainable Ratio: 4176297/128231337=3.256846%
[INFO|(OpenDelta)basemodel:702]2024-04-20 01:58:27,471 >> Delta Parameter Ratio: 3584936/128231337=2.795679%
[INFO|(OpenDelta)basemodel:704]2024-04-20 01:58:27,472 >> Static Memory 0.96 GB, Max Memory 10.11 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.699  acc 0.522: 100%|█████████▉| 408/410 [02:11<00:00,  3.10it/s]
epoch 0 loss 0.699  acc 0.523: 100%|█████████▉| 408/410 [02:11<00:00,  3.10it/s]
epoch 0 loss 0.699  acc 0.523: 100%|█████████▉| 409/410 [02:11<00:00,  3.10it/s]
epoch 0 loss 0.699  acc 0.523: 100%|█████████▉| 409/410 [02:11<00:00,  3.10it/s]
epoch 0 loss 0.699  acc 0.523: 100%|██████████| 410/410 [02:11<00:00,  3.35it/s]
epoch 0 loss 0.699  acc 0.523: 100%|██████████| 410/410 [02:11<00:00,  3.11it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.689  acc 0.547: 100%|█████████▉| 408/410 [02:11<00:00,  3.12it/s]
epoch 1 loss 0.69  acc 0.547: 100%|█████████▉| 408/410 [02:11<00:00,  3.12it/s] 
epoch 1 loss 0.69  acc 0.547: 100%|█████████▉| 409/410 [02:11<00:00,  3.11it/s]
epoch 1 loss 0.69  acc 0.546: 100%|█████████▉| 409/410 [02:11<00:00,  3.11it/s]
epoch 1 loss 0.69  acc 0.546: 100%|██████████| 410/410 [02:11<00:00,  3.37it/s]
epoch 1 loss 0.69  acc 0.546: 100%|██████████| 410/410 [02:11<00:00,  3.11it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.672  acc 0.576: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 2 loss 0.672  acc 0.576: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 2 loss 0.672  acc 0.576: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 2 loss 0.672  acc 0.576: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 2 loss 0.672  acc 0.576: 100%|██████████| 410/410 [02:10<00:00,  3.39it/s]
epoch 2 loss 0.672  acc 0.576: 100%|██████████| 410/410 [02:11<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.655  acc 0.6: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 3 loss 0.655  acc 0.6: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 3 loss 0.655  acc 0.6: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.655  acc 0.6: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 3 loss 0.655  acc 0.6: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 3 loss 0.655  acc 0.6: 100%|██████████| 410/410 [02:11<00:00,  3.13it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.646  acc 0.602: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.646  acc 0.602: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.646  acc 0.602: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.646  acc 0.602: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.646  acc 0.602: 100%|██████████| 410/410 [02:10<00:00,  3.39it/s]
epoch 4 loss 0.646  acc 0.602: 100%|██████████| 410/410 [02:10<00:00,  3.13it/s]
INFO:name:Cycle 10, the best candidate in pop has score 0.587, and best in the run 0.595
INFO:name:Mean 0.5653666666666666 and standard deviation 0.01344738718942164 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 02:10:18,464 >> Trainable Ratio: 4451673/128506713=3.464156%
[INFO|(OpenDelta)basemodel:702]2024-04-20 02:10:18,464 >> Delta Parameter Ratio: 3860312/128506713=3.003977%
[INFO|(OpenDelta)basemodel:704]2024-04-20 02:10:18,464 >> Static Memory 1.44 GB, Max Memory 10.22 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.698  acc 0.523: 100%|█████████▉| 408/410 [02:11<00:00,  3.09it/s]
epoch 0 loss 0.698  acc 0.523: 100%|█████████▉| 408/410 [02:11<00:00,  3.09it/s]
epoch 0 loss 0.698  acc 0.523: 100%|█████████▉| 409/410 [02:11<00:00,  3.09it/s]
epoch 0 loss 0.698  acc 0.523: 100%|█████████▉| 409/410 [02:12<00:00,  3.09it/s]
epoch 0 loss 0.698  acc 0.523: 100%|██████████| 410/410 [02:12<00:00,  3.34it/s]
epoch 0 loss 0.698  acc 0.523: 100%|██████████| 410/410 [02:12<00:00,  3.10it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.685  acc 0.557: 100%|█████████▉| 408/410 [02:11<00:00,  3.10it/s]
epoch 1 loss 0.685  acc 0.557: 100%|█████████▉| 408/410 [02:12<00:00,  3.10it/s]
epoch 1 loss 0.685  acc 0.557: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 1 loss 0.685  acc 0.557: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 1 loss 0.685  acc 0.557: 100%|██████████| 410/410 [02:12<00:00,  3.35it/s]
epoch 1 loss 0.685  acc 0.557: 100%|██████████| 410/410 [02:12<00:00,  3.09it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.674  acc 0.562: 100%|█████████▉| 408/410 [02:11<00:00,  3.13it/s]
epoch 2 loss 0.674  acc 0.562: 100%|█████████▉| 408/410 [02:11<00:00,  3.13it/s]
epoch 2 loss 0.674  acc 0.562: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 2 loss 0.674  acc 0.562: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 2 loss 0.674  acc 0.562: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 2 loss 0.674  acc 0.562: 100%|██████████| 410/410 [02:11<00:00,  3.11it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.665  acc 0.576: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 3 loss 0.665  acc 0.576: 100%|█████████▉| 408/410 [02:11<00:00,  3.12it/s]
epoch 3 loss 0.665  acc 0.576: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 3 loss 0.665  acc 0.576: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 3 loss 0.665  acc 0.576: 100%|██████████| 410/410 [02:11<00:00,  3.37it/s]
epoch 3 loss 0.665  acc 0.576: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.655  acc 0.594: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 4 loss 0.655  acc 0.593: 100%|█████████▉| 408/410 [02:11<00:00,  3.12it/s]
epoch 4 loss 0.655  acc 0.593: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 4 loss 0.654  acc 0.594: 100%|█████████▉| 409/410 [02:11<00:00,  3.12it/s]
epoch 4 loss 0.654  acc 0.594: 100%|██████████| 410/410 [02:11<00:00,  3.37it/s]
epoch 4 loss 0.654  acc 0.594: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]
INFO:name:Cycle 11, the best candidate in pop has score 0.595, and best in the run 0.595
INFO:name:Mean 0.5669666666666665 and standard deviation 0.014010670536733348 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 02:22:12,583 >> Trainable Ratio: 3910001/127965041=3.055523%
[INFO|(OpenDelta)basemodel:702]2024-04-20 02:22:12,583 >> Delta Parameter Ratio: 3318640/127965041=2.593396%
[INFO|(OpenDelta)basemodel:704]2024-04-20 02:22:12,583 >> Static Memory 0.96 GB, Max Memory 10.22 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.695  acc 0.517: 100%|█████████▉| 408/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.695  acc 0.517: 100%|█████████▉| 408/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.695  acc 0.517: 100%|█████████▉| 409/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.695  acc 0.518: 100%|█████████▉| 409/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.695  acc 0.518: 100%|██████████| 410/410 [02:06<00:00,  3.47it/s]
epoch 0 loss 0.695  acc 0.518: 100%|██████████| 410/410 [02:06<00:00,  3.23it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.687  acc 0.557: 100%|█████████▉| 408/410 [02:06<00:00,  3.22it/s]
epoch 1 loss 0.687  acc 0.556: 100%|█████████▉| 408/410 [02:06<00:00,  3.22it/s]
epoch 1 loss 0.687  acc 0.556: 100%|█████████▉| 409/410 [02:06<00:00,  3.23it/s]
epoch 1 loss 0.686  acc 0.556: 100%|█████████▉| 409/410 [02:07<00:00,  3.23it/s]
epoch 1 loss 0.686  acc 0.556: 100%|██████████| 410/410 [02:07<00:00,  3.49it/s]
epoch 1 loss 0.686  acc 0.556: 100%|██████████| 410/410 [02:07<00:00,  3.22it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.669  acc 0.573: 100%|█████████▉| 408/410 [02:05<00:00,  3.25it/s]
epoch 2 loss 0.669  acc 0.574: 100%|█████████▉| 408/410 [02:06<00:00,  3.25it/s]
epoch 2 loss 0.669  acc 0.574: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 2 loss 0.67  acc 0.573: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s] 
epoch 2 loss 0.67  acc 0.573: 100%|██████████| 410/410 [02:06<00:00,  3.51it/s]
epoch 2 loss 0.67  acc 0.573: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.654  acc 0.604: 100%|█████████▉| 408/410 [02:05<00:00,  3.24it/s]
epoch 3 loss 0.654  acc 0.604: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 3 loss 0.654  acc 0.604: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 3 loss 0.654  acc 0.605: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 3 loss 0.654  acc 0.605: 100%|██████████| 410/410 [02:06<00:00,  3.51it/s]
epoch 3 loss 0.654  acc 0.605: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.645  acc 0.612: 100%|█████████▉| 408/410 [02:05<00:00,  3.24it/s]
epoch 4 loss 0.645  acc 0.612: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 4 loss 0.645  acc 0.612: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 4 loss 0.645  acc 0.612: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 4 loss 0.645  acc 0.612: 100%|██████████| 410/410 [02:06<00:00,  3.50it/s]
epoch 4 loss 0.645  acc 0.612: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]
INFO:name:Cycle 12, the best candidate in pop has score 0.6, and best in the run 0.6
INFO:name:Mean 0.5686 and standard deviation 0.014883100035498847 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 02:33:38,939 >> Trainable Ratio: 2940721/126995761=2.315606%
[INFO|(OpenDelta)basemodel:702]2024-04-20 02:33:38,939 >> Delta Parameter Ratio: 2349360/126995761=1.849952%
[INFO|(OpenDelta)basemodel:704]2024-04-20 02:33:38,939 >> Static Memory 1.44 GB, Max Memory 10.22 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.528: 100%|█████████▉| 408/410 [02:03<00:00,  3.28it/s]
epoch 0 loss 0.695  acc 0.529: 100%|█████████▉| 408/410 [02:03<00:00,  3.28it/s]
epoch 0 loss 0.695  acc 0.529: 100%|█████████▉| 409/410 [02:03<00:00,  3.29it/s]
epoch 0 loss 0.695  acc 0.529: 100%|█████████▉| 409/410 [02:03<00:00,  3.29it/s]
epoch 0 loss 0.695  acc 0.529: 100%|██████████| 410/410 [02:03<00:00,  3.56it/s]
epoch 0 loss 0.695  acc 0.529: 100%|██████████| 410/410 [02:03<00:00,  3.31it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.689  acc 0.551: 100%|█████████▉| 408/410 [02:03<00:00,  3.31it/s]
epoch 1 loss 0.689  acc 0.552: 100%|█████████▉| 408/410 [02:04<00:00,  3.31it/s]
epoch 1 loss 0.689  acc 0.552: 100%|█████████▉| 409/410 [02:04<00:00,  3.31it/s]
epoch 1 loss 0.689  acc 0.552: 100%|█████████▉| 409/410 [02:04<00:00,  3.31it/s]
epoch 1 loss 0.689  acc 0.552: 100%|██████████| 410/410 [02:04<00:00,  3.58it/s]
epoch 1 loss 0.689  acc 0.552: 100%|██████████| 410/410 [02:04<00:00,  3.30it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.679  acc 0.569: 100%|█████████▉| 408/410 [02:03<00:00,  3.33it/s]
epoch 2 loss 0.679  acc 0.57: 100%|█████████▉| 408/410 [02:03<00:00,  3.33it/s] 
epoch 2 loss 0.679  acc 0.57: 100%|█████████▉| 409/410 [02:03<00:00,  3.33it/s]
epoch 2 loss 0.679  acc 0.57: 100%|█████████▉| 409/410 [02:03<00:00,  3.33it/s]
epoch 2 loss 0.679  acc 0.57: 100%|██████████| 410/410 [02:03<00:00,  3.60it/s]
epoch 2 loss 0.679  acc 0.57: 100%|██████████| 410/410 [02:03<00:00,  3.32it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.665  acc 0.573: 100%|█████████▉| 408/410 [02:02<00:00,  3.31it/s]
epoch 3 loss 0.665  acc 0.573: 100%|█████████▉| 408/410 [02:03<00:00,  3.31it/s]
epoch 3 loss 0.665  acc 0.573: 100%|█████████▉| 409/410 [02:03<00:00,  3.32it/s]
epoch 3 loss 0.665  acc 0.573: 100%|█████████▉| 409/410 [02:03<00:00,  3.32it/s]
epoch 3 loss 0.665  acc 0.573: 100%|██████████| 410/410 [02:03<00:00,  3.59it/s]
epoch 3 loss 0.665  acc 0.573: 100%|██████████| 410/410 [02:03<00:00,  3.32it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.652  acc 0.603: 100%|█████████▉| 408/410 [02:02<00:00,  3.32it/s]
epoch 4 loss 0.652  acc 0.603: 100%|█████████▉| 408/410 [02:03<00:00,  3.32it/s]
epoch 4 loss 0.652  acc 0.603: 100%|█████████▉| 409/410 [02:03<00:00,  3.32it/s]
epoch 4 loss 0.652  acc 0.603: 100%|█████████▉| 409/410 [02:03<00:00,  3.32it/s]
epoch 4 loss 0.652  acc 0.603: 100%|██████████| 410/410 [02:03<00:00,  3.59it/s]
epoch 4 loss 0.652  acc 0.603: 100%|██████████| 410/410 [02:03<00:00,  3.32it/s]
INFO:name:Cycle 13, the best candidate in pop has score 0.6, and best in the run 0.6
INFO:name:Mean 0.5690333333333334 and standard deviation 0.01507643488656678 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 02:44:50,206 >> Trainable Ratio: 3403865/127458905=2.670559%
[INFO|(OpenDelta)basemodel:702]2024-04-20 02:44:50,206 >> Delta Parameter Ratio: 2812504/127458905=2.206597%
[INFO|(OpenDelta)basemodel:704]2024-04-20 02:44:50,206 >> Static Memory 0.96 GB, Max Memory 10.22 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.694  acc 0.531: 100%|█████████▉| 408/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.694  acc 0.53: 100%|█████████▉| 408/410 [02:02<00:00,  3.32it/s] 
epoch 0 loss 0.694  acc 0.53: 100%|█████████▉| 409/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.694  acc 0.53: 100%|█████████▉| 409/410 [02:02<00:00,  3.32it/s]
epoch 0 loss 0.694  acc 0.53: 100%|██████████| 410/410 [02:02<00:00,  3.59it/s]
epoch 0 loss 0.694  acc 0.53: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.688  acc 0.54: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.689  acc 0.539: 100%|█████████▉| 408/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.689  acc 0.539: 100%|█████████▉| 409/410 [02:02<00:00,  3.34it/s]
epoch 1 loss 0.689  acc 0.539: 100%|█████████▉| 409/410 [02:03<00:00,  3.34it/s]
epoch 1 loss 0.689  acc 0.539: 100%|██████████| 410/410 [02:03<00:00,  3.61it/s]
epoch 1 loss 0.689  acc 0.539: 100%|██████████| 410/410 [02:03<00:00,  3.33it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.67  acc 0.58: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 2 loss 0.67  acc 0.579: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 2 loss 0.67  acc 0.579: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 2 loss 0.67  acc 0.579: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 2 loss 0.67  acc 0.579: 100%|██████████| 410/410 [02:02<00:00,  3.63it/s]
epoch 2 loss 0.67  acc 0.579: 100%|██████████| 410/410 [02:02<00:00,  3.34it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.663  acc 0.585: 100%|█████████▉| 408/410 [02:01<00:00,  3.35it/s]
epoch 3 loss 0.663  acc 0.585: 100%|█████████▉| 408/410 [02:02<00:00,  3.35it/s]
epoch 3 loss 0.663  acc 0.585: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 3 loss 0.663  acc 0.586: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 3 loss 0.663  acc 0.586: 100%|██████████| 410/410 [02:02<00:00,  3.63it/s]
epoch 3 loss 0.663  acc 0.586: 100%|██████████| 410/410 [02:02<00:00,  3.35it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.654  acc 0.594: 100%|█████████▉| 408/410 [02:01<00:00,  3.36it/s]
epoch 4 loss 0.654  acc 0.594: 100%|█████████▉| 408/410 [02:02<00:00,  3.36it/s]
epoch 4 loss 0.654  acc 0.594: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 4 loss 0.654  acc 0.593: 100%|█████████▉| 409/410 [02:02<00:00,  3.36it/s]
epoch 4 loss 0.654  acc 0.593: 100%|██████████| 410/410 [02:02<00:00,  3.63it/s]
epoch 4 loss 0.654  acc 0.593: 100%|██████████| 410/410 [02:02<00:00,  3.35it/s]
INFO:name:Cycle 14, the best candidate in pop has score 0.6, and best in the run 0.6
INFO:name:Mean 0.5700666666666667 and standard deviation 0.014865919263723807 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 02:55:56,787 >> Trainable Ratio: 3910001/127965041=3.055523%
[INFO|(OpenDelta)basemodel:702]2024-04-20 02:55:56,787 >> Delta Parameter Ratio: 3318640/127965041=2.593396%
[INFO|(OpenDelta)basemodel:704]2024-04-20 02:55:56,787 >> Static Memory 1.43 GB, Max Memory 10.22 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.528: 100%|█████████▉| 408/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.696  acc 0.528: 100%|█████████▉| 408/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.696  acc 0.528: 100%|█████████▉| 409/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.696  acc 0.528: 100%|█████████▉| 409/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.696  acc 0.528: 100%|██████████| 410/410 [02:06<00:00,  3.47it/s]
epoch 0 loss 0.696  acc 0.528: 100%|██████████| 410/410 [02:06<00:00,  3.23it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.687  acc 0.55: 100%|█████████▉| 408/410 [02:06<00:00,  3.23it/s]
epoch 1 loss 0.687  acc 0.55: 100%|█████████▉| 408/410 [02:07<00:00,  3.23it/s]
epoch 1 loss 0.687  acc 0.55: 100%|█████████▉| 409/410 [02:07<00:00,  3.23it/s]
epoch 1 loss 0.687  acc 0.549: 100%|█████████▉| 409/410 [02:07<00:00,  3.23it/s]
epoch 1 loss 0.687  acc 0.549: 100%|██████████| 410/410 [02:07<00:00,  3.49it/s]
epoch 1 loss 0.687  acc 0.549: 100%|██████████| 410/410 [02:07<00:00,  3.22it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.682  acc 0.566: 100%|█████████▉| 408/410 [02:05<00:00,  3.25it/s]
epoch 2 loss 0.682  acc 0.566: 100%|█████████▉| 408/410 [02:06<00:00,  3.25it/s]
epoch 2 loss 0.682  acc 0.566: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 2 loss 0.682  acc 0.567: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 2 loss 0.682  acc 0.567: 100%|██████████| 410/410 [02:06<00:00,  3.52it/s]
epoch 2 loss 0.682  acc 0.567: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.666  acc 0.583: 100%|█████████▉| 408/410 [02:05<00:00,  3.25it/s]
epoch 3 loss 0.666  acc 0.583: 100%|█████████▉| 408/410 [02:06<00:00,  3.25it/s]
epoch 3 loss 0.666  acc 0.583: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 3 loss 0.666  acc 0.583: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 3 loss 0.666  acc 0.583: 100%|██████████| 410/410 [02:06<00:00,  3.51it/s]
epoch 3 loss 0.666  acc 0.583: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.654  acc 0.604: 100%|█████████▉| 408/410 [02:05<00:00,  3.24it/s]
epoch 4 loss 0.654  acc 0.605: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 4 loss 0.654  acc 0.605: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 4 loss 0.654  acc 0.604: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 4 loss 0.654  acc 0.604: 100%|██████████| 410/410 [02:06<00:00,  3.51it/s]
epoch 4 loss 0.654  acc 0.604: 100%|██████████| 410/410 [02:06<00:00,  3.25it/s]
INFO:name:Cycle 15, the best candidate in pop has score 0.6, and best in the run 0.6
INFO:name:Mean 0.5698666666666666 and standard deviation 0.014784526445653283 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 03:07:23,456 >> Trainable Ratio: 3910001/127965041=3.055523%
[INFO|(OpenDelta)basemodel:702]2024-04-20 03:07:23,456 >> Delta Parameter Ratio: 3318640/127965041=2.593396%
[INFO|(OpenDelta)basemodel:704]2024-04-20 03:07:23,456 >> Static Memory 0.96 GB, Max Memory 10.22 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.698  acc 0.516: 100%|█████████▉| 408/410 [02:06<00:00,  3.20it/s]
epoch 0 loss 0.698  acc 0.515: 100%|█████████▉| 408/410 [02:06<00:00,  3.20it/s]
epoch 0 loss 0.698  acc 0.515: 100%|█████████▉| 409/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.698  acc 0.515: 100%|█████████▉| 409/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.698  acc 0.515: 100%|██████████| 410/410 [02:06<00:00,  3.47it/s]
epoch 0 loss 0.698  acc 0.515: 100%|██████████| 410/410 [02:06<00:00,  3.23it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.689  acc 0.547: 100%|█████████▉| 408/410 [02:06<00:00,  3.23it/s]
epoch 1 loss 0.689  acc 0.546: 100%|█████████▉| 408/410 [02:07<00:00,  3.23it/s]
epoch 1 loss 0.689  acc 0.546: 100%|█████████▉| 409/410 [02:07<00:00,  3.23it/s]
epoch 1 loss 0.689  acc 0.547: 100%|█████████▉| 409/410 [02:07<00:00,  3.23it/s]
epoch 1 loss 0.689  acc 0.547: 100%|██████████| 410/410 [02:07<00:00,  3.49it/s]
epoch 1 loss 0.689  acc 0.547: 100%|██████████| 410/410 [02:07<00:00,  3.22it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.679  acc 0.564: 100%|█████████▉| 408/410 [02:06<00:00,  3.25it/s]
epoch 2 loss 0.679  acc 0.564: 100%|█████████▉| 408/410 [02:06<00:00,  3.25it/s]
epoch 2 loss 0.679  acc 0.564: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 2 loss 0.68  acc 0.564: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s] 
epoch 2 loss 0.68  acc 0.564: 100%|██████████| 410/410 [02:06<00:00,  3.51it/s]
epoch 2 loss 0.68  acc 0.564: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.663  acc 0.587: 100%|█████████▉| 408/410 [02:05<00:00,  3.24it/s]
epoch 3 loss 0.663  acc 0.587: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 3 loss 0.663  acc 0.587: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 3 loss 0.663  acc 0.587: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 3 loss 0.663  acc 0.587: 100%|██████████| 410/410 [02:06<00:00,  3.51it/s]
epoch 3 loss 0.663  acc 0.587: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.651  acc 0.605: 100%|█████████▉| 408/410 [02:05<00:00,  3.25it/s]
epoch 4 loss 0.651  acc 0.605: 100%|█████████▉| 408/410 [02:06<00:00,  3.25it/s]
epoch 4 loss 0.651  acc 0.605: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 4 loss 0.651  acc 0.605: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 4 loss 0.651  acc 0.605: 100%|██████████| 410/410 [02:06<00:00,  3.51it/s]
epoch 4 loss 0.651  acc 0.605: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]
INFO:name:Cycle 16, the best candidate in pop has score 0.6, and best in the run 0.6
INFO:name:Mean 0.5707666666666668 and standard deviation 0.014916843574369926 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 03:18:50,398 >> Trainable Ratio: 5140825/129195865=3.979094%
[INFO|(OpenDelta)basemodel:702]2024-04-20 03:18:50,398 >> Delta Parameter Ratio: 4549464/129195865=3.521370%
[INFO|(OpenDelta)basemodel:704]2024-04-20 03:18:50,398 >> Static Memory 1.44 GB, Max Memory 10.22 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 408/410 [02:12<00:00,  3.05it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 408/410 [02:12<00:00,  3.05it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 409/410 [02:12<00:00,  3.06it/s]
epoch 0 loss 0.696  acc 0.527: 100%|█████████▉| 409/410 [02:13<00:00,  3.06it/s]
epoch 0 loss 0.696  acc 0.527: 100%|██████████| 410/410 [02:13<00:00,  3.31it/s]
epoch 0 loss 0.696  acc 0.527: 100%|██████████| 410/410 [02:13<00:00,  3.08it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.686  acc 0.544: 100%|█████████▉| 408/410 [02:12<00:00,  3.09it/s]
epoch 1 loss 0.686  acc 0.543: 100%|█████████▉| 408/410 [02:13<00:00,  3.09it/s]
epoch 1 loss 0.686  acc 0.543: 100%|█████████▉| 409/410 [02:13<00:00,  3.08it/s]
epoch 1 loss 0.686  acc 0.543: 100%|█████████▉| 409/410 [02:13<00:00,  3.08it/s]
epoch 1 loss 0.686  acc 0.543: 100%|██████████| 410/410 [02:13<00:00,  3.33it/s]
epoch 1 loss 0.686  acc 0.543: 100%|██████████| 410/410 [02:13<00:00,  3.07it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.675  acc 0.57: 100%|█████████▉| 408/410 [02:12<00:00,  3.09it/s]
epoch 2 loss 0.675  acc 0.571: 100%|█████████▉| 408/410 [02:12<00:00,  3.09it/s]
epoch 2 loss 0.675  acc 0.571: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 2 loss 0.674  acc 0.571: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 2 loss 0.674  acc 0.571: 100%|██████████| 410/410 [02:12<00:00,  3.35it/s]
epoch 2 loss 0.674  acc 0.571: 100%|██████████| 410/410 [02:12<00:00,  3.09it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.659  acc 0.589: 100%|█████████▉| 408/410 [02:11<00:00,  3.10it/s]
epoch 3 loss 0.659  acc 0.589: 100%|█████████▉| 408/410 [02:12<00:00,  3.10it/s]
epoch 3 loss 0.659  acc 0.589: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 3 loss 0.659  acc 0.588: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 3 loss 0.659  acc 0.588: 100%|██████████| 410/410 [02:12<00:00,  3.35it/s]
epoch 3 loss 0.659  acc 0.588: 100%|██████████| 410/410 [02:12<00:00,  3.10it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.648  acc 0.605: 100%|█████████▉| 408/410 [02:12<00:00,  3.09it/s]
epoch 4 loss 0.648  acc 0.606: 100%|█████████▉| 408/410 [02:12<00:00,  3.09it/s]
epoch 4 loss 0.648  acc 0.606: 100%|█████████▉| 409/410 [02:12<00:00,  3.09it/s]
epoch 4 loss 0.648  acc 0.606: 100%|█████████▉| 409/410 [02:12<00:00,  3.09it/s]
epoch 4 loss 0.648  acc 0.606: 100%|██████████| 410/410 [02:12<00:00,  3.35it/s]
epoch 4 loss 0.648  acc 0.606: 100%|██████████| 410/410 [02:12<00:00,  3.09it/s]
INFO:name:Cycle 17, the best candidate in pop has score 0.6, and best in the run 0.6
INFO:name:Mean 0.5717333333333333 and standard deviation 0.014539448254853232 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 03:30:49,746 >> Trainable Ratio: 5140825/129195865=3.979094%
[INFO|(OpenDelta)basemodel:702]2024-04-20 03:30:49,746 >> Delta Parameter Ratio: 4549464/129195865=3.521370%
[INFO|(OpenDelta)basemodel:704]2024-04-20 03:30:49,746 >> Static Memory 0.97 GB, Max Memory 10.22 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.693  acc 0.532: 100%|█████████▉| 408/410 [02:12<00:00,  3.07it/s]
epoch 0 loss 0.692  acc 0.533: 100%|█████████▉| 408/410 [02:12<00:00,  3.07it/s]
epoch 0 loss 0.692  acc 0.533: 100%|█████████▉| 409/410 [02:12<00:00,  3.07it/s]
epoch 0 loss 0.692  acc 0.534: 100%|█████████▉| 409/410 [02:12<00:00,  3.07it/s]
epoch 0 loss 0.692  acc 0.534: 100%|██████████| 410/410 [02:12<00:00,  3.31it/s]
epoch 0 loss 0.692  acc 0.534: 100%|██████████| 410/410 [02:13<00:00,  3.08it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.688  acc 0.542: 100%|█████████▉| 408/410 [02:12<00:00,  3.08it/s]
epoch 1 loss 0.688  acc 0.541: 100%|█████████▉| 408/410 [02:13<00:00,  3.08it/s]
epoch 1 loss 0.688  acc 0.541: 100%|█████████▉| 409/410 [02:13<00:00,  3.08it/s]
epoch 1 loss 0.688  acc 0.541: 100%|█████████▉| 409/410 [02:13<00:00,  3.08it/s]
epoch 1 loss 0.688  acc 0.541: 100%|██████████| 410/410 [02:13<00:00,  3.33it/s]
epoch 1 loss 0.688  acc 0.541: 100%|██████████| 410/410 [02:13<00:00,  3.07it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.675  acc 0.571: 100%|█████████▉| 408/410 [02:12<00:00,  3.09it/s]
epoch 2 loss 0.676  acc 0.571: 100%|█████████▉| 408/410 [02:12<00:00,  3.09it/s]
epoch 2 loss 0.676  acc 0.571: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 2 loss 0.675  acc 0.571: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 2 loss 0.675  acc 0.571: 100%|██████████| 410/410 [02:12<00:00,  3.35it/s]
epoch 2 loss 0.675  acc 0.571: 100%|██████████| 410/410 [02:12<00:00,  3.09it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.661  acc 0.59: 100%|█████████▉| 408/410 [02:11<00:00,  3.10it/s]
epoch 3 loss 0.661  acc 0.589: 100%|█████████▉| 408/410 [02:12<00:00,  3.10it/s]
epoch 3 loss 0.661  acc 0.589: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 3 loss 0.661  acc 0.589: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 3 loss 0.661  acc 0.589: 100%|██████████| 410/410 [02:12<00:00,  3.34it/s]
epoch 3 loss 0.661  acc 0.589: 100%|██████████| 410/410 [02:12<00:00,  3.09it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.65  acc 0.605: 100%|█████████▉| 408/410 [02:11<00:00,  3.11it/s]
epoch 4 loss 0.65  acc 0.605: 100%|█████████▉| 408/410 [02:11<00:00,  3.11it/s]
epoch 4 loss 0.65  acc 0.605: 100%|█████████▉| 409/410 [02:11<00:00,  3.11it/s]
epoch 4 loss 0.65  acc 0.605: 100%|█████████▉| 409/410 [02:12<00:00,  3.11it/s]
epoch 4 loss 0.65  acc 0.605: 100%|██████████| 410/410 [02:12<00:00,  3.36it/s]
epoch 4 loss 0.65  acc 0.605: 100%|██████████| 410/410 [02:12<00:00,  3.10it/s]
INFO:name:Cycle 18, the best candidate in pop has score 0.6, and best in the run 0.6
INFO:name:Mean 0.5725333333333332 and standard deviation 0.014750668534755344 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 03:42:48,164 >> Trainable Ratio: 4865793/128920833=3.774249%
[INFO|(OpenDelta)basemodel:702]2024-04-20 03:42:48,164 >> Delta Parameter Ratio: 4274432/128920833=3.315548%
[INFO|(OpenDelta)basemodel:704]2024-04-20 03:42:48,164 >> Static Memory 0.97 GB, Max Memory 10.31 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.694  acc 0.528: 100%|█████████▉| 408/410 [02:11<00:00,  3.09it/s]
epoch 0 loss 0.694  acc 0.529: 100%|█████████▉| 408/410 [02:11<00:00,  3.09it/s]
epoch 0 loss 0.694  acc 0.529: 100%|█████████▉| 409/410 [02:11<00:00,  3.10it/s]
epoch 0 loss 0.694  acc 0.53: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s] 
epoch 0 loss 0.694  acc 0.53: 100%|██████████| 410/410 [02:12<00:00,  3.35it/s]
epoch 0 loss 0.694  acc 0.53: 100%|██████████| 410/410 [02:12<00:00,  3.10it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.686  acc 0.552: 100%|█████████▉| 408/410 [02:11<00:00,  3.10it/s]
epoch 1 loss 0.686  acc 0.552: 100%|█████████▉| 408/410 [02:12<00:00,  3.10it/s]
epoch 1 loss 0.686  acc 0.552: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 1 loss 0.685  acc 0.552: 100%|█████████▉| 409/410 [02:12<00:00,  3.10it/s]
epoch 1 loss 0.685  acc 0.552: 100%|██████████| 410/410 [02:12<00:00,  3.36it/s]
epoch 1 loss 0.685  acc 0.552: 100%|██████████| 410/410 [02:12<00:00,  3.10it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.671  acc 0.572: 100%|█████████▉| 408/410 [02:11<00:00,  3.12it/s]
epoch 2 loss 0.671  acc 0.572: 100%|█████████▉| 408/410 [02:11<00:00,  3.12it/s]
epoch 2 loss 0.671  acc 0.572: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 2 loss 0.671  acc 0.572: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 2 loss 0.671  acc 0.572: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 2 loss 0.671  acc 0.572: 100%|██████████| 410/410 [02:11<00:00,  3.11it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.658  acc 0.594: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 3 loss 0.658  acc 0.593: 100%|█████████▉| 408/410 [02:10<00:00,  3.12it/s]
epoch 3 loss 0.658  acc 0.593: 100%|█████████▉| 409/410 [02:10<00:00,  3.13it/s]
epoch 3 loss 0.658  acc 0.593: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 3 loss 0.658  acc 0.593: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 3 loss 0.658  acc 0.593: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.644  acc 0.611: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.644  acc 0.612: 100%|█████████▉| 408/410 [02:10<00:00,  3.13it/s]
epoch 4 loss 0.644  acc 0.612: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 4 loss 0.644  acc 0.612: 100%|█████████▉| 409/410 [02:11<00:00,  3.13it/s]
epoch 4 loss 0.644  acc 0.612: 100%|██████████| 410/410 [02:11<00:00,  3.38it/s]
epoch 4 loss 0.644  acc 0.612: 100%|██████████| 410/410 [02:11<00:00,  3.12it/s]
INFO:name:Cycle 19, the best candidate in pop has score 0.6, and best in the run 0.6
INFO:name:Mean 0.5735 and standard deviation 0.014384598244882125 of score in the population
loading weights file pytorch_model.bin from cache at /home/aakli/.cache/huggingface/hub/models--microsoft--codebert-base/snapshots/3b0952feddeffad0063f274080e3c23d75e7eb39/pytorch_model.bin
Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|(OpenDelta)basemodel:700]2024-04-20 03:54:40,819 >> Trainable Ratio: 3910001/127965041=3.055523%
[INFO|(OpenDelta)basemodel:702]2024-04-20 03:54:40,819 >> Delta Parameter Ratio: 3318640/127965041=2.593396%
[INFO|(OpenDelta)basemodel:704]2024-04-20 03:54:40,819 >> Static Memory 1.45 GB, Max Memory 10.31 GB

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 0 loss 0.698  acc 0.522: 100%|█████████▉| 408/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.699  acc 0.522: 100%|█████████▉| 408/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.699  acc 0.522: 100%|█████████▉| 409/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.699  acc 0.521: 100%|█████████▉| 409/410 [02:06<00:00,  3.21it/s]
epoch 0 loss 0.699  acc 0.521: 100%|██████████| 410/410 [02:06<00:00,  3.48it/s]
epoch 0 loss 0.699  acc 0.521: 100%|██████████| 410/410 [02:06<00:00,  3.23it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 1 loss 0.686  acc 0.546: 100%|█████████▉| 408/410 [02:06<00:00,  3.21it/s]
epoch 1 loss 0.686  acc 0.546: 100%|█████████▉| 408/410 [02:07<00:00,  3.21it/s]
epoch 1 loss 0.686  acc 0.546: 100%|█████████▉| 409/410 [02:07<00:00,  3.21it/s]
epoch 1 loss 0.686  acc 0.546: 100%|█████████▉| 409/410 [02:07<00:00,  3.21it/s]
epoch 1 loss 0.686  acc 0.546: 100%|██████████| 410/410 [02:07<00:00,  3.47it/s]
epoch 1 loss 0.686  acc 0.546: 100%|██████████| 410/410 [02:07<00:00,  3.22it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 2 loss 0.673  acc 0.58: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 2 loss 0.673  acc 0.58: 100%|█████████▉| 408/410 [02:06<00:00,  3.24it/s]
epoch 2 loss 0.673  acc 0.58: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 2 loss 0.673  acc 0.58: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 2 loss 0.673  acc 0.58: 100%|██████████| 410/410 [02:06<00:00,  3.50it/s]
epoch 2 loss 0.673  acc 0.58: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 3 loss 0.659  acc 0.592: 100%|█████████▉| 408/410 [02:05<00:00,  3.25it/s]
epoch 3 loss 0.658  acc 0.592: 100%|█████████▉| 408/410 [02:06<00:00,  3.25it/s]
epoch 3 loss 0.658  acc 0.592: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 3 loss 0.658  acc 0.592: 100%|█████████▉| 409/410 [02:06<00:00,  3.25it/s]
epoch 3 loss 0.658  acc 0.592: 100%|██████████| 410/410 [02:06<00:00,  3.51it/s]
epoch 3 loss 0.658  acc 0.592: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]

  0%|          | 0/410 [00:00<?, ?it/s]
epoch 4 loss 0.645  acc 0.61: 100%|█████████▉| 408/410 [02:05<00:00,  3.23it/s]
epoch 4 loss 0.645  acc 0.61: 100%|█████████▉| 408/410 [02:06<00:00,  3.23it/s]
epoch 4 loss 0.645  acc 0.61: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 4 loss 0.645  acc 0.61: 100%|█████████▉| 409/410 [02:06<00:00,  3.24it/s]
epoch 4 loss 0.645  acc 0.61: 100%|██████████| 410/410 [02:06<00:00,  3.50it/s]
epoch 4 loss 0.645  acc 0.61: 100%|██████████| 410/410 [02:06<00:00,  3.24it/s]
INFO:name:Cycle 20, the best candidate in pop has score 0.6, and best in the run 0.6
INFO:name:Mean 0.5748666666666667 and standard deviation 0.014414653038565357 of score in the population
Best of all candidates :([{'insert_modules': ('attention.self',), 'bottleneck_dim': (64,), 'non_linearity': 'gelu_new'}, {'insert_modules': ('intermediate', 'output', 'attention.self'), 'bottleneck_dim': (128, 64), 'non_linearity': 'gelu_new'}, {'insert_modules': ('intermediate',), 'bottleneck_dim': (16, 32), 'non_linearity': 'gelu_new'}, {'insert_modules': ('attention.self', 'intermediate'), 'bottleneck_dim': (64, 32), 'non_linearity': 'gelu_new'}, {'insert_modules': ('attention', 'attention.self', 'output'), 'bottleneck_dim': (128,), 'non_linearity': 'gelu_new'}, {'insert_modules': ('attention', 'attention.self', 'intermediate'), 'bottleneck_dim': (128,), 'non_linearity': 'relu'}, 0, 0, 0, 0, 0, 0], 0.6)
